[06:26] *** human-blip left
[06:27] *** human-blip joined
[15:10] *** DarthGandalf left
[19:39] <disbot3> <apogee> Just pushed LLM::Chat.   It's, as the name suggests, a library to facilitate inferencing LLMs over the API.  It can do text completion, chat completion (both with or without streaming) as well as context shifting (pruning old messages with exceptions you choose) & inserting specific messages at chosen depth.

[19:40] <disbot3> <apogee> Documentation is sparse but the examples cover most of it.

[19:41] <disbot3> <apogee> cc @antononcube because I know this is in your wheelhouse. ðŸ˜„

[19:52] *** DarthGandalf joined
[19:52] <disbot3> <antononcube> How does it compare to the chat objects in "LLM::Functions ".

[19:56] <disbot3> <apogee> It's hard to say, it's more focused on managing things like context shifting. I couldn't tell whether LLM::Functions supports streaming when I looked at the docs tbh.

[19:57] <disbot3> <apogee> I do most of my inferencing with local LLMs on 4x A6000.

[20:12] <disbot3> <apogee> I have a fairly lengthy roadmap rn, what I'm pushing is mostly components towards an end goal.

[20:32] <disbot3> <jubilatious1_98524> That's what UTF8-C8 is for: https://docs.raku.org/language/unicode#UTF8-C8

[21:21] <disbot3> <oshaboy> Ok but is there any way to convert UTF8-C8 to Uni?

