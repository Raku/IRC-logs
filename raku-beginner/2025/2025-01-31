[03:15] *** human-blip left
[03:16] *** human-blip joined
[04:00] *** deadmarshal_ left
[04:34] *** deadmarshal_ joined
[04:51] *** human-blip left
[04:53] *** human-blip joined
[06:14] <discord-raku-bot> <rcmlz> That was a good reading, thanks Liz for the good stuff you create.

[06:41] *** jmcgnh left
[06:42] *** jmcgnh joined
[08:11] <discord-raku-bot> <librasteve> I definitely second that, it makes me think that raku should be renamed tortoise

[08:14] <discord-raku-bot> <librasteve> I recall raiph wrote something on how everything in raku is built on something else (that can be used by coders if they are unhappy with the standard abstractions) words like Int.^HOW.^HOW.^HOW come to mind … wonder if lizmat you wrote anything on this?

[09:26] <lizmat> I haven't written anything about that (yet)

[09:35] <discord-raku-bot> <rcmlz> Is there already a publication emphazising/comparing best practice on using lazy-stuff over non-lazy-things? Maybe this would help people like me leaping forward - like „learning apply in R“ did.

[09:37] <discord-raku-bot> <rcmlz> Maybe with some latency/throughput comparison between booth aporoaches as „Code-You-Should-Try-Yourself“

[09:42] <discord-raku-bot> <rcmlz> I remember trying the code from here: https://github.com/Raku/marketing/blob/9ba54324d46ab84b2d8b300c9507fcc5a1ecb3e5/Flyers/All-Your-Cores-Flyer--1524053519/All-Your-Cores-Flyer--1524053519.pdf

[09:43] <discord-raku-bot> <rcmlz> Just to find out that it is not faster as promised on the poster

[09:43] <discord-raku-bot> <rcmlz> ➜  ~ time raku -e "(^∞).grep(.is-prime)[10000].say"       104743 raku -e "(^∞).grep(.is-prime)[10000].say"  0,28s user 0,04s system 115% cpu 0,274 total ➜  ~ time raku -e "(^∞).hyper.grep(.is-prime)[10000].say" 104743 raku -e "(^∞).hyper.grep(.is-prime)[10000].say"  0,47s user 0,08s system 158% cpu 0,350 total

[09:44] <discord-raku-bot> <rcmlz> So for me it would help to have code snippets to execute to understand and test the iterator/laziness-things.

[09:48] <discord-raku-bot> <rcmlz> sort of see the benefits in terms of: "seeing is believing"

[10:09] <lizmat> since the time of that example, two things have happened:

[10:10] <lizmat> the determination of .is-prime has become *much* faster, so the 10000th prime number almost takes no effort

[10:15] <lizmat> m: (^∞).grep(*.is-prime)[500000].say; say now - INIT now

[10:15] <camelia> rakudo-moar 6c5afe65d: OUTPUT: «7368791␤2.796253531␤»

[10:16] <lizmat> now because of the fact that .is-prime is much faster, the default batch-size for .hyper is waaay too small

[10:16] <lizmat> m: (^∞).hyper.grep(*.is-prime)[500000].say; say now - INIT now

[10:16] <camelia> rakudo-moar 6c5afe65d: OUTPUT: «7368791␤4.734258993␤»

[10:16] <lizmat> you the overhead of hyper kicking in

[10:16] <lizmat> m: (^∞).hyper(batch => 2048).grep(*.is-prime)[500000].say; say now - INIT now

[10:16] <camelia> rakudo-moar 6c5afe65d: OUTPUT: «7368791␤0.712647335␤»

[10:16] <lizmat> with a greater batch size, you *can* see that hyper makes a difference

[10:18] <lizmat> sadly, the design of .hyper does not allow the batch size to be adapted "on the fly"

[10:18] <lizmat> that's what I tried to address with https://raku.land/zef:lizmat/ParaSeq

[10:30] <lizmat> however, it looks like that module will need some TLC and possible some re-design at some stage as well

[11:57] <discord-raku-bot> <rcmlz> Thank you for the explanation. I enjoy using Raku in its current state a lot, so no stress on that. And Raku execution speed was so far no issue for my problems.

[11:58] <discord-raku-bot> <rcmlz> You mean the "red flags" from the actions?

[12:01] <lizmat> yes... the red flags are because some of the tests fail...  and they fail intermittently, pointing at some kind of race condition

[12:01] <lizmat> which I haven't been able to fix yet  :-(

[12:02] <lizmat> also, it looks like the overhead in ParaSeq is too high, need to think of a better way to pass performance info between the dispatcher and the threads

[12:08] <discord-raku-bot> <rcmlz> I asked ChatGPT what programming languages actually have "batch size of parallel executed tasks adapted on the fly". It came up with Python, Julia, Scala, Rust, Go, C++ and Raku:  7. Raku      Raku supports parallel constructs (start, await) and hyperoperators (>>, <<). While not as sophisticated as dedicated frameworks, custom task schedulers can adapt workloads.  Would you like to see a Raku example for such

[12:08] <discord-raku-bot> adaptive batching?

[12:09] <discord-raku-bot> <rcmlz> The example it gave me was not adjusting the batch size of parallel execution, but of single-thread execution ...

[12:12] <discord-raku-bot> <antononcube> Wolfram Language / Mathematica has parallel batch size calculations “a on the fly.” (Of course!)

[12:13] <discord-raku-bot> <rcmlz> If it is not in ChatGPT - it does not exist!

[12:14] <discord-raku-bot> <rcmlz> You know, in the old days "you had to be on the first page on google", but that might change ;-) LOL

[12:20] <discord-raku-bot> <rcmlz> PS: It probalbly does not realy matter if Python or any of the listed languages really have that capability - e.g. to my understanding removing the GIL in Python just started recently - but "information" from sources like ChatGPT might influence a lot of "less technical" people  as falsification is a lot of effort that is probably avoided by those people.

[13:07] <discord-raku-bot> <antononcube> Chat-GPT - Shmack-Shmi-PT -- using it is so 2023 or 2024. In 2025 what matters is what DeepSeek says.

[13:08] <discord-raku-bot> <antononcube> Which means, I guess, that I have to demonstrate soon how to use DeepSeek with Raku.

[13:38] <discord-raku-bot> <rcmlz> LOL

[17:52] *** msiism joined
[18:03] *** snonux joined
[23:10] *** msiism left
[23:28] *** DarthGandalf left
[23:29] *** DarthGandalf joined
