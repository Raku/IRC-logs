[00:17] *** dct left
[03:00] <releasable6> Next release in ≈1 day and ≈15 hours. 4 blockers. Please log your changes in the ChangeLog: https://github.com/rakudo/rakudo/wiki/ChangeLog-Draft

[04:38] *** Ven`` joined
[04:47] *** Ven`` left
[05:01] *** tyil left
[05:15] *** tyil joined
[05:48] *** robertle left
[06:33] *** jmerelo joined
[06:34] <jmerelo> Hi

[07:22] *** ufobat_ joined
[07:38] *** jmerelo left
[07:57] <samcv> nice. ok i'm free tomorrow for release

[07:57] <samcv> also i am looking into being organizer for PerlCon 2020

[07:58] <[TuxCM]> goed bezig Sam!

[08:02] <samcv> i am in talks with people at booking about getting time off to do it or possibly doing it on work time, or partially on wowrk time

[08:02] <samcv> the guy was very suprised i'd only been working there for 5 weeks :P

[08:08] *** tyil left
[08:17] *** tyil joined
[09:51] <Tux__> Rakudo version 2018.12-302-g121ca5fda - MoarVM version 2018.12-105-gc0435e55a

[09:51] <Tux__> csv-test-xs-20      0.430 -  0.431

[09:51] <Tux__> csv-ip5xs           0.741 -  0.758

[09:51] <Tux__> test-t --race       0.861 -  0.903

[09:51] <Tux__> test-t              1.868 -  1.883

[09:51] <Tux__> csv-ip5xs-20        6.097 -  6.245

[09:51] <Tux__> test                7.117 -  7.185

[09:51] <Tux__> test-t-20 --race   10.680 - 10.872

[09:51] <Tux__> csv-parser         22.378 - 22.926

[09:51] <Tux__> test-t-20          31.840 - 33.044

[10:23] <jnthn> Good morning o/

[10:44] <lizmat> jnthn o/

[10:45] <lizmat> m: FOO: for <a b> { FIRST say "bar"; .say; last }   # works fine

[10:45] <camelia> rakudo-moar 121ca5fda: OUTPUT: «bar␤a␤»

[10:45] <lizmat> m: FOO: for <a b> { FIRST say "bar"; .say; last FOO }   # huh?

[10:45] <camelia> rakudo-moar 121ca5fda: OUTPUT: «bar␤labeled last without loop construct␤  in block <unit> at <tmp> line 1␤␤a␤»

[10:59] *** dogbert2_ joined
[10:59] <lizmat> bisectable: FOO: for <a b> { FIRST say "bar"; .say; last FOO } 

[10:59] <bisectable6_> lizmat, Bisecting by exit code (old=2015.12 new=121ca5f). Old exit code: 0

[11:00] <bisectable6_> lizmat, bisect log: https://gist.github.com/478f77f09ea66e4ac69bafa244cfbb77

[11:00] <bisectable6_> lizmat, (2017-01-29) https://github.com/rakudo/rakudo/commit/031efe0d9fa1cf38d6d631d0e98e28481f13ac5f

[11:00] <dogbert2_> jnthn: any theories as to why  https://github.com/rakudo/rakudo/commit/ee2238f731f548861e26d745ea1be2ba22d24f4e slows down some code as much as 80% and increasing the amount of GC's massively?

[11:04] <jnthn> dogbert2_: No

[11:04] <jnthn> What code?

[11:04] <dogbert2_> jnthn: sec

[11:04] <dogbert2_> jnthn: https://gist.github.com/dogbert17/7423640a8bc4d5b9dd7c960295154ee9

[11:05] <dogbert2_> I can't say that it's $work code but worth mentioning anyway :)

[11:05] <jnthn> Hmmmm

[11:05] <dogbert2_> uh oh

[11:06] <lizmat> 29M Scalar allocations ?

[11:06] <dogbert2_> yes

[11:07] <dogbert2_> before it was 2M allocations

[11:10] <lizmat> looks to me that @a[$i * $j++] = 1 is boxing the 1 every time ???

[11:10] <lizmat> also: it is winding up in the ASSIGN-POS(int,Int) candidate, rather than the ASSIGN-POS(int,int) one

[11:11] <dogbert2_> compared with older code I noticed that a call to push-all was no longer jit'ed although it was before

[11:15] <dogbert2_> lizmat: I guess that the increased number of gc's has something to do with the 29M scalars

[11:15] <lizmat> yeah, that's just the result of the many allocations

[11:16] <dogbert2_> lizmat: the code is one of my test benchmarks which I use quite often so I knew that the change causing this was quite recent

[11:16] <lizmat> dogbert2_++  # good to know these exist

[11:17] <jnthn> There's only one NQP commit involved too

[11:18] <dogbert2_> jnthn: does it make any sense?

[11:18] <jnthn> No.

[11:19] <jnthn> In that the code in question doesn't seem to be doing any compile-time evaluation

[11:20] <dogbert2_> it is very odd, but the previous rakudo commit, i.e. https://github.com/rakudo/rakudo/commit/d904b7048bfaa8b7e1583baa1af0665f29c5b8c3 doesn't have this regression

[11:30] * jnthn has a poke at it

[11:30] <dogbert2_> jnthn++

[11:31] <lizmat> jnthn: changing the last lines to "say @a.grep(* > 1).sum;" seems to tickle a weird OSR / deopt case: 148484 On Stack Replacements / 282176 deoptimizations

[11:33] <jnthn> I can imagine how that one happens

[11:33] <jnthn> We don't yet have anything to discard bad choices in specialization

[11:33] <dogbert2_> runtime: HEAD: real 0m5,946s     d904b7048bf: real 0m3,490s

[11:34] <jnthn> Oh, I guess CORE.setting perhaps has things that are compile-time evaluated

[11:34] <jnthn> And it's tripping over one of those

[11:34] <jnthn> Hmm

[11:35] * jnthn wonders how to have cake and eat it...

[11:36] * lizmat suspects the " = 1 "

[11:36] <jnthn> ?

[11:36] <lizmat> as a thing that may get compile-time evaluated ?

[11:37] <lizmat> as part of constant folding >

[11:37] <lizmat> ?

[11:37] <jnthn> Oh,

[11:38] <jnthn> No, that's not a problem

[11:38] <jnthn> It's that some code in CORE.setting is used at BEGIN time

[11:38] <jnthn> I suspect it's that, anyway

[11:38] <lizmat> perhaps the BEGIN hack I put in the core_epilogue recently ?

[11:39] <jnthn> No, it's the commit dogbert2_ found

[11:39] <jnthn> And it'll be because we index an array or some such in a BEGIN or constant or whatever in CORE.setting

[11:39] <jnthn> There's probably quite a few places that do that

[11:41] <lizmat> ah, I think I understand

[11:42] <dogbert2_> note that both head and d904b7048bf allocates 29M scalars, it's 2018.10 which only allocates 2M, still both 2018.10 and d904b7048bf are a lot faster than HEAD

[11:43] <dogbert2_> 2018.10 is the fastest though: real 0m3,317s and 63 GC's. Head and d904b7048bf do 376 GC's on my machine

[11:43] <jnthn> The change I put in was becuase if a closure was taken inside of a BEGIN block then it was against a provisional compilation and it could have bogus lexical indices

[11:43] <jnthn> ...somehow :)

[11:44] <jnthn> Since I think it only afflicted a closure in such code, I guess we might recover it by making it check we're actually inside of one

[11:46] <jnthn> Adding such a check gets a bit back, but not everything

[11:46] <jnthn> Or I think not everything, need another measurement

[11:48] <jnthn> No, not close to everything.

[11:48] <jnthn> I wonder if I can find a different solution to the regression I was fixing

[11:53] <dogbert2_> perhaps you can if you have some l...h

[11:54] <jnthn> :)

[11:54] <jnthn> Mm....celery soup today, I think

[11:58] <dogbert2_> no Indian food?

[11:58] <jnthn> Not today :)

[11:58] <jnthn> Had some earlier this week

[11:58] <jnthn> And probably will at the weekend

[11:59] <jnthn> I can relax it a bit and not regress on the bug and get back some of the performance

[12:00] <jnthn> But still not all of it

[12:00] <dogbert2_> a tricky problem then

[12:02] <dogbert2_> I find it interesting that 2018.10 only needs to do 2M scalar allocs given that the profiler isn't lying to me

[12:02] <jnthn> I'm a bit curious which cases are triggering compilation of slower lookups that are on the hot path here

[12:03] <jnthn> After my tweak, I mean. 

[12:03] <jnthn> I expected that'd get the bunch of them

[12:10] <jnthn> Ah, think I got a fix that does both

[12:10] <jnthn> It can spectest while I lunch )

[12:10] <dogbert2_> clever move, jnthn++

[12:11] * dogbert2_ follows jnthn's example and goes for lunch

[12:15] *** lucasb joined
[12:46] *** tyil left
[12:58] <jnthn> Yay, it passes

[13:00] <dogbert2_> hooray

[13:02] *** tyil joined
[13:05] <jnthn> Pushed both NQP and Rakudo fixes

[13:09] <lizmat> did we lose Geth again?

[13:11] <dogbert2_> how much speed did you manage to regain?

[13:14] <jnthn> lizmat: Looks like

[13:14] <jnthn> dogbert2_: I think all of it

[13:14] <jnthn> haha

[13:14] <jnthn> PEA: replacement impossible due to prof_allocated

[13:14] <jnthn> The profiler is preventing scalar replacement :P

[13:15] <jnthn> I guess ideally we'd rewrite it to an op that logs the number of replaced allocations

[13:20] <dogbert2_> on my system I now get: real 0m3,679s

[13:21] <dogbert2_> that's a two second improvement

[13:32] <jnthn> And as good as it used to be?

[13:33] <jnthn> btw, working on the profiler

[13:34] <lizmat> fwiw, this also seems to affect test-t in a positive way :-)

[13:36] <jnthn> :)

[13:36] <jnthn> So, it wasn't the pea merge after all

[13:38] <lizmat> let's see what the next test-t says...  pretty sure it won't be near 1.6, but rather at about 1.8  still

[13:38] <lizmat> perhaps 1.75 

[13:43] <dogbert2_> jnthn: yes, as good as it used to be

[13:45] <jnthn> Good.

[13:46] <dogbert2_> only 2018.10 is a bit faster

[13:47] <dogbert2_> but not by much

[13:50] <jnthn> m: say 

[13:50] <camelia> rakudo-moar 4a2124a62: OUTPUT: «5===SORRY!5===␤Argument to "say" seems to be malformed␤at <tmp>:1␤------> 3say7⏏5<EOL>␤Other potential difficulties:␤    Unsupported use of bare "say"; in Perl 6 please use .say if you meant to call it as a method on $_, or use an …»

[13:50] <jnthn> grr

[13:50] <jnthn> m: say 29326275 - 153082

[13:50] <camelia> rakudo-moar 4a2124a62: OUTPUT: «29173193␤»

[13:51] <jnthn> That's how many Scalar container allocations PEA gets rid of :)

[13:51] <jnthn> Now that the profiler doesn't block the optimization happening

[13:51] <jnthn> Profiler UI doesn't report that yet

[13:51] <jnthn> In fact, not spitting the info out for it yet

[13:52] <jnthn> Oh, also not JITting that prof_replaced yet either, but that's in theory easy :)

[13:54] <jnthn> Yay, there we go

[13:56] <japhb> Always nice to see someone having a productive day.  :-)

[13:56] <dogbert2_> jnthn++ is on a roll

[13:56] <dogbert2_> must be that Celery soup :)

[14:18] <jnthn> timotimo++ can probably do nice stuff in his shiny new profiler UI, but I've also put a simple calculation/display on the overview page of the HTML output of the built-in one too

[14:18] <jnthn> In this case, "Scalar replacement eliminated 29172848 allocations (that's 89.4%)." :)

[14:18] <jnthn> Which is, uh, unusually good :P

[14:24] <jnthn> .tell timotimo https://github.com/MoarVM/MoarVM/commit/64d41cc1b4337a0c7d05689ce95a18603d6f595b

[14:24] <yoleaux> jnthn: I'll pass your message to timotimo.

[14:24] <jnthn> All pushed :)

[14:55] <timotimo> o/

[14:55] <yoleaux> 14:24Z <jnthn> timotimo: https://github.com/MoarVM/MoarVM/commit/64d41cc1b4337a0c7d05689ce95a18603d6f595b

[14:56] <timotimo> jnthn: do we want logging for "scalar-replaced object was materialized"?

[14:57] <dogbert2_> hi timotimo

[14:59] <jnthn> timotimo: Hmm, maybe some day

[14:59] <jnthn> Though I guess deopt is relatively rare

[14:59] <jnthn> I mean, if you're motivated to add it right away, sure :)

[15:04] <timotimo> motivation? in this economy?! ;)

[15:05] *** Geth left
[15:05] *** Geth joined
[15:05] *** ChanServ sets mode: +v Geth

[15:19] *** Tux__ is now known as |Tux|

[15:20] <|Tux|> The number of languages is still growing :) http://tux.nl/Talks/CSV6/speed5.html

[15:36] <dogbert2_> |Tux| interesting numbers

[15:40] <dogbert2_> I see that, according to the profiler, running test-t, from |Tux| CSV repo, there are 30k local deopts. Is that much?

[15:43] <jnthn> dogbert2_: It's quite a lot, but not automatically bad

[15:43] <dogbert2_> I guess it's application dependant but it 'felt' high

[15:45] <timotimo> hard to say what percentage of a frame's run are pre- vs post-deopt

[15:45] <timotimo> like, does the frame have a loop in it that runs a whole bunch before it stumbles upon a deopt? or is it a short frame that deopts on the second-to-last instruction? or is it a frame that deopts in the first 10%?

[15:45] <timotimo> but if there's a loop, it'll try to OSR again, right?

[15:47] <jnthn> Yes, it'll typically try to deopt again

[15:47] <timotimo> re-opt :)

[15:47] <jnthn> I mean, if you look at a `for $fh.lines() { }` it'll typically deopt quite often - once per time it needs to read from the filesystem because it depleted its buffer.

[15:48] <jnthn> And it's a strategic deopt there, 'cus it keeps the rare path out of the optimized code

[15:48] <timotimo> the moarperf profiler will display deopt and OSR info better soon-ish

[15:48] <jnthn> But yeah, it feels high enough it'd be worth looking into where it's happening

[15:48] <jnthn> timotimo: yay :)

[15:48] <timotimo> i haven't looked into why exactly it doesn't already, i've put a little bit of code in, but it seems to always give 0s

[15:54] * |Tux| has no idea what a deopt is:/

[15:55] <timotimo> it's when the optimized code was built based on an assumption and the assumption wasn't upheld in one spot

[15:55] <|Tux|> thnx

[15:55] <timotimo> when that happens, the optimized code isn't allowed to continue running as it was; it has to be "deoptimized"

[15:55] <timotimo> depending on how complex the optimizations have been, undoing them can become more and more complex

[15:56] <dogbert2_> 10k deopts in e.g. reify-until-lazy SETTING::src/core/List.pm6:99

[15:56] <|Tux|> in this test case, the code is pretty "stable": there are no attributes of the global CSV object that change during the process

[15:56] <dogbert2_> thx for the explanation

[15:56] <|Tux|> (which is allowed in user-cases)

[15:58] <dogbert2_> perhaps it has something to do that the csv file in question contains 10k lines

[15:59] <|Tux|> likely then 

[15:59] <dogbert2_> was a bit shocked when I saw that Perl6 is slower than Rust :)

[16:01] <timotimo> we've got a bit more work to do for sure :)

[16:41] *** ufobat___ joined
[16:45] *** ufobat_ left
[17:36] <lucasb> can I request the creation of additional labels to help categorize rakudo issues?

[17:36] <timotimo> you can definitely request it :)

[17:37] <lucasb> I think "native types" and "type captures" would fit into some issues

[17:37] <lucasb> there's nativecall but no "native types"

[17:38] <lucasb> I wish I could label the issues myself, but I don't have the rights...

[17:39] <timotimo> hm, "type captures"

[17:39] <timotimo> maybe that can be made a little less specific

[17:39] <lucasb> timotimo: there's a list of bugs with regard type captures, there's why I thought of a dedicated label

[17:40] <timotimo> are all type capture related issues regarding signatures of subs, or also some for roles?

[17:40] <lucasb> bugs when "role R[::Type] {...}" and "sub f(::Type $x)" doesn't get properly captured

[17:41] <timotimo> mhm

[17:42] <timotimo> would you say CStruct would also fit into "native types"? how about Buf? my int @foo?

[17:44] <lucasb> I was thinking initially of int/uint/num/str issues, but hey, the more aways issues are labeled, the better, no? :)

[17:44] <lucasb> *more ways

[17:44] <timotimo> we can probably have a single label just for "unsigned" support

[17:45] <timotimo> "native types" now exists, though the description could probably use an improvement

[17:46] <lucasb> timotimo: thanks

[17:47] <timotimo> could "generic types" be a label for the "type captures" thing?

[17:48] <lucasb> don't you prefer a Perl 6 term instead of a Java one? :)

[17:49] <lucasb> j/k

[17:50] <jnthn> I thought that terminology was...generic :)

[17:51] <timotimo> hey, that's type erasure

[17:51] <AlexDaniel`> samcv: 

[17:51] <AlexDaniel`> samcv: well, I'm away till Monday…

[17:54] <lucasb> native types issues: R#2692 R#2579 R#2542 R#2541 R#2414 (if someone could label them :)

[17:54] <synopsebot> R#2692 [open]: https://github.com/rakudo/rakudo/issues/2692 Accessing native attributes discards sign information

[17:54] <synopsebot> R#2579 [open]: https://github.com/rakudo/rakudo/issues/2579 smallnatives are neither tested nor working

[17:54] <synopsebot> R#2542 [open]: https://github.com/rakudo/rakudo/issues/2542 A native variable definition + initialization is **not** a left value

[17:54] <synopsebot> R#2541 [open]: https://github.com/rakudo/rakudo/issues/2541 Post-incrementing native integers across overflow boundary

[17:54] <synopsebot> R#2414 [open]: https://github.com/rakudo/rakudo/issues/2414 [perf] Native attrs compile into an attrref instead of attr

[17:54] <AlexDaniel`> and I think there were some changes in moarvm after the last time I ran Blin

[17:54] <AlexDaniel`> so-o-o… enjoy that free time? :)

[17:54] <AlexDaniel`> I mean, maybe having a changelog ahead of time would be nice…

[17:56] <lucasb> R#2354

[17:56] <synopsebot> R#2354 [open]: https://github.com/rakudo/rakudo/issues/2354 uintN handles signature bit differently for scalars and attributes

[18:04] *** dct joined
[19:21] *** dct left
[19:52] *** robertle joined
[20:04] <[TuxCM]> On request of lizmat 

[20:04] <[TuxCM]> Rakudo version 2018.12-303-g4a2124a62 - MoarVM version 2018.12-106-g64d41cc1b

[20:04] <[TuxCM]> csv-test-xs-20      0.438 -  0.446

[20:04] <[TuxCM]> csv-ip5xs           0.734 -  0.753

[20:04] <[TuxCM]> test-t --race       0.874 -  0.977

[20:04] <[TuxCM]> test-t              1.912 -  1.929

[20:04] <[TuxCM]> csv-ip5xs-20        6.333 -  6.462

[20:04] <[TuxCM]> test                7.690 -  8.365

[20:04] <[TuxCM]> test-t-20 --race   10.764 - 10.920

[20:04] <[TuxCM]> csv-parser         23.568 - 23.837

[20:04] <[TuxCM]> test-t-20          32.344 - 32.636

[20:04] <lizmat> hmmm... disappointing  :-(  I'd expected more  :-(

[20:04] <lizmat> in reduction of test-t

[21:36] *** MasterDuke left
[21:54] *** pyrimidine left
[21:55] *** pyrimidine joined
[22:49] *** sivoais left
[22:55] *** lucasb left
[23:00] <releasable6> Next release in ≈19 hours. 4 blockers. Please log your changes in the ChangeLog: https://github.com/rakudo/rakudo/wiki/ChangeLog-Draft

[23:07] *** TreyHarris joined
