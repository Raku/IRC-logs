[00:44] *** abraxxa-home left
[01:32] *** jgaz left
[01:59] *** Manifest0 left
[02:39] *** guifa joined
[02:50] *** hulk joined
[02:51] *** kylese left
[03:15] *** hulk left
[03:15] *** kylese joined
[03:39] *** Aedil joined
[05:45] <SmokeMachine> I think ASTQuery is getting better and better... https://usercontent.irccloud-cdn.com/file/qpS95TqN/image.png

[05:47] <SmokeMachine> I think the next step is to have a good set of default groups... currently that's what we have (https://github.com/FCO/ASTQuery/blob/main/lib/ASTQuery/Matcher.rakumod#L8-L60)... does anyone have any suggestion what groups we should have?

[05:48] <SmokeMachine> (that query (`.var-declaration[scope="has"]`) searches for all attributes)

[06:03] *** guifa left
[06:27] *** teatwo joined
[06:30] *** teatime left
[06:56] *** Aedil left
[07:14] *** Aedil joined
[07:53] *** suman joined
[07:57] <discord-raku-bot> <librasteve> lizmat: happy to hear that :a:b:c in non function call settings should work ‚Ä¶ any idea where it is covered in the docs? anyway suggest any bug issue fix should also update docs of course‚Ä¶

[07:59] <suman> q: # The input expression containing Raku code

[07:59] <suman> my $input_code = '1+3; 2/3; say "Hello" ';

[07:59] <tellable6> 2023-11-28T22:08:40Z #raku <librasteve> suman: suggest you also try @array>>.num  (ie. native Num) ... maybe also worth looking at the ingestion phase and coercing to Num|num when you do the csv parse or whatever

[07:59] <suman> # execute and capture the output

[07:59] <suman> my $code = qqx{raku -e $input_code};

[07:59] <suman> say $code

[07:59] <suman> camelia: # The input expression containing Raku code

[07:59] <suman> my $input_code = '1+3; 2/3; say "Hello" ';

[07:59] <suman> # execute and capture the output

[07:59] <suman> my $code = qqx{raku -e $input_code};

[07:59] <suman> say $code

[08:00] <suman> camelia: my $input_code = '1+3; 2/3; say "Hello" '; my $code = qqx{raku -e $input_code}; say $code

[08:02] <suman> m: my $input_code = '1+3; 2/3; say "Hello" '; my $code = qqx{raku -e $input_code}; say $code

[08:02] <camelia> rakudo-moar dc0342502: OUTPUT: ¬´WARNINGS for -e:‚ê§Useless use of "+" in expression "1+3" in sink context (line 1)‚ê§/bin/sh: 2/3: No such file or directory‚ê§/bin/sh: say: command not found‚ê§‚ê§¬ª

[08:05] <suman> m: my $code = '1+3; 2/3; say "Hello" '; my $output = shell("raku -e $code", :out).out.slurp; say $output

[08:05] <camelia> rakudo-moar dc0342502: OUTPUT: ¬´WARNINGS for -e:‚ê§Useless use of "+" in expression "1+3" in sink context (line 1)‚ê§/bin/sh: 2/3: No such file or directory‚ê§/bin/sh: say: command not found‚ê§‚ê§¬ª

[08:20] *** suman left
[09:33] *** Xliff left
[09:37] *** sena_kun joined
[10:17] *** Manifest0 joined
[10:58] *** Aedil left
[11:28] *** Sgeo left
[12:13] *** guifa joined
[13:37] <discord-raku-bot> <librasteve> suman: your code works fine on my machine - with the same Useless use of warning ... I suspect that the raku eval here in the channel is configured so that you can't spawn a shell process via qqx

[13:37] <tellable6> librasteve, I'll pass your message to suman

[15:09] <discord-raku-bot> <antononcube> @librasteve Any "Inline::Python" related epiphanies while being in the pub?

[15:46] *** Xliff joined
[15:52] *** Aedil joined
[16:43] <discord-raku-bot> <librasteve> it now installs okish ‚Ä¶ i have pushed a patch PR to GH that removed a couple of failing tests and new README advice if you hit the config error I hit but meantime you should be able to zef install from GH librasteve fork

[17:29] <guifa> antononcube: which module was it that would run local LLM modules?

[17:31] <discord-raku-bot> <antononcube> See ‚ÄúWWW::LLaMA‚Äù. The generic modules ‚ÄúLLM::Functions‚Äù and ‚ÄúLLM::Prompts‚Äù also apply. Chatbooks can also be used,

[17:31] <guifa> ah right

[17:32] <guifa> Now I just need to figure out the simplest way to rev up the server.  WWW::LLaMA doesn't handle the model loading and all itself, right?

[17:32] <discord-raku-bot> <antononcube> See : https://www.youtube.com/watch?v=zVX-SqRfFPA

[17:33] <discord-raku-bot> <antononcube> You have to download the llamafiles yourself.

[17:34] <guifa> alright -- thanks

[17:36] <guifa> oh wow I didn't realize the llama files were universal executables

[17:36] <guifa> that's....really cool

[17:36] <discord-raku-bot> <antononcube> Yeah!

[17:38] <discord-raku-bot> <antononcube> There is a request from @rcmlz to implement an Ollama front end. I will do it some point but it is  of low priority for me.

[17:38] <guifa> I'm using it for a project in a class but professor would prefer me to give him a single install script and / or very simple instructions for install

[17:39] <discord-raku-bot> <antononcube> I.e. I strongly suspect you a module-client like this in mind: https://ollama.com

[17:39] <guifa> so it sounds like I could put the llama file in resources and go from there 

[17:40] <guifa> yeah -- right now biggest thing is just have...something :) I'll perfect if I ever decide to release

[17:40] <discord-raku-bot> <antononcube> I think script that does this is fairly easy to do.

[17:41] <guifa> yeah. Hoping I could get it down to "install rakudo" and "install this module" :) 

[17:44] <discord-raku-bot> <antononcube> You can ask an LLM to generate that script, BTW.

[17:44] *** zenmov joined
[17:45] <guifa> ha there are still some things I surprisingly take enjoyment writing :)

[17:46] <discord-raku-bot> <antononcube> Ok. But this also involves writing (in Raku.)

[17:47] <guifa> Plus, my project is supposed to take me X number of hours

[17:47] <guifa> some of the other parts have gone smoother than expected

[17:47] <guifa> haha

[17:52] <discord-raku-bot> <antononcube> @guifa This is what I have in mind with "writing up an LLM script generation":

[17:52] <discord-raku-bot> <antononcube> https://cdn.discordapp.com/attachments/633753286209699870/1305228694045987027/Generat-LLaMA-model-download-Raku-script.png?ex=67324472&is=6730f2f2&hm=243ab2320e28923d219a08cd760af11a3f8adcaceaa2e929b7b0c585e4bd9d21&

[17:53] <guifa> nice

[17:54] *** zenmov left
[17:54] *** zenmov joined
[17:54] <discord-raku-bot> <antononcube> Even if does not work it s a good start to make the actual script. More instructions can be added to make the files executable, launch them, etc.

[17:54] <discord-raku-bot> <antononcube> And that also means that you code writing. üôÇ

[17:57] <discord-raku-bot> <antononcube> Here is the generation with the additional prompt element: >     "Make the downloaded model executable. Have an option to start the downloaded model or not."

[17:57] <discord-raku-bot> <antononcube> https://cdn.discordapp.com/attachments/633753286209699870/1305229745839149077/Generat-LLaMA-model-download-Raku-script-2.png?ex=6732456d&is=6730f3ed&hm=3efe07e9aae3225a4af6632db9c0cc2ec398fdba696e2e3e05212b6456612c3e&

[17:57] <guifa> yeah

[17:58] <guifa> I think one of my frustrations with LLMs is they're always really good but absolutely require that just-in-case human intervention.

[17:58] <guifa> was a bit saddening when I tried using it to adjust the difficulty of texts for reading -- generally quite good for English

[17:58] <guifa> but the more inflected a language was, the more its performance dropped, and unfortunately for teaching reading, you need accuracy to be high.  Great for helping teachers save a lot of time but not for 100% automated performance yet

[18:00] <discord-raku-bot> <antononcube> No, it is just a tool to speed up work.

[18:02] <discord-raku-bot> <antononcube> For programming with LLMs, very often one has to decide: > Should debug this (possibly mediocre) code generated by an LLM, or program the thing from scratch.

[18:04] <guifa> yes exactly

[18:11] <guifa> oh this is kinda cool.  I figured out a simple way to do let a role do a TWEAK without it being eaten up by its implementor

[18:14] <guifa> method new(|args) { my $new = self.bless: args; LEAVE $new.ROLE-TWEAK: args; $new }; method ROLE-TWEAK { ... }

[18:15] <guifa> it always gets called after the class TWEAKA, which in my case is a benefit

[18:32] *** silug4 joined
[18:32] *** silug left
[18:32] *** silug4 is now known as silug

[18:56] *** zenmov left
[19:00] *** Aedil left
[19:27] <ab5tract> guifa: slick!

[19:28] <ab5tract> I wonder if it switching to ENTER would always call ahead of the class TWEAK?

[19:28] <ab5tract> in case one needed the opposite dynamic

[19:32] *** avuserow joined
[20:49] <[Coke]> "i don't know who needs to hear this right now", but make sure you don't have any cloud stuff running you forgot about.

[20:50] <[Coke]> (my blin VM was up for a month doing nothing, oops)

[20:52] *** Sgeo joined
[20:59] <discord-raku-bot> <librasteve> ;-(

[21:01] <discord-raku-bot> <librasteve> fwiw I go raws-ec2 nuke ...https://raku.land/zef:librasteve/CLI::AWS::EC2-Simple

[21:58] *** sena_kun left
[22:50] *** guifa_ joined
[22:51] *** guifa left
[22:55] *** guifa_ left
