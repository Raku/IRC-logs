[00:02] *** arkiuat joined
[00:34] *** abraxxa left
[00:35] *** abraxxa joined
[01:05] *** librasteve_ left
[01:31] <disbot> <simon_sibl> I encounter an issue here, not sure if thats because of my code or not

[01:31] <disbot> <simon_sibl> https://termbin.com/v3bi

[01:31] <disbot> <simon_sibl> the http part works well, but the netcat part (to create a paste) doesnt work well

[01:31] <disbot> <simon_sibl> basically netcat doesnt wait for the server to send back the file ID

[01:32] <disbot> <simon_sibl> I tried many different flags, but none of them seems to solve it, it seems that netcat doesnt want to read after the first_line  receive part for some reason

[01:34] *** hulk joined
[01:35] *** kylese left
[02:07] *** Guest13 joined
[02:15] *** hulk left
[02:15] *** kylese joined
[02:23] *** Guest13 left
[03:04] *** Aedil joined
[03:17] <disbot> <simon_sibl> alright, this work: https://termbin.com/9bvp

[03:17] <disbot> <simon_sibl> but I need to use the --no-shutdown flag on netcat

[03:23] <disbot> <simon_sibl> okok, that with nmap/ncat with gnu/netcat it works as expected !

[04:24] *** arkiuat left
[04:32] *** arkiuat joined
[05:48] *** crnlskn joined
[06:06] *** kst``` joined
[06:06] *** perryprog_ joined
[06:06] *** hulk joined
[06:07] *** apa_c joined
[06:07] *** kylese left
[06:07] *** jmcgnh left
[06:07] *** apac left
[06:07] *** sftp left
[06:07] *** releasable6 left
[06:07] *** perryprog left
[06:07] *** kst`` left
[06:07] *** jdv left
[06:08] *** jmcgnh_ joined
[06:09] *** jdv joined
[06:10] *** sftp joined
[06:10] *** sftp left
[06:10] *** sftp joined
[06:10] *** releasable6 joined
[06:10] *** jmcgnh_ is now known as jmcgnh

[06:11] *** arkiuat left
[06:20] *** lizmat left
[06:23] *** arkiuat joined
[06:25] *** kst``` left
[06:27] *** arkiuat left
[06:29] *** lizmat joined
[06:34] *** [Coke]_ joined
[06:37] *** [Coke] left
[06:57] *** arkiuat joined
[07:02] *** arkiuat left
[07:22] *** Sgeo left
[07:25] *** ACfromTX left
[07:28] *** ACfromTX joined
[07:29] *** arkiuat joined
[07:36] *** lichtkind joined
[07:38] *** arkiuat left
[07:48] *** ACfromTX left
[07:51] *** arkiuat joined
[07:56] *** arkiuat left
[07:58] *** lizmat left
[07:59] *** lizmat joined
[08:00] *** ACfromTX joined
[08:01] *** Aedil left
[08:17] *** dakkar joined
[08:25] *** arkiuat joined
[08:30] *** arkiuat left
[08:33] *** arkiuat joined
[08:38] *** arkiuat left
[08:55] *** abraxxa left
[08:56] *** abraxxa joined
[09:09] *** arkiuat joined
[09:23] *** arkiuat left
[09:24] *** icovnik left
[09:51] *** arkiuat joined
[09:58] *** arkiuat left
[10:10] <disbot> <antononcube> @librasteve One way to explain LLM graph to you is the following scenario. We make free different graph nodes computing measurements for queries by:   1. Crag 2. WolframAlpha 3. LLM And a judge node: 4. LLM to pick a result from 1,2,3

[10:11] <disbot> <antononcube> The computations in the nodes 1,2,3 happen asynchronously.

[10:12] <disbot> <librasteve> .oO - that's is very cool

[10:12] <disbot> <antononcube> This can be an interesting experiment, actually, and a good example use case.

[10:13] *** arkiuat joined
[10:14] <disbot> <librasteve> my use case is a "handy calculator on the command line"

[10:16] <disbot> <antononcube> "LLM::Graph" is infrastructural, it should be nice and easy to use it "under the hood."

[10:18] *** arkiuat left
[10:21] *** Aedil joined
[10:25] <disbot> <librasteve> =b

[10:47] *** arkiuat joined
[10:52] *** arkiuat left
[11:08] *** arkiuat joined
[11:12] *** pierrot left
[11:12] *** pierrot joined
[11:13] *** arkiuat left
[11:16] *** apa_c left
[11:19] *** apogee_ntv left
[11:20] *** apogee_ntv joined
[11:37] *** arkiuat joined
[11:41] <disbot> <antononcube> Does “App::Crag” use LLMs, or do

[11:42] *** arkiuat left
[11:42] <disbot> <antononcube> I mean, that there is also the inversion of control possibility using the so-called LLM function calling.

[11:43] <disbot> <librasteve> okaay - never heard of that - maybe a quick way to write an LLM::Graph node

[11:45] <disbot> <antononcube> Please, see here: https://rakuforprediction.wordpress.com/2025/06/01/llm-function-calling-workflows-part-1-openai/

[11:47] <disbot> <antononcube> The idea is what LLMs can call a function running locally on your computer. That function is being described to them with some JSON schema.

[11:48] <disbot> <antononcube> Until a year ago was unreliable.Now many people say it is.

[11:48] <disbot> <librasteve> ok if you have App::Crag installed, then you can go crag '?^<elephant mass in kg>' on the command line

[11:50] <disbot> <librasteve> you get back a Physics::Measure.new(value => 6000, units => 'kg').Str

[11:50] <disbot> <librasteve> (ie its stringified to 6000kg)

[11:51] <disbot> <librasteve> maybe should go in the "ways to check the calculation output of your LLM"

[11:57] <disbot> <antononcube> That is LLM-graph way. The LLM-tool way is describe to the LLM what is the tool (run-cmd in this case) and the LLM would decide when to call it.

[11:57] <disbot> <antononcube> The LLM-graph can be seen as a generalization of

[11:59] <Voldenet> mcp is still still not strictly reliable

[11:59] <Voldenet> it's more reliable, but sometimes ollama updates just break things there

[12:00] *** arkiuat joined
[12:00] <disbot> <librasteve> please dont use App::Crag to build a satellite

[12:00] <disbot> <antononcube> No, just to build an elephant.

[12:01] <Voldenet> and hide it in the room

[12:01] <disbot> <antononcube> Well, or just measure a few…

[12:01] <Voldenet> there are problems with mcp servers still, for example destructive or non-repeatable calls get called multiple times

[12:02] <Voldenet> in non-interactive way

[12:02] <disbot> <antononcube> @Voldenet Yeah, that is the challenge with updating the LLM packages— LLM providers can’t help themselves but break their APIs little by little.

[12:03] <disbot> <librasteve> ?^<typical room volume in m3> / ?^<elephant volume in m3>

[12:03] <disbot> <librasteve> 6①

[12:04] <disbot> <antononcube> To some extent, that’s why I implemented LLM-graph. It can give you more control and let you use multiple agents to get results.

[12:04] <Voldenet> I'm not really annoyed too much about it, because the models ecosystem still in very early stages

[12:04] <disbot> <librasteve> ?^<elephant cost in US$>  => 100000USD

[12:04] *** arkiuat left
[12:05] <disbot> <antononcube> Well, I’m annoyed, because people demanded at work and expect a lot.

[12:08] <Voldenet> well, walking on water is as easy as implementing software according to spec

[12:08] <Voldenet> very easy if they're both frozen

[12:12] <disbot> <antononcube> At some point some developers want to vaporize them all.

[12:15] <disbot> <antononcube> This reminds me — I like pointing out that Agile was invented/shaped/verbilized at ski resort. I.e. around “frozen water” with clear vision of obstacles when skiing downhill.

[12:20] *** arkiuat joined
[12:21] <Voldenet> it still works when floor is lava, you just need to sprint more ;)

[12:25] *** arkiuat left
[12:31] *** crnlskn left
[12:36] *** crnlskn joined
[12:41] <disbot> <antononcube> Yeah, limited scenarios with floors. If they came up with such a methodology in the Everglades, here in Florida, I would be much more impressed.

[12:49] <disbot> <antononcube> @librasteve Can you "surface" run-cmd of "App::Crag", so it can be used in Raku sessions?

[12:51] *** apa_c joined
[12:52] *** apa_c left
[12:52] *** apa_c joined
[12:54] *** arkiuat joined
[13:00] *** apa_c left
[13:05] <Geth> ¦ raku-mode: ttn-ttn++ created pull request #62: Indent method calls when they happen on a new line

[13:05] <Geth> ¦ raku-mode: review: https://github.com/Raku/raku-mode/pull/62

[13:12] *** apa_c joined
[13:24] <disbot> <librasteve> ok if you have App::Crag installed, then you can go crag '?^<elephant mass in kg>' on the command line

[13:26] <disbot> <antononcube> I get that. But I want to use "App::Crag" functionality within Raku sessions. Having those functionalities via run or shell seems too indirect.

[13:27] <disbot> <antononcube> But, well, that is way to make a prototype of comparing "App::Crag" to other systems.

[13:51] *** apa_c left
[14:12] *** crnlskn left
[14:23] *** perryprog_ is now known as perryprog

[14:27] <disbot> <librasteve> use App::Crag; run-cmd '?^<elephant mass in kg>'; now supported (give me a couple of mins to fez upload v0.0.33)

[14:29] <disbot> <librasteve> done the fez ....

[14:30] *** apa_c joined
[15:08] *** [Coke]_ is now known as [Coke]

[15:14] *** apa_c left
[16:10] *** melezhik joined
[16:32] *** japhb left
[16:38] *** japhb joined
[16:40] *** dakkar left
[16:42] *** arkiuat left
[16:58] <disbot> <antononcube> @librasteve Cool, but can give an example of using run-cmd and/or eval-me ?

[17:03] <disbot> <librasteve> run-cmd '?^<elephant mass in kg>';

[17:03] <disbot> <antononcube> I tried that and others...

[17:04] <disbot> <antononcube> I get: > Error: X::Method::NotFound «No such method 'tell' for invocant of type 'Out'»

[17:05] <disbot> <librasteve> looks like you are in the repl - that's what you start with the crag cmd with no args (kust like going raku on the command line

[17:20] <disbot> <librasteve> assume I know nothing about jupyter - so you are either (i) in some raku code (eg in a file) that will be run via raku file.raku in which case put use App::Crag; run-cmd '?^<elephant mass in kg>'; in the file or (ii) you are at the Unix command prompt > in which case type in crag '?^<elephant mass in kg>' (like raku -e say "hi";')

[17:21] <disbot> <librasteve> alles klar?

[17:27] <tbrowder> hi, any user here ever hear of multi-os app Ventoy used to created live USB's

[17:28] <tbrowder> if so, is it known to be save for use with private data and free from viruses?

[17:28] <tbrowder> *safe

[17:29] *** human-blip left
[17:31] *** human-blip joined
[17:55] <disbot> <antononcube> @librasteve This is what I got so far:

[17:55] <disbot> <antononcube> https://cdn.discordapp.com/attachments/633753286209699870/1412858612530937966/Screenshot_2025-09-03_at_1.55.26_PM.png?ex=68b9d29e&is=68b8811e&hm=d42d1949596d722c31afad8e4598c506a22aad8cf0b863f5511ad89481f67cdf&

[17:56] <disbot> <antononcube> I cannot run crag in Jupyter easily and / or  asynchronously.

[17:59] <disbot> <librasteve> aha

[18:01] <disbot> <librasteve> there are 3 raku prefixes in play - ^<foo>, ?<bar> and ?^<baz>

[18:02] <disbot> <librasteve> ^<foo> means ^<value unit [±error]>

[18:03] <disbot> <librasteve> ?<bar> means ?<some text goes to Gemini via LLM-DWIM>

[18:05] <disbot> <librasteve> ?^<baz> means ?^<some text to llm 'in units'>

[18:05] <disbot> <librasteve> note thats query caret ?^

[18:06] <disbot> <librasteve> btw all three are raku prefixes so any quoting format '', "" <> will work

[18:06] <disbot> <librasteve> use "" if you want to interppolate $vars of course

[18:07] <disbot> <librasteve> (<> is adjusted to join the words with a space to make a string)

[18:08] <disbot> <librasteve> in the case of baz - there is some prompt engineering and the in units part at the end is both passed to the LLM and regexed out to make the units part, so has to be valid

[18:30] *** melezhik left
[18:31] *** librasteve_ joined
[18:33] *** phogg left
[18:44] <disbot> <antononcube> The LLM extension is not that interesting using the LLM-graph POV. It is better to know when to use "App::Crag", with what kind of queries. (I.e. a certain Raku code.)

[18:44] <disbot> <antononcube> I am not sure to what degree "Physics::*" packages know about elephants and other animals.

[18:46] *** phogg joined
[18:46] <disbot> <antononcube> Again, this is the perspective of using LLM-tools -- LLMs do the the LLM-ing and deterministic computations are done locally.

[20:02] *** apac joined
[20:33] *** Aedil left
[20:48] <disbot> <librasteve> did you get ?^ to run?

[20:52] <Geth> ¦ Pod-To-HTML/main: 127a900228 | (Steve Dondley)++ (committed using GitHub Web editor) | README.md

[20:52] <Geth> ¦ Pod-To-HTML/main: correct module name for install instructions

[20:52] <Geth> ¦ Pod-To-HTML/main: review: https://github.com/raku-community-modules/Pod-To-HTML/commit/127a900228

[20:52] <Geth> ¦ Pod-To-HTML/main: c2204db2f5 | librasteve++ (committed using GitHub Web editor) | README.md

[20:52] <Geth> ¦ Pod-To-HTML/main: Merge pull request #94 from sdondley/master

[20:52] <Geth> ¦ Pod-To-HTML/main: 

[20:52] <Geth> ¦ Pod-To-HTML/main: correct module name for install instructions

[20:52] <Geth> ¦ Pod-To-HTML/main: review: https://github.com/raku-community-modules/Pod-To-HTML/commit/c2204db2f5

[20:58] <disbot> <antononcube> @librasteve Not in Jupyter -- used run. On the command line -- yes.

[21:02] <disbot> <librasteve> the point is this ?^<some text to llm 'in units'> ... ie dual parsing of LLM query ... one to condition the Unit::Parser, the other to shape the LLM response (right?)

[21:02] <disbot> <librasteve> so to ingest the LLM response to an analytic codebase

[21:06] <disbot> <antononcube> Ok. But I am more interested in the following: - Make a classifier for recognizing "App::Crag" deterministic computations input - Decide should "App::Crag" be used or not - If not just use Wolfram|Alpha and LLM - Or, use Wolfram|Alpha and LLM always - Have a LLM judge to choose the final answer

[21:06] <disbot> <antononcube> The classifier can be made with the inputs in "App::Crag" test files.

[21:13] *** LainIwakura joined
[21:16] *** LainIwakura left
[21:28] *** jgaz joined
[21:34] *** LainIwakura joined
[22:05] *** phogg` joined
[22:05] *** japhb_ joined
[22:10] *** phogg left
[22:10] *** japhb left
[22:12] *** LainIwakura left
[22:18] *** Sgeo joined
[22:40] *** LainIwakura joined
[22:52] *** LainIwakura left
[23:24] *** phogg` is now known as phogg

[23:47] *** lichtkind left
