[02:02] *** guifa joined
[04:03] *** guifa left
[07:21] <ab5tract> [Coke]: I feel like adding an sqlite database that gets a row per tested module would be helpful. Or it could add only failures. Either way, restarting a blin job would be trivial if this were in place, right?

[07:21] <ab5tract> just spitballing ideas for the next version

[07:24] <ab5tract> melezhik: these changes to whateverable are probably necessary too https://github.com/Raku/whateverable/compare/main...ab5tract:whateverable:main

[07:24] <tellable6> ab5tract, I'll pass your message to melezhik

[07:45] *** melezhik joined
[07:50] <melezhik> [Coke]: I am going to make a proposal today how I see further progress evolution of blin . In two words I would replace blin by sparrow and run tests in distributed cluster , this would give us a simplicity and scalability . I think I am ready to share a concept , I may create a ticket in blin2 repository for discussion .

[07:50] <tellable6> 2025-10-22T07:24:06Z #raku-dev <ab5tract> melezhik: these changes to whateverable are probably necessary too https://github.com/Raku/whateverable/compare/main...ab5tract:whateverable:main

[07:50] *** lizmat left
[07:51] <melezhik> ugexe: I get that , the uniformity of tested environment is achievable if we deliver test boxes as docker containers , I will reflect it in my proposal .

[07:53] <melezhik> lizmat: your eco-system-cache could be also useful if we pack it inside docker containers which would run actually tests

[07:53] <tellable6> melezhik, I'll pass your message to lizmat

[07:56] <melezhik> ab5tract: thanks for that.  Again on my taste blin is great when run on single host mode but abit over engineered if  to use it in distributed environment, I would like to have simplicity and scalability, I will reflect it in my proposal

[08:26] *** melezhik_ joined
[08:27] <melezhik_> [Coke]: https://github.com/coke/blin2/issues/1 , I wonder if we can give access to all who is interested , so we can discuss or I van just put it into sort of public GitHub gist 

[08:38] <melezhik_> anyways - https://gist.github.com/melezhik/1fee7ae372c6c18c146ab5740aab3fd5 cc [Coke]: ugexe: lizmat: ab5tract:  

[08:42] <melezhik_> solubility -> scalability ))

[08:45] *** melezhik_ left
[11:05] *** melezhik left
[11:06] *** patrickb left
[11:34] *** melezhik joined
[11:36] <melezhik> .

[11:37] *** librasteve_ left
[11:42] <[Tux]> Rakudo v2025.10-22-ge1b3f9277 (v6.d) on MoarVM 2025.10-8-g36fcd3d1f

[11:42] <[Tux]> csv-test-xs         0.014 -  0.014

[11:42] <[Tux]> csv-test-xs-20      0.114 -  0.115

[11:42] <[Tux]> csv-ip5xs           0.264 -  0.267

[11:42] <[Tux]> test-t --race       0.279 -  0.285

[11:42] <[Tux]> test-t              0.455 -  0.464

[11:42] <[Tux]> csv-ip5xs-20        1.096 -  1.143

[11:42] <[Tux]> csv-parser          1.145 -  1.157

[11:42] <[Tux]> test-t-20 --race    1.412 -  1.429

[11:42] <[Tux]> test                1.872 -  1.913

[11:42] <[Tux]> test-t-20           5.820 -  5.985

[11:42] <[Tux]> https://tux.nl/Talks/CSV6/speed4-20.html / https://tux.nl/Talks/CSV6/speed4.html https://tux.nl/Talks/CSV6/speed.log

[11:48] *** lizmat joined
[11:56] *** lizmat left
[12:19] *** patrickb joined
[12:27] *** lizmat joined
[12:28] *** lizmat left
[12:29] *** lizmat_ joined
[12:29] *** lizmat_ left
[12:45] *** rba left
[13:00] *** rba joined
[13:15] *** lizmat joined
[13:16] *** lizmat_ joined
[13:16] *** lizmat left
[13:19] *** lizmat_ left
[13:32] *** librasteve_ joined
[13:55] *** melezhik left
[14:10] <[Coke]> Anyone wants access, let me know.

[14:12] *** melezhik_ joined
[14:48] *** melezhik_ left
[14:48] *** melezhik_ joined
[15:02] *** finanalyst joined
[15:41] *** melezhik_ left
[15:53] *** finanalyst left
[15:56] *** melezhik_ joined
[15:57] <ugexe> i think if you wanted to do a more cpan testers type thing that we'd be able to group the results in some way, potentially by environment / container / author etc. That could allow one to view only the results of blin runs using a specific container with a known environment (a blin view) but also the results of installs on arbitrary environments (a cpan testers matrix like view)

[15:59] <ugexe> for example it can be useful to see the test failures of some relatively unused os/arch, but those failures aren't important in the context of what blin wishes to achieve (and indeed since it is an unknown environment it could very well be user error)

[16:01] *** melezhik_ left
[16:02] *** melezhik_ joined
[16:07] <[Coke]> And right now we're only testing one arch. Even adding a common second one like windows would be amazing.

[16:18] *** timo2 is now known as timo

[16:24] *** melezhik_ left
[16:24] *** melezhik_ joined
[16:27] *** melezhik_ left
[16:28] *** melezhik_ joined
[16:29] <melezhik_> .

[16:31] <melezhik_> ok. looks like I chose cpan testers way and it became misleading , sorry for that, what I really meant, main idea proposed by me is to instead of run all tests on single host using blin, I would rather see it as running tests on multiple hosts , in distributed way  as cpan testers do, but not because we want to test on many envs and collect stat. no, the agent docker image will be the same

[16:32] <melezhik_> across all agents, this will ensure uniformity 

[16:32] <melezhik_> but yes, generally speaking we can have eventually many groups of agents for different archs, if we really need it

[16:33] <ugexe> that seems more along the lines of users providing agents to the pool

[16:33] <melezhik_> however the distribute nature of tests is the key, as it scale and managed much better 

[16:33] <melezhik_> ugexe: exactly

[16:33] <melezhik_> like azure pipeline agents, or gitlab workers if you are familiar with that

[16:34] <melezhik_> the same PULL pattern

[16:35] <melezhik_> with job orchestrator blend a central node, but it's CPU/memory pressure is minimal as it does not actually tests, only orchestrate them across cluster . get results and create all sort of reports

[16:37] *** melezhik_ left
[16:37] *** melezhik_ joined
[16:37] <melezhik_> also say we test against pre release rakudo version, SHA whatever able, and we have 500 modules or even whole eco system, from the start agents have 0 modules installed , but with time the more zef install Foo happend on them the bigger "cache" they have 

[16:38] <melezhik_> this will eventually makes time zef install Foo for other modules smaller and smaller

[16:39] <melezhik_> I can it eventual performance increase 

[16:39] <melezhik_> we don't even need to think about DAG depth first traversal, installing modules without dependencies first as blin does

[16:39] <melezhik_> the strategy becomes much simpler

[16:42] <melezhik_> say we have 10 agents, and we have 500 modules or so, we can just install 5 modules at one shot on every agents, and I guess within about 30 minutes we may end every agent have a decent installed modules "cache"

[16:44] <melezhik_> and If we are lucky enough and randomly shake list of modules and decencies tree for all 500 modules is well balanced we have decent chance to have a good time of testing all 500 modules 

[16:45] <melezhik_> this is of course only a speculation, but I would like to check it by creating within few days a simple 2-3 agents 1 orchestrator demo and test against 50-100 modules to check this theory 

[16:46] <melezhik_> I can it -> I call it

[16:46] <melezhik_> with job orchestrator blend -> with job orchestrator being 

[16:48] <melezhik_> decencies tree -> dependency tree 

[16:48] <melezhik_> sorry for typos

[16:51] <melezhik_> another idea derived from this architecture is incremental testing, when we can always stop testing ( due to agents become offline or whatever reasons ) and then after while resume testing

[16:52] <melezhik_> as agents have state in a sense of installed modules, we won't start from the scratch , also orchestrator will keep track of already checked modules and won't ask agents to test them again

[16:52] <melezhik_> it's quite convenient 

[16:53] <melezhik_> of course cache needs to be build again for every band new rakudo SHA version, but for old versions it will be kept forever on agents ( via installed modules ) and on orchestrator ( via records of checked modules )

[16:54] <[Coke]> Can probably also get rid of old caches - if your cache is for a tagged release from 2 months ago, probably done.

[16:54] <melezhik_> cache could be cleared if required ( all agents containers should just restart )

[16:54] <[Coke]> if it's from a sha1 commit even a week ago, it's probably done

[16:55] <[Coke]> one nice thing about managing outside of a restart is that if you've already started on the latest round.

[16:55] <timo> it would be good if we could make installed modules from the cache "not show up" when they are not actually in the (transitive) dependencies of the module we actually want to test

[16:55] <[Coke]> (but yah, that isn't a hard requirement.)

[16:56] <[Coke]> I think blin does this by downloading the module but installing it as needed into a separate location each time, then wiping when done.

[16:56] <melezhik_> yep, or to make cache persistent  through container restarts, need to use  docker volumes to mount file system for zef install into container 

[16:58] <melezhik_> cache/cachless could be configurable as well, depending on how we need it, but main idea containers allow do it via volume mounts

[16:58] <timo> we could also turn building the dependencies into a "docker build" / buildah like thing where we start with the base container image, adding JSON::Fast is the first step, then adding the next dependency and so on

[16:59] <timo> if we sort things properly, we can then re-use these intermediate layers

[16:59] <timo> but that's DAG traversal again

[17:00] <melezhik_> timo: my view - agent docker image is only for agent usage, it won't have anything related modules being tested

[17:01] <melezhik_> and we test modules under completely different rakudo coming from whatever able archive 

[17:07] *** melezhik_ left
[17:07] *** melezhik_ joined
[17:13] *** melezhik_ left
[17:14] *** melezhik_ joined
[17:21] *** melezhik_ left
[17:21] *** melezhik_ joined
[17:24] *** melezhik_ left
[17:39] *** melezhik_ joined
[17:48] *** melezhik_ left
[17:51] *** melezhik_ joined
[17:59] *** melezhik_ left
[17:59] *** melezhik_ joined
[18:15] *** melezhik_ left
[18:16] *** melezhik_ joined
[18:21] *** melezhik_ left
[18:21] *** melezhik_ joined
[18:26] *** melezhik_ left
[18:27] *** melezhik_ joined
[18:32] *** melezhik_ left
[18:32] *** melezhik_ joined
[18:38] *** melezhik_ left
[18:38] *** melezhik_ joined
[18:45] *** melezhik_ left
[18:45] *** melezhik_ joined
[18:52] *** melezhik_ left
[18:52] *** melezhik_ joined
[18:57] *** melezhik_ left
[18:57] *** melezhik_ joined
[19:02] *** melezhik_ left
[19:28] <librasteve_> cwiggins: good luck … I did something like this for a clean AWS Ubuntu machine … the code is here https://github.com/librasteve/raku-CLI-AWS-EC2-Simple/blob/92d40b902f58049398f9b83bcca8143de3035691/lib/CLI/AWS/EC2-Simple.rakumod#L314

[19:28] <tellable6> librasteve_, I'll pass your message to cwiggins123

[19:29] <librasteve_> cwiggins: to get that to work I made a template perl script and then (since this is a module) copy that over on the zef module install (see the Build.rakumod for how to do that if you want to make a raku module)

[19:29] <tellable6> librasteve_, I'll pass your message to cwiggins123

[19:31] <librasteve_> cwiggins: if I did it again, I would likely look at the Sparky (or is it Sparrowdo) modules since that has the ssh boostrap done in a more “standardized” way

[19:31] <tellable6> librasteve_, I'll pass your message to cwiggins123

[22:25] *** Voldenet left
[22:27] *** Voldenet joined
[23:41] *** lizmat joined
[23:44] *** lizmat left
