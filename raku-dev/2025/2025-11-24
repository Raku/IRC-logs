[00:00] <[Coke]> I have an agent running here, can see a single agent under "recent builds" in the UI, but it doesn't appear to be doing anything.

[05:26] *** melezhik_ joined
[05:33] *** melezhik_ left
[05:34] *** melezhik_ joined
[05:43] *** melezhik_ left
[05:43] *** melezhik_ joined
[05:50] *** melezhik_ left
[05:50] *** melezhik_ joined
[05:57] *** melezhik_ left
[05:57] *** melezhik_ joined
[06:04] *** melezhik_ left
[06:04] *** melezhik_ joined
[06:11] *** melezhik_ left
[06:11] *** melezhik_ joined
[06:23] *** melezhik_ left
[06:23] *** melezhik_ joined
[06:32] *** melezhik_ left
[06:32] *** melezhik_ joined
[06:39] *** melezhik_ left
[06:39] *** melezhik_ joined
[06:46] *** melezhik_ left
[06:46] *** melezhik_ joined
[06:58] *** melezhik_ left
[06:58] *** melezhik_ joined
[07:04] *** melezhik_ left
[07:05] *** melezhik_ joined
[07:11] *** melezhik_ left
[07:12] *** melezhik_ joined
[07:19] *** melezhik_ left
[07:19] *** melezhik_ joined
[07:28] *** melezhik_ left
[07:28] *** melezhik_ joined
[07:38] *** melezhik_ left
[07:38] *** melezhik_ joined
[07:52] *** melezhik_ left
[07:52] *** melezhik_ joined
[08:02] *** melezhik_ left
[08:02] *** melezhik_ joined
[08:07] *** melezhik joined
[08:07] <melezhik> .

[08:07] <tellable6> 2025-11-23T23:59:13Z #raku-dev <[Coke]> melezhik - agent instructions say to use 127.0.0.1 in the browser, but here that fails and I need literal 'localhost'

[08:09] *** melezhik_ left
[08:09] *** melezhik_ joined
[08:10] <melezhik> [Coke]: o10r seems to stopped yesterday , I am restarting it now . I guess you need to restart your agent and it will start receiving payload . Localhost or 127.0.0.1 I don‚Äôt know why but on some OS one need to choose one or another . 127.0.0.1 usually works for me. Technically container forward 4000 port to default host network interface port 4000

[08:14] <melezhik> .

[08:21] *** melezhik_ left
[08:21] *** melezhik_ joined
[08:28] *** melezhik_ left
[08:28] *** melezhik_ joined
[08:38] *** melezhik_ left
[08:38] *** melezhik_ joined
[08:50] *** melezhik_ left
[08:50] *** melezhik_ joined
[08:58] *** melezhik_ left
[08:58] *** melezhik_ joined
[09:12] *** melezhik_ left
[09:12] *** melezhik_ joined
[09:19] *** melezhik_ left
[09:19] *** melezhik_ joined
[09:30] *** melezhik_ left
[09:30] *** melezhik_ joined
[09:42] <disbot5> <melezhik.> ^^ ab5tract:

[09:51] *** melezhik_ left
[09:51] *** melezhik_ joined
[10:00] *** melezhik_ left
[10:01] *** melezhik_ joined
[10:17] *** melezhik_ left
[10:17] *** melezhik_ joined
[10:18] *** tellable6 left
[10:18] *** greppable6 left
[10:18] *** evalable6 left
[10:19] <[Tux]> Rakudo v2025.11-29-g3e9f25272 (v6.d) on MoarVM 2025.11-3-g17b39b0c9

[10:19] <[Tux]> csv-test-xs         0.014 -  0.014

[10:19] <[Tux]> csv-test-xs-20      0.115 -  0.116

[10:19] <[Tux]> csv-ip5xs           0.265 -  0.270

[10:19] <[Tux]> test-t --race       0.281 -  0.283

[10:19] <[Tux]> test-t              0.451 -  0.453

[10:19] <[Tux]> csv-parser          1.100 -  1.107

[10:19] <[Tux]> csv-ip5xs-20        1.121 -  1.139

[10:19] <[Tux]> test-t-20 --race    1.415 -  1.430

[10:19] <[Tux]> test                1.880 -  1.885

[10:19] <[Tux]> test-t-20           5.669 -  5.795

[10:19] <[Tux]> https://tux.nl/Talks/CSV6/speed4-20.html / https://tux.nl/Talks/CSV6/speed4.html https://tux.nl/Talks/CSV6/speed.log

[10:28] *** melezhik_ left
[10:29] *** melezhik_ joined
[10:36] *** melezhik_ left
[10:36] *** melezhik_ joined
[10:45] *** melezhik_ left
[10:45] *** melezhik_ joined
[10:54] *** melezhik_ left
[10:54] *** melezhik_ joined
[11:01] *** melezhik_ left
[11:01] *** melezhik_ joined
[11:18] *** melezhik_ left
[11:18] *** melezhik_ joined
[11:24] *** melezhik_ left
[11:24] *** melezhik_ joined
[11:34] *** melezhik_ left
[11:34] *** melezhik_ joined
[11:41] *** melezhik_ left
[11:41] *** melezhik_ joined
[11:43] *** tellable6 joined
[11:43] *** greppable6 joined
[11:43] *** evalable6 joined
[11:44] *** librasteve_ joined
[11:50] *** melezhik_ left
[11:50] *** melezhik_ joined
[11:58] *** melezhik_ left
[11:58] *** melezhik_ joined
[12:07] *** melezhik_ left
[12:07] *** melezhik_ joined
[12:14] *** melezhik_ left
[12:14] *** melezhik_ joined
[12:34] *** melezhik left
[12:44] *** melezhik_ left
[12:44] *** melezhik_ joined
[12:51] *** melezhik_ left
[12:51] *** melezhik_ joined
[13:10] *** melezhik_ left
[13:10] *** melezhik_ joined
[13:17] *** melezhik_ left
[13:17] *** melezhik_ joined
[13:26] <[Coke]> restarted, getting "jobs-run-cnt: 0, max-threads: 4

[13:26] <[Coke]> every 5 s

[13:27] <[Coke]> BUILD SUMMARY STATE: FAILED

[13:32] *** melezhik_ left
[13:32] *** melezhik_ joined
[13:48] *** melezhik_ left
[13:49] *** melezhik_ joined
[14:00] *** melezhik_ left
[14:00] *** melezhik_ joined
[14:07] *** melezhik_ left
[14:07] *** melezhik_ joined
[14:14] *** melezhik_ left
[14:14] *** melezhik_ joined
[14:15] *** melezhik joined
[14:29] *** melezhik_ left
[14:29] *** melezhik_ joined
[14:36] *** melezhik_ left
[14:36] *** melezhik_ joined
[14:52] *** melezhik_ left
[14:52] *** melezhik_ joined
[14:54] <melezhik> [Coke]: ab5tract o10r is ready to distribute jobs

[15:02] *** melezhik_ left
[15:02] *** melezhik_ joined
[15:09] *** melezhik_ left
[15:09] *** melezhik_ joined
[15:15] <[Coke]> running agent. again, looks like getting 0 jobs every 5s

[15:15] <[Coke]> BRW_AGENT_NAME_PREFIX=cokebot

[15:15] <[Coke]> ah, got "installing zstd"

[15:16] <[Coke]> ok, doing stuff.

[15:16] <[Coke]> melezhik++

[15:17] <[Coke]> 15:16:48 :: mykrcijfhszdgubntloa.130

[15:17] <[Coke]>  ?

[15:17] <[Coke]> is that basically a custom GUID?

[15:21] *** melezhik_ left
[15:21] *** melezhik_ joined
[15:31] *** melezhik_ left
[15:31] *** melezhik_ joined
[15:43] <melezhik_> ping from agent: cokebot-42925061, version: 0.0.20, jobs-run-cnt: 4, max-threads: 4

[15:43] <melezhik_> [Coke]: your agent has 4 threads, and they are all busy right now

[15:43] <melezhik_> depending on your CPU availability you may want to increase threads max num

[15:44] <melezhik_> on recent builds on your agent UI you should see running agent.job jobs

[15:45] <melezhik_> agent.job_number to be accurate 

[15:46] <melezhik_> "ah, got "installing zstd"" - yep agent first install rakudo from whatever able and then it starts spawning agent.job_number jobs

[15:59] *** melezhik_ left
[15:59] *** melezhik_ joined
[16:08] <melezhik_> did you stop your agent ? don't see any pings from it?

[16:15] <[Coke]> nope, it's still running

[16:15] <[Coke]> seeing a lot of "skip job: agent is busy"

[16:15] <[Coke]> I don't think it's telling me *what* it's running, but I see 4 copies of sparky-runner.raku

[16:16] <[Coke]> I assume that while I'm busy, I don't reach back with a heartbeat or anything?

[16:16] <[Coke]> (might want a heartbeat that is much less frequent so you know I'm not dead?)

[16:23] *** melezhik_ left
[16:23] *** melezhik_ joined
[16:29] <melezhik_> ok, that agent name is printed on your main agent job ?

[16:35] <[Coke]> Looks like http://localhost:4000/ is now dead

[16:35] <[Coke]> the docker container is still running.

[16:35] <[Coke]> 2025-11-24T16:29:44.670395Z --- sparkyd: parse sparky job yaml config from: /root/.sparky/projects/agent/sparky.yaml

[16:35] <[Coke]> 2025-11-24T16:29:44.675814Z --- [agent] neither crontab  nor scm setup found, consider manual start, SKIP ...

[16:38] <lizmat> timo: almost got the "call original token if mixed in token doesn't match" logic working: https://gist.github.com/lizmat/eb3786140bc933248fc855d804dbcedc

[16:39] <lizmat> problem is that a multi method is not installed properly in the grammar, and the non-multi *is* installed, but then nextcallee logic fails

[16:45] <melezhik_> if docker container is running, then http://localhost:4000/ should be accessible

[16:46] <melezhik_> if you bash to container, what do you get by `tail -f ~/.sparky/sparky-web.log` ?

[16:47] <melezhik_> and `ps uax|grep sparky-web` also, does it give anything ?

[16:48] <melezhik_> it could be a case if container does not have enough RAM , then kernel kill some process by OOM

[16:48] <melezhik_> but it's hard to say with any details 

[16:48] <librasteve_> notable6: weekly

[16:48] <tellable6> librasteve_, I'll pass your message to notable6

[16:48] <librasteve_> hmmm - maybe I am holding it wrong

[16:49] <melezhik_> last heartbeat from  cokebot-42925061 was 1 hour and 17 minutes ago ...

[16:50] <melezhik_> "but I see 4 copies of sparky-runner.raku" - this just means you have ( had ) 4 agent.job jobs executed in parallel which is correct 

[17:00] *** greppable6 left
[17:00] *** quotable6 left
[17:00] *** nativecallable6_ left
[17:00] *** tellable6 left
[17:00] *** unicodable6 left
[17:00] *** bloatable6__ left
[17:00] *** evalable6 left
[17:00] *** coverable6__ left
[17:00] *** releasable6 left
[17:00] *** linkable6 left
[17:00] *** sourceable6 left
[17:02] *** unicodable6 joined
[17:02] *** greppable6 joined
[17:02] *** bisectable6 joined
[17:02] *** sourceable6 joined
[17:02] *** quotable6 joined
[17:03] *** nativecallable6 joined
[17:03] *** shareable6 joined
[17:03] *** coverable6 joined
[17:03] *** benchable6 joined
[17:03] *** notable6 joined
[17:03] *** bloatable6 joined
[17:04] *** linkable6 joined
[17:04] *** tellable6 joined
[17:04] *** releasable6 joined
[17:04] *** evalable6 joined
[17:04] *** committable6 joined
[17:06] <[Coke]> sparky-web isn't there.

[17:06] <lizmat> ugexe tonyo I've uploaded a Sigil::Nogil 1.2 to fez, but it still hasn't shown up on zef

[17:06] <[Coke]> there is one that starts: "/bin/sh -c cd /opt/sparky && sparman"

[17:06] *** melezhik_ left
[17:06] <lizmat> after 4+ hours

[17:07] <[Coke]> put-job-file: create job file - /root/.sparky/projects/agent.job_1763998057/.files/hscxdmetkrfgaubpnwio.130/Concurrent::File::Find.log.txt ...

[17:07] <[Coke]> ws: send data to client: 20 lines

[17:07] <[Coke]> ws: send data to client: 28 lines

[17:07] <[Coke]> ^^ last 3 lines from that log file, which was last updated about 90 minutes ago

[17:08] <[Coke]> so my zef issue on windows 11 started with the 2025.08 release. Worked fine in 2025.05

[17:10] <[Coke]> er, it's specifically when using it with rakubrew, it's not a zef issue per se.

[17:11] <[Coke]> so my guess is that's when we pushed the changes to the wrapper scripts.

[17:19] <lizmat> patrickb ^^

[17:20] <lizmat> [Coke]: I seem to recall needing to remove any .bat extension handling around that time, could that be it ?

[17:21] <melezhik> Ok. Can I get data from agent main job file ? Wgst does it say ?

[17:21] <[Coke]> fwiw, there is no zef.bat in that folder.

[17:22] <[Coke]> what is the "main agent job file"?

[17:22] <lizmat> ugexe tonyo looks like the last module uploaded is 22 hours ago

[17:23] <melezhik> And am I correct that ps aux | grep sparky-web does not produce anything ?

[17:23] <melezhik> I mean agent job , not agent.job job

[17:24] <melezhik> Oh, my irc is mixing messages order

[17:27] <melezhik> Ahh ok, finally got it, these were logs from sparky web

[17:44] *** melezhik_ joined
[17:45] <librasteve_> https://rakudoweekly.blog/2025/11/24/2025-47-advent-calling/

[17:49] *** melezhik_ left
[17:52] <lizmat> liibrasteve++

[18:04] <ab5tract> m: role R[::T] { my class A is Array[T] {}; has A $.a = A.new }; dd R[Int].new.a.HOW

[18:04] <camelia> rakudo-moar 3e9f25272: OUTPUT: ¬´Perl6::Metamodel::ClassHOW+{<anon>}+{<anon>}.new‚ê§¬ª

[18:04] <ab5tract> with RakuAST, we get the following error: # Type check failed in assignment to $!a; expected R::A[Int] but got R::A (Array[T].new())

[18:05] <ab5tract> So it gets the right type for the container but gets the wrong type for the initializer

[18:05] <ab5tract> üôÉ

[18:05] <ab5tract> :cloun_shoes:

[18:06] <ab5tract> clown shoes is how I feel, not a statement on the code or the problem

[18:07] <ab5tract> nine: I wonder if you have any thoughts given the above?

[18:59] *** melezhik_ joined
[19:00] <melezhik_> .

[19:04] <[Coke]> melezhik: just restarted the agent

[19:04] <melezhik> Ok. It‚Äôd be interesting to see logs

[19:06] <[Coke]> which?

[19:12] <melezhik_> ah, ok, sorry, thought this was from ab5tract )

[19:12] <melezhik_> ping from agent: cokebot-26318746, version: 0.0.20, jobs-run-cnt: 4, max-threads: 4

[19:12] <melezhik_> do see pings now

[19:15] *** lucs_ is now known as lucs

[19:19] <melezhik_> reports arriving as well - http://brw.sparrowhub.io/report/cokebot-26318746.job_1764011656.report/341

[19:32] *** melezhik_ left
[19:32] <melezhik> [Coke]: if there are any red builds now on recent build page ?

[19:33] <melezhik> Oh, reports started coming again

[19:33] <melezhik> Anyway it‚Äôd be interesting to know if there are any red ones now, or they all are green ‚Ä¶

[19:37] <melezhik> It‚Äôs still suspicious that there are not so many reports from coke-agent , but it‚Äôs hard to say what‚Äôs going on without having recent builds info

[19:46] <melezhik> Ok. Now I see that coke-agent does not send any reports , only pings, similar to the issue we had before with ab5tract agent as well. Again it‚Äôd be extremely valuable to 1) get a screenshot of recent builds page ( i expect many red ones there ) 2) go to any agent.job_number job and get full report via UI

[19:50] <melezhik> And btw I do see that agent now has some jobs https://usercontent.irccloud-cdn.com/file/xnsFSBew/1764013689.JPG

[19:51] <melezhik> But they just fail to send results back to o10r

[19:52] <melezhik> Technically code either stopped before this line - https://github.com/melezhik/brownie/blob/12f608f41bc98e0404599c45277013d14d50f08a/agent/sparrowfile#L232 or data fail to transfer over network

[19:55] <[Coke]> melezhik: no, there is now, as there was before, a single "last build" still in running state

[19:56] <ab5tract> melezhik: I think reporting on this should be somewhat automatic

[19:56] <ab5tract> at least a script that can upload the logs to the o10r or something

[19:57] <[Coke]> here's the full dump of the log from the UI: https://gist.github.com/coke/12285e3c11644749bbb76c1dc648f0bf

[19:58] <melezhik> Ok , this is main agent job, thanks , can you please provide full report from any failed agent.job job from recent builds page ?

[19:58] <[Coke]> the docker run output ended with "BUILD SUMMARY" "SATE: FAILED" "PROJECT: agent.job_1764013995" "CONFIG: { }"

[19:58] <[Coke]> I have never seen one of those jobs.

[19:58] <melezhik> You should see them on recent builds page

[19:59] <melezhik> If you go to UI

[19:59] <[Coke]> at localhost:4000, under projects, single project "agent" last build 1 state "running"

[19:59] <melezhik> And click on ‚Äúrecent builds‚Äù link on the top

[19:59] <melezhik> ‚ÄúRecent builds‚Äù

[19:59] <[Coke]> ahhh

[19:59] <[Coke]> that's confusing.

[20:00] <[Coke]> 4 running, many failed, 8 succeed, 1 running

[20:00] <[Coke]> (where the one last running is the agent itself again)

[20:01] <[Coke]> docker output now includes a bunch of 2025-11-24T20:00:19.787148Z --- [agent] neither crontab  nor scm setup found, consider manual start, SKIP ...

[20:01] <melezhik> http://127.0.0.1:4000/builds

[20:01] <melezhik> Please don‚Äôt look at docket output , use Ui

[20:01] <[Coke]> again, 127.0.0.1 does not work here, and I think that's the problem.

[20:02] <[Coke]> because in the "running" job, I see:>>> send request: PUT job file to http://127.0.0.1:4000/file/project/agent.job_1764014252/job/qzkgntjyeohvsambcxur.130/filename/Acme::‡≤†_‡≤†.log.txt

[20:02] <melezhik> Do you use laptop of phone ? On phone browser top menu is not shown

[20:02] <[Coke]> >>> (599 recieved) http retry: #01

[20:02] <[Coke]> >>> (599 recieved) http retry: #02

[20:02] <[Coke]> >>> (599 recieved) http retry: #03

[20:02] <[Coke]> also 19:59:32 :: Cannot create a Zef::Distribution from non-existent path: /tmp/.zef.1764014365.76134/Acme%3A%3A‡≤†_‡≤†%3Aver%3C0.0.1%3E%3Aauth%3Ccpan%3AELIZABETH%3E.tar.gz/Acme-\340\262\240_\340\262\240-0.0.1/META6.json

[20:02] <[Coke]> https://gist.github.com/coke/5f7bc95bf121f9a2d89cff246fb03c53

[20:03] <melezhik> Ok, this is what I needed

[20:03] <[Coke]> If it helps: I'm running this on my mac.

[20:04] <melezhik> >>> send request: PUT job file to http://127.0.0.1:4000

[20:04] <[Coke]> I assumed that if I'm docker, I'm good. (I did see some instances of "rosetta" running somewhere)

[20:04] <melezhik> This is not going to work on your env ))

[20:05] <[Coke]> that's from inside the container, right? let me try a curl from inside.

[20:05] <melezhik> Looks like 127.0.0.1 is not resolved from within your docker container

[20:05] <melezhik> And I have no idea why ))

[20:06] <melezhik> This explains why reports are not send back to o10r, as test job fails in the end and never reach the mentioned code line

[20:06] <melezhik> Line of code

[20:08] <melezhik> Hold on , it‚Äôs not that simple

[20:10] <[Coke]> curl to http://127.0.0.1:4000/ and http://localhost:4000/ both fail from inside the container.

[20:10] *** finanalyst joined
[20:10] <melezhik> I guess it‚Äôs just at some points http requests from within docker to agent ui itself 127.0.0.1:4000 stop to work , however they succeed for some time , it‚Äôs probably some docker related weirdness , I don‚Äôt know what to say ‚Ä¶

[20:10] <[Coke]> ps auxww | grep -i sparky | wc -l # 29 lines

[20:10] <melezhik> Yes it fails NOW

[20:11] <[Coke]> what's the process serving out port 4000 inside the container?

[20:11] <melezhik> But I guess it succeeded at least several time ms before

[20:11] <melezhik> This agent UI/web server , it serves on 127.0.0.1:4000 within agent

[20:12] <melezhik> Technically this is sparky web server

[20:12] <melezhik> You can reach it from within host machine via localhost:4000

[20:13] <melezhik> But sparky job itself can‚Äôt reach it from within container by 127.0.0.1:4000 which is strange

[20:14] <[Coke]> ... if I do a ps, what's the name of the process?

[20:14] <[Coke]> it's now not responding from my laptop.

[20:14] <[Coke]> so I'm assuming it's dead.

[20:14] <[Coke]> so I'm guessing it's not a networking issue, but something else.

[20:15] <melezhik> Because for example in the beginning  of the log you send me some requests from within container to 127.0.0.1:4000 succeed , for example this line ‚Äúsend request: PUT job file to http://127.0.0.1:4000/file/project/agent.job_1764014252/job/qzkgntjyeohvsambcxur.130/filename/rakuenv.txt‚Äù

[20:16] <melezhik> The name of the process for sparky web server is ‚Äúps aux|grep sparky-web"

[20:16] <melezhik> 599 errors means the port is not served by

[20:16] <[Coke]> ok, yup, not running

[20:17] <melezhik> That may mean that at some point something ( kernel ? ) kills the process , maybe by OOM

[20:17] <[Coke]> what folder is this stuff running out of in the container?

[20:17] <melezhik> You may run dmesg or grep syslog messages from within container

[20:18] <melezhik> Folder ?

[20:18] <[Coke]> dmesg: read kernel buffer failed: Operation not permitted

[20:18] <[Coke]> where does "sparky-web" live in the container? 

[20:18] <melezhik> Ah ok

[20:20] <melezhik> "/opt/sparky/bin/sparky-web" I guess

[20:22] <[Coke]> (just found that with /proc also, thanks)

[20:22] <[Coke]> is this normal:

[20:22] <[Coke]> 2025-11-24T20:20:58.014496Z --- sparkyd: parse sparky job yaml config from: /root/.sparky/projects/agent/sparky.yaml

[20:22] <melezhik> Good

[20:22] <[Coke]> 2025-11-24T20:20:58.032649Z --- [agent] neither crontab  nor scm setup found, consider manual start, SKIP ...

[20:22] <[Coke]> getting a bunch of those while sparky-web is still running

[20:23] <melezhik> It‚Äôs ok. Those are messages from sparkyd which is sparky job runner , not to confuse with sparky-web

[20:23] <[Coke]> AFK

[20:23] <melezhik> And those messages are completely valid

[20:24] <melezhik> So the task is to find out what and why kill sparky-web at some point at the docker container

[20:25] <melezhik> I guess maybe ab5tract had/have similar issue with his podman agent

[20:26] <melezhik> Also there‚Äôs log for sparky-web itself , however I doubt it will provide anything essential, it‚Äôs in /root/.sparky/sparky-web.log or something

[20:27] <melezhik> I would try to find something in dmesg or syslog messages relevant to killing Rakudo or sparky-web process which is just a cro application

[20:28] <melezhik> I am not sure if this helps but I run the same docker container on MacBook and don‚Äôt have such an issue . However I use ORB stack docker container runtime . HTH

[20:30] <[Coke]> again, dmesg doesn't work.

[20:30] <[Coke]> and the fix requires sysctl which is permission denied.

[20:34] <[Coke]> wow, all my iterm windows just froze.

[20:43] *** finanalyst left
[20:48] <ab5tract> Yeah I‚Äôve had something similar happen too. I‚Äôve tried every which way I could find to set resource restraints on the containers

[20:48] <ab5tract> Alas, to no avail

[20:49] <[Coke]> -1 on debian, I guess.

[22:54] *** melezhik left
[23:01] *** [Coke] left
[23:01] *** [Coke] joined
