[00:03] *** reportable6 left
[00:25] <Geth> ¦ rakudo: coke self-assigned 42 as sum of cubes - doesn't work on Windows https://github.com/rakudo/rakudo/issues/3176
[00:28] <Geth> ¦ rakudo: coke self-assigned The Str.succ method on 1, ¹, ⒈ and ①  works differently https://github.com/rakudo/rakudo/issues/3379
[00:51] <Geth> ¦ rakudo: coke self-assigned Raku repl cant't correctly decode `π` or any other no-acsii chars https://github.com/rakudo/rakudo/issues/4540
[01:03] *** linkable6 left
[01:03] *** reportable6 joined
[03:04] *** linkable6 joined
[03:31] *** evalable6 joined
[04:15] *** codesections5 joined
[04:20] *** [Tux] left
[04:20] *** codesections left
[04:20] *** codesections5 is now known as codesections
[04:22] *** MasterDuke left
[04:23] *** Kaipi joined
[04:24] *** Kaiepi left
[04:24] *** Altai-man joined
[04:27] *** bisectable6_ joined
[04:27] *** committable6_ joined
[04:28] *** sourceable6_ joined
[04:28] *** sena_kun left
[04:28] *** committable6 left
[04:28] *** bisectable6 left
[04:28] *** sourceable6 left
[04:28] *** rypervenche left
[04:28] *** releasable6 left
[04:28] *** notable6 left
[04:28] *** ugexe left
[04:28] *** [Tux] joined
[04:29] *** ugexe joined
[04:29] *** jdv_ left
[04:29] *** jdv joined
[04:29] *** rypervenche joined
[04:32] *** SmokeMachine_ joined
[04:32] *** shareable6_ joined
[04:33] *** unicodable6_ joined
[04:33] *** coverable6 left
[04:33] *** SmokeMachine left
[04:33] *** shareable6 left
[04:33] *** unicodable6 left
[04:33] *** SmokeMachine_ is now known as SmokeMachine
[04:33] *** tbrowder left
[04:33] *** tbrowder joined
[04:41] *** nativecallable6 left
[04:41] *** nativecallable6 joined
[04:47] *** gfldex_ joined
[04:47] *** gfldex left
[04:48] *** statisfiable6 left
[04:48] *** statisfiable6 joined
[04:49] *** benchable6 left
[04:49] *** benchable6 joined
[04:50] *** squashable6_ joined
[04:51] *** squashable6 left
[05:01] *** bloatable6_ joined
[05:03] *** bloatable6 left
[05:25] *** releasable6 joined
[05:27] *** coverable6 joined
[05:28] *** notable6 joined
[06:02] *** reportable6 left
[06:05] *** reportable6 joined
[06:26] <Geth> ¦ problem-solving/JJ-patch-1: a17ae4d921 | (Juan Julián Merelo Guervós)++ (committed using GitHub Web editor) | solutions/meta/TheRakuFoundation.md
[06:26] <Geth> ¦ problem-solving/JJ-patch-1: Addressing @vrurg comments
[06:26] <Geth> ¦ problem-solving/JJ-patch-1: review: https://github.com/Raku/problem-solving/commit/a17ae4d921
[07:22] *** MasterDuke joined
[08:31] *** lizmat_ joined
[09:06] *** lizmat_ left
[09:06] *** Geth joined
[09:07] *** TempIRCLogger joined
[09:08] *** gfldex_ is now known as gfldex
[09:29] *** lizmat_ joined
[09:29] *** Geth left
[09:30] *** Geth joined
[09:30] *** TempIRCLogger__ joined
[09:31] *** RakuIRCLogger left
[09:31] *** lizmat left
[09:31] *** TempIRCLogger left
[09:32] *** lizmat_ left
[09:32] *** lizmat joined
[10:53] *** linkable6 left
[10:53] *** evalable6 left
[10:53] *** evalable6 joined
[11:55] *** linkable6 joined
[12:03] *** reportable6 left
[12:04] *** reportable6 joined
[12:37] *** jgaz joined
[13:02] *** jgaz left
[13:49] *** jgaz joined
[14:21] *** jgaz left
[18:02] *** reportable6 left
[18:06] *** japhb left
[18:09] *** japhb joined
[19:16] <timo> interprete with the rakudo compiler using the best optimization level.
[19:16] <timo> $ raku --optimize=0 prime.rk
[19:16] <timo> ^- ???
[19:16] <timo> https://github.com/PlummersSoftwareLLC/Primes/blob/drag-race/PrimeRaku/solution_1/prime.rk
[19:20] <MasterDuke> ha
[19:20] <timo> uh
[19:20] <timo> it is actually faster with --optimize=0
[19:24] <timo> with a "my uint8 @!Bits" and using 0 and 1 as True and False respectively, it's almost 10x faster
[19:24] <timo> 25 passes instead of just 3
[19:25] <MasterDuke> oh damn
[19:25] <timo> setting optimize to 0 there gives just 16 passes
[19:27] <timo> now i'm at 41 passes
[19:28] <timo> 80
[19:29] <MasterDuke> sounds like a rakudo issue is needed
[19:29] <japhb> timo: I'm confused.  What are you changing between different runs?
[19:30] <timo> well, first i went from "my @!Bits is default(True)" to "my int8 @!Bits" and creating it to size in BUILD
[19:30] <timo> then i made more and more variables native ints. also, trying uint first, which is slower than int, so changing it to int
[19:31] <japhb> I'm weirded out by uint being slower than int (I can imagine UInt being slower than Int, but not the native versions)
[19:32] <timo> i guess we have uint/int conversions all over the place
[19:32] <timo> we're not exactly smart about that yet
[19:34] <japhb> timo: I've also found that we don't handle the final bit correctly for uint64 -- I had to special case in a few places in CBOR::Simple to work around that.
[19:34] <japhb> And when I read through the MoarVM JIT code, it certainly looks like we're playing fast and loose with the difference between int64 and uint64
[19:34] <timo> perhaps we're clipping it off somewhere? you'll want to inspect the mbc
[19:34] <timo> indeed
[19:36] <timo> https://plummerssoftwarellc.github.io/PrimeView/?rc=30&sc=dt&sd=True
[19:37] <timo> https://plummerssoftwarellc.github.io/PrimeView/report?id=rbergen-1633261689.json&hi=False&hf=False&hp=False&fi=basic~python~raku~red~rexx~ruby~zig&fp=&fa=&ff=&fb=&tp=True&sc=pp&sd=True
[19:40] <MasterDuke> sounds like you're well on your way to moving it up that list
[19:40] <timo> interesting, a lisp implementation beats even the fastest C++ and D implementations
[19:41] <japhb> Wow, PrimeView is quite possibly the most overengineered tool for that task I've ever seen
[19:41] <timo> well, we're faster than one Clojure implementation ...
[19:43] <timo> i could send this as a pull request and describe it as an improvement over the solution by draco1006
[19:43] <MasterDuke> have you looked at a profile or spesh log to figure out what's up with the --optimize=0 being faster?
[19:44] <timo> not yet
[19:45] <timo> especially since the benefit goes away at higher speeds
[19:48] <timo> it's also allowed to have multithreaded implementations, would have to read up on what that entails, exactly
[19:49] <lizmat> for reference: it looks like test-t is *not* slower with --optimize=0
[19:49] <timo> it looks like the base rules require either that i use 1 for prime or to generate an array of prime numbers
[19:49] <japhb> lizmat: Meaning it's the same speed?
[19:50] <lizmat> within noise, yes
[19:50] <japhb> timo: Can you use a buf8, and .allocate all bytes to 1?
[19:50] <japhb> lizmat: Huh.  That's kinda odd
[19:51] <lizmat> I'd even say it is tending to be a tad faster with --optimize=0
[19:51] <MasterDuke> what about with --optimize=off ?
[19:52] <lizmat> stage Optimize: 0.009 secds
[19:52] <lizmat> the tends to be a bit slower
[19:53] <timo> the dockerfile for that solution uses the 2021.04 rakudo star image
[19:53] <timo> i would suggest that, since it uses no dependencies at all, that we instead give it a base raku image
[19:53] <MasterDuke> yeah
[19:56] <timo> what's our fastest way to initialize a native int array to all 1s?
[19:57] <japhb> m: my $buf = buf8.allocate(1_000_000, 1); say now - INIT now;
[19:57] <camelia> rakudo-moar 21271aa5f: OUTPUT: «0.014089853␤»
[19:57] <japhb> I *think* that's near the top
[19:58] <timo> okay buf instead of iny @foo
[19:58] <timo> m: say buf8.allocate(1_000_000, 1);
[19:58] <camelia> rakudo-moar 21271aa5f: OUTPUT: «Buf[uint8]:0x<01 01 01 01 01 01 01 01 01 01 01 01 01 01 01 01 01 01 01 01 01 01 01 01 01 01 01 01 01 01 01 01 01 01 01 01 01 01 01 01 01 01 01 01 01 01 01 01 01 01 01 01 01 01 01 01 01 01 01 01 01 01 01 01 01 01 01 01 01 01 01 01 01 01 01 01 01 01 01 …»
[19:59] <japhb> Yeah, Blob has 2-arg allocate, intarray only has 1-arg allocate
[19:59] <lizmat> intarray has allocate ?
[20:00] <lizmat> m: my int @a; @a.allocate(1000)
[20:00] <camelia> rakudo-moar 21271aa5f: OUTPUT: «No such method 'allocate' for invocant of type 'array[int]'␤  in block <unit> at <tmp> line 1␤␤»
[20:00] <timo> i'm down to 20 runs now
[20:00] <japhb> https://github.com/rakudo/rakudo/blob/master/src/core.c/Buf.pm6#L82
[20:00] <japhb> Is that better or worse?
[20:01] <timo> worse, but i'm not allowed to submit the faster one because its values are inverted
[20:01] <japhb> Ah, interesting
[20:02] <timo> somehow i'm getting very strange profiler output
[20:02] <japhb> lizmat: Sorry, CArray was the other one with allocate, braino there
[20:03] <lizmat> well, perhaps native arrays should have an allocate
[20:04] <timo> looks like the "last" does it
[20:04] *** reportable6 joined
[20:04] <timo> unfortunately my code allocates a whole boatload of IntLexRef, and so the garbage collector runs a whole lot
[20:05] <timo> only 6% of total time, but of course setting up and using the IntLexRef objects costs something
[20:06] <japhb> lizmat: I'm not entirely clear on the intended differences between the APIs of Blob, array, and CArray.  (I understand that CArray and VMArray are different REPRs, I'm talking purely about the differences in Raku-level API)
[20:08] <lizmat> I wonder whether IntLexRefs could be prevented by a similar trick that is being used for scalar parameters, aka bind directly ?
[20:08] <japhb> In other words, I can't tell what differences are accidents of history and which are intentional design
[20:08] <lizmat> I think the differences are just that: accidents of history
[20:10] <japhb> Feels like fixing that wants a tracking issue then
[20:27] <timo> now i would love it if my nullOut method, the one that goes through from beginning to end and sets all ultiples of one number to 0, wouldn't allocate IntLexRef objects
[20:28] <MasterDuke> what if you do the trick of creating a variable with the value of 0 and then just assigning that?
[20:29] <timo> using nqp::bindpos_i directly instead of @!Bits.ASSIGN-POS, which is just a null-check against the index followed by a bindpos_i, i go from 20 to 90
[20:30] <lizmat> I have a feeling we could do that null-check in a special dispatch program
[20:31] <timo> hm, potentially using the "guard not equal" after a number check, but how do we emit the number arithmetic there
[20:32] <lizmat> I wouldn't know :-)
[20:35] <timo> using the nqp op got gc runs down to 25 from 10x that amount
[20:38] <timo> for 90 passes through in the 5 seconds it runs, allocate now takes 1 second, or 1.25 ms per entry. the size we allocate is one million
[20:39] <timo> so here's a silly idea
[20:40] <japhb> Re: nqp::ops and GC runs -- that's one of the reasons that both JSON::Fast and CBOR::Simple use a non-trivial amount of nqp.
[20:40] <timo> m: my buf8 $a .= new; $a[0, 1, 2, 3, 4] = 1; $a.splice($a.elems, 0, $a); $a.splice($a.elems, 0, $a); $a.splice($a.elems, 0, $a); $a.splice($a.elems, 0, $a); $a.splice($a.elems, 0, $a); $a.splice($a.elems, 0, $a); $a.splice($a.elems, 0, $a); $a.splice($a.elems, 0, $a); $a.splice($a.elems, 0, $a); $a.splice($a.elems, 0, $a); say $a.elems; say $a.pick(100);
[20:40] <camelia> rakudo-moar 21271aa5f: OUTPUT: «Cannot unbox a type object (Nil) to int.␤  in block <unit> at <tmp> line 1␤␤»
[20:41] <timo> m: my buf8 $a .= new; $a[0, 1, 2, 3, 4] >>=>> 1; $a.splice($a.elems, 0, $a); $a.splice($a.elems, 0, $a); $a.splice($a.elems, 0, $a); $a.splice($a.elems, 0, $a); $a.splice($a.elems, 0, $a); $a.splice($a.elems, 0, $a); $a.splice($a.elems, 0, $a); $a.splice($a.elems, 0, $a); $a.splice($a.elems, 0, $a); $a.splice($a.elems, 0, $a); say $a.elems; say $a.pick(100);
[20:41] <camelia> rakudo-moar 21271aa5f: OUTPUT: «5===SORRY!5=== Error while compiling <tmp>␤Missing << or >>␤at <tmp>:1␤------> 3y buf8 $a .= new; $a[0, 1, 2, 3, 4] >>=>7⏏5> 1; $a.splice($a.elems, 0, $a); $a.spli␤    expecting any of:␤        infix␤        infix stopper␤»
[20:41] <timo> m: my buf8 $a .= new; $a[0, 1, 2, 3, 4] »=» 1; $a.splice($a.elems, 0, $a); $a.splice($a.elems, 0, $a); $a.splice($a.elems, 0, $a); $a.splice($a.elems, 0, $a); $a.splice($a.elems, 0, $a); $a.splice($a.elems, 0, $a); $a.splice($a.elems, 0, $a); $a.splice($a.elems, 0, $a); $a.splice($a.elems, 0, $a); $a.splice($a.elems, 0, $a); say $a.elems; say $a.pick(100);
[20:41] <camelia> rakudo-moar 21271aa5f: OUTPUT: «5120␤(1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1)␤»
[20:41] <timo> m: my buf8 $a .= new; $a[0, 1, 2, 3, 4] »=» 1; $a.splice($a.elems, 0, $a); $a.splice($a.elems, 0, $a); $a.splice($a.elems, 0, $a); $a.splice($a.elems, 0, $a); $a.splice($a.elems, 0, $a); $a.splice($a.elems, 0, $a); $a.splice($a.elems, 0, $a); $a.splice($a.elems, 0, $a); $a.splice($a.elems, 0, $a); $a.splice($a.elems, 0, $a); say $a.elems; say now - INIT now
[20:41] <camelia> rakudo-moar 21271aa5f: OUTPUT: «5120␤0.006869058␤»
[20:42] <timo> m: my buf8 $a .= buf8.allocate(5120, 1); say $a.elems; say now - INIT now
[20:42] <camelia> rakudo-moar 21271aa5f: OUTPUT: «No such method 'buf8' for invocant of type 'Buf[uint8]'.  Did you mean␤'Buf'?␤  in block <unit> at <tmp> line 1␤␤»
[20:42] <timo> m: my buf8 $a = buf8.allocate(5120, 1); say $a.elems; say now - INIT now
[20:42] <camelia> rakudo-moar 21271aa5f: OUTPUT: «5120␤0.002985832␤»
[20:42] <timo> hm. not very good like this, is it
[20:42] <lizmat> nope  :-)
[20:42] <japhb> timo: You mean your idea of self-splicing?
[20:42] <MasterDuke> couldn't allocate just memset a large chunk of memory?
[20:42] <timo> perhaps it starts being better at bigger sizes
[20:43] <timo> MasterDuke: we don't have a memset op :D
[20:43] <japhb> timo: Although that would be nice, yes ....
[20:43] <MasterDuke> not yet we don't...
[20:43] <timo> doesn't even have to be an op now that we have the syscalls mechanism
[20:44] <lizmat> but that would be MoarVM specific, no?
[20:44] <timo> yes
[20:45] <timo> i wonder how to get much better than the current implementation of allocate
[20:45] <timo> perhaps loop unrolling?
[20:47] <timo> it's using lexicals for $i, which isn't optial
[20:47] <japhb> nqp::while?
[20:47] <timo> https://github.com/rakudo/rakudo/blob/master/src/core.c/Buf.pm6#L82
[20:48] <japhb> Also, avoiding $i++?
[20:49] <japhb> My fast copy loops look like this: https://github.com/japhb/CBOR-Simple/blob/main/lib/CBOR/Simple.rakumod#L407 (or in the 8-bit case, the slightly simplified https://github.com/japhb/CBOR-Simple/blob/main/lib/CBOR/Simple.rakumod#L427)
[20:49] <timo>       param_rp_i        r8(1), liti16(2)  # [000] bailed argument spesh: expected arg flag 2 to be int or box an int; type at position was IntLexRef
[20:52] <timo> the loop that it turns into is pretty tight, were it not for usage of using lexicals
[20:54] <timo> getlex, const_i64_16, add_i, bindlex, getlex, lt_i, unless_i, getlex, bindpos, goto
[20:58] <lizmat> PRs to make buf8.allocate faster are welcome :-)
[20:59] <japhb> There's something sublime about thinking 'Meh, 14ns/element is TOO DANG SLOW'
[21:01] <timo> if we get it into moarvm and get it to actually use memset in case we have an 8 bit buffer, we can reach more than that, right? because of vector operations
[21:01] <japhb> (I mean, I don't disagree.  But my high-school self trying to speed up 8086 code is grinning from ear to ear.)
[21:02] <japhb> Oh certainly.  Heck, I'm pretty sure we can do slightly better without even editing MoarVM
[21:04] <timo> if we use 1 bit per element instead of 8 bit per element, that would give us a lot less memory we have to comb through, even though we'll have to do bit-level fiddling
[21:04] *** linkable6 left
[21:04] *** evalable6 left
[21:04] <timo> if i'm not mistaken, we'll at most have to have 8 masks no matter what $factor is (for nulling out bits at $factor distance starting at 0)
[21:05] *** linkable6 joined
[21:05] <timo> another 2x can be saved by not even storing what even elements would have
[21:05] <timo> (my at most 8 masks actually assumes we want to work with every bit, not just every odd one)
[21:06] <timo> OTOH then we still have to make sure the return value is according to the rules
[21:06] <timo> gotta have a look at some of the faster implementations if they have any tricks
[21:09] <timo> here's a python implementation, it sets zeroes with             self._bits[start :: step] = b"\x00" * (size // step + bool(size % step))
[21:10] <timo> and self._bits = bytearray(b"\x01") * ((self._size + 1) // 2)
[21:10] <timo> so they are only saving odd elements anyway?
[21:11] <timo> hold up, they return a Seq that iterates primes, they don't return an array of primes at all!
[21:12] <timo> that is allowed?
[21:18] <timo> then i can go back to not having to allocate at all, you know
[21:24] <japhb> Returning an iterator and returning an allocated value are wildly different performance-wise.  If that's how other languages are jumping ahead, that's definitely apples and oranges.
[21:25] <timo> https://github.com/PlummersSoftwareLLC/Primes/issues/630 - relevant to this question, but i didn't read all of it yet and gotta go afk for a bit first
[21:26] <timo> "the contributing document clearly states that the solutions can return a pre-sieved sieve buffer as one of the options (just that the sieving must occur on every pass of the timed loop as is done here),"
[21:27] <japhb> m: use nqp; class B8 does Buf[uint8] is repr('VMArray') is array_type(uint8) { method allo(Blob:U: int $elems, int $value) { my $blob := nqp::setelems(nqp::create(self),$elems); my int $i = -1; nqp::while(nqp::islt_i(($i = nqp::add_i($i, 1)), $elems), nqp::bindpos_i($blob,$i,$value)) } };  my $t0 = now; B8.allocate(1_000_000, 1); my $t1 = now; B8.allo(1_000_000, 1); my $t2 = now; .say for $t1-$t0, $t2-$t1;
[21:27] <camelia> rakudo-moar 21271aa5f: OUTPUT: «0.010042925␤0.0084734␤»
[21:27] <japhb> m: use nqp; class B8 does Buf[uint8] is repr('VMArray') is array_type(uint8) { method allo(Blob:U: int $elems, int $value) { my $blob := nqp::setelems(nqp::create(self),$elems); my int $i = -1; nqp::while(nqp::islt_i(($i = nqp::add_i($i, 1)), $elems), nqp::bindpos_i($blob,$i,$value)) } };  my $t0 = now; B8.allocate(10_000_000, 1); my $t1 = now; B8.allo(10_000_000, 1); my $t2 = now; .say for $t1-$t0, 
[21:27] <camelia> rakudo-moar 21271aa5f: OUTPUT: «0.098266638␤»
[21:27] <japhb> $t2-$t1;
[21:28] <japhb> GAH
[21:29] <japhb> m: use nqp; class B8 does Buf[uint8] is repr('VMArray') is array_type(uint8) { method allo(Blob:U: int $e, int $v) { my $b := nqp::setelems(nqp::create(self),$e); my int $i = -1; nqp::while(nqp::islt_i(($i = nqp::add_i($i,1)),$e), nqp::bindpos_i($b,$i,$v)) } }; my $t0 = now; B8.allocate(10_000_000, 1); my $t1 = now; B8.allo(10_000_000, 1); my $t2 = now; .say for $t1-$t0, $t2-$t1;
[21:29] <camelia> rakudo-moar 21271aa5f: OUTPUT: «0.097264776␤0.081980577␤»
[21:33] <japhb> Not like a doubling of speed, but at least 2-digit %age
[21:33] <timo> don't forget to swap them around to make sure warmup and such aren't distorting the results
[21:33] <MasterDuke> i would time those two in two separate invocations
[21:34] <timo> in favor of the second impl
[21:34] <timo> yeah, or that
[21:34] <japhb> m: use nqp; class B8 does Buf[uint8] is repr('VMArray') is array_type(uint8) { method allo(Blob:U: int $e, int $v) { my $b := nqp::setelems(nqp::create(self),$e); my int $i = -1; nqp::while(nqp::islt_i(($i = nqp::add_i($i,1)),$e), nqp::bindpos_i($b,$i,$v)) } }; my $t0 = now; B8.allocate(10_000_000, 1); my $t1 = now; .say for $t1-$t0;
[21:34] <camelia> rakudo-moar 21271aa5f: OUTPUT: «0.094383851␤»
[21:34] <timo> probably have the same code for both cases so compilation doesn't make a difference either. just have like an if/else that decides at run time which one to pick
[21:34] <japhb> m: use nqp; class B8 does Buf[uint8] is repr('VMArray') is array_type(uint8) { method allo(Blob:U: int $e, int $v) { my $b := nqp::setelems(nqp::create(self),$e); my int $i = -1; nqp::while(nqp::islt_i(($i = nqp::add_i($i,1)),$e), nqp::bindpos_i($b,$i,$v)) } }; my $t0 = now; B8.allo(10_000_000, 1); my $t1 = now; .say for $t1-$t0;
[21:34] <camelia> rakudo-moar 21271aa5f: OUTPUT: «0.087945616␤»
[21:35] <timo> if we can take this speedup to the core, so be it :) :)
[21:36] <japhb> Yeah, I think there's still a valid difference there.
[21:36] <timo> can you compare the speeds at different array sizes as well?
[21:38] <japhb> timo: Effect is there for both 1_000_000 and 10_000_000; much smaller is getting down to noise levels; how much bigger are you thinking of?
[21:38] <japhb> s/Effect/Improvement/
[21:40] <japhb> m: use nqp; class B8 does Buf[uint8] is repr('VMArray') is array_type(uint8) { method allo(Blob:U: int $e, int $v) { my $b := nqp::setelems(nqp::create(self),$e); my int $i = -1; nqp::while(nqp::islt_i(($i = nqp::add_i($i,1)),$e), nqp::bindpos_i($b,$i,$v)) } }; my $t0 = now; B8.allocate(100_000_000, 1); my $t1 = now; .say for $t1-$t0;
[21:40] <camelia> rakudo-moar 21271aa5f: OUTPUT: «0.877247417␤»
[21:40] <japhb> m: use nqp; class B8 does Buf[uint8] is repr('VMArray') is array_type(uint8) { method allo(Blob:U: int $e, int $v) { my $b := nqp::setelems(nqp::create(self),$e); my int $i = -1; nqp::while(nqp::islt_i(($i = nqp::add_i($i,1)),$e), nqp::bindpos_i($b,$i,$v)) } }; my $t0 = now; B8.allo(100_000_000, 1); my $t1 = now; .say for $t1-$t0;
[21:40] <camelia> rakudo-moar 21271aa5f: OUTPUT: «0.833379933␤»
[22:05] *** linkable6 left
[22:05] <timo> i was thinking smaller; can a easurable difference be teased out by repeating it a hundred times or whatever?
[22:06] *** linkable6 joined
[22:07] *** evalable6 joined
[22:12] <timo> if we have a new, faster loop teplate, this could be spread all over the core setting
[22:51] <timo> 95 with japhb's allo method
[22:51] <timo> did i do something wrong? lmao
[22:53] <timo> 19 when i go back to [] instead of ASSIGN-POS and AT-POS
[22:53] <MasterDuke> [] isn't being inlined?
[23:02] <timo> i think it was
[23:02] <timo> let me look again
[23:06] <timo> ok, [] is inlined, but ASSIGN-POS isn't inlined
[23:19] <timo>       # [012] could not inline 'ASSIGN-POS' (4081) candidate 0: bytecode is too large to inline
[23:20] <timo> 670 whole bytes
[23:22] <timo> there's three dispatches in there that have all never been dispatched, that's all from the bounds checking and exception throwing in the error case
[23:22] <timo> it also contains a dynamic variable lookup to give something more interesting than "Index" in some cases
[23:28] <timo> i'm trying a private method that creates the failure now, see if that makes the bytecode small enough
[23:34] <timo>       # [013] inline-preventing instruction: param_rp_o
[23:34] <timo>       # [012] could not inline 'ASSIGN-POS' (4082) candidate 0: target has a :noinline instruction
[23:34] <timo> *sigh* lmao
[23:36] <timo>       param_rp_i        r3(1), liti16(1)  # [000] bailed argument spesh: expected arg flag 1 to be int or box an int; type at position was IntLexRef
[23:36] <timo> ^- this most probably caused that
[23:41] <timo> would be great to have more control about when refs are passed, especially when we're in the core and calling other stuff from core
[23:46] <japhb> timo: I had to deal with some $RL stuff (BAK now), but there is another variant of the loop I wanted to try, I'll post in a couple minutes.
[23:47] <timo> d'oh, the frame is small enough now of course, but the argument passed to ASSIGN-POS was the issue that prevented inlining also
[23:51] <japhb> Ah dang, the other method I wanted to try is slower, sigh
[23:55] <japhb> m: use nqp; class B8 does Buf[uint8] is repr('VMArray') is array_type(uint8) { method allo(Blob:U: int $e, int $v) { my $b := nqp::setelems(nqp::create(self),$e); my int $i = -1; nqp::while(nqp::islt_i(($i = nqp::add_i($i,1)),$e), nqp::bindpos_i($b,$i,$v)) } }; my $t0 = now; B8.allo(100_000_000, 1); my $t1 = now; .say for $t1-$t0;
[23:55] <camelia> rakudo-moar 21271aa5f: OUTPUT: «0.832321031␤»
[23:55] <japhb> Dang it, not what I meant to do
[23:56] <japhb> m: use nqp; class B8 does Buf[uint8] is repr('VMArray') is array_type(uint8) { method allo(Blob:U: int $e, int $v) { my $b := nqp::setelems(nqp::create(self),$e); my int $i = -1; nqp::while(nqp::islt_i(($i = nqp::add_i($i,1)),$e), nqp::bindpos_i($b,$i,$v)) } }; my $t = now; B8.allo(100_000, 1) for ^1_000; say now - $t;
[23:56] <camelia> rakudo-moar 21271aa5f: OUTPUT: «0.854373476␤»
[23:56] <japhb> m: use nqp; class B8 does Buf[uint8] is repr('VMArray') is array_type(uint8) { method allo(Blob:U: int $e, int $v) { my $b := nqp::setelems(nqp::create(self),$e); my int $i = -1; nqp::while(nqp::islt_i(($i = nqp::add_i($i,1)),$e), nqp::bindpos_i($b,$i,$v)) } }; my $t = now; B8.allo(1_000, 1) for ^100_000; say now - $t;
[23:56] <camelia> rakudo-moar 21271aa5f: OUTPUT: «0.782978554␤»
[23:56] <japhb> m: use nqp; class B8 does Buf[uint8] is repr('VMArray') is array_type(uint8) { method allo(Blob:U: int $e, int $v) { my $b := nqp::setelems(nqp::create(self),$e); my int $i = -1; nqp::while(nqp::islt_i(($i = nqp::add_i($i,1)),$e), nqp::bindpos_i($b,$i,$v)) } }; my $t = now; B8.allocate(1_000, 1) for ^100_000; say now - $t;
[23:56] <camelia> rakudo-moar 21271aa5f: OUTPUT: «0.898699618␤»
[23:57] <japhb> timo: ^^  # Yep, difference between my tweak and core's is larger for looping over smaller allocations.
[23:57] *** squashable6_ left
[23:58] <timo> cool
[23:59] <timo> cool
[23:59] *** squashable6 joined
