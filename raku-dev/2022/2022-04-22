[00:07] *** reportable6 left
[00:08] *** reportable6 joined
[00:14] *** melezhik left
[00:46] *** frost joined
[01:24] *** londoed_ left
[01:24] *** londoed_ joined
[03:31] *** committable6 left
[03:31] *** nativecallable6 left
[03:31] *** releasable6 left
[03:31] *** benchable6 left
[03:31] *** bisectable6 left
[03:31] *** unicodable6 left
[03:31] *** greppable6 left
[03:31] *** coverable6 left
[03:31] *** linkable6 left
[03:31] *** tellable6 left
[03:31] *** notable6 left
[03:31] *** statisfiable6 left
[03:31] *** bloatable6 left
[03:31] *** sourceable6 left
[03:31] *** reportable6 left
[03:31] *** shareable6 left
[03:31] *** quotable6 left
[03:31] *** evalable6 left
[03:31] *** quotable6 joined
[03:31] *** sourceable6 joined
[03:31] *** releasable6 joined
[03:31] *** statisfiable6 joined
[03:31] *** shareable6 joined
[03:32] *** linkable6 joined
[03:32] *** nativecallable6 joined
[03:32] *** bloatable6 joined
[03:32] *** benchable6 joined
[03:32] *** reportable6 joined
[03:32] *** tellable6 joined
[03:33] *** bisectable6 joined
[03:33] *** evalable6 joined
[03:34] *** coverable6 joined
[03:34] *** greppable6 joined
[03:34] *** committable6 joined
[03:34] *** notable6 joined
[03:34] *** unicodable6 joined
[03:43] *** frost left
[05:44] *** bloatable6 left
[05:44] *** committable6 left
[05:44] *** sourceable6 left
[05:44] *** bisectable6 left
[05:44] *** releasable6 left
[05:44] *** coverable6 left
[05:44] *** linkable6 left
[05:44] *** statisfiable6 left
[05:44] *** notable6 left
[05:44] *** evalable6 left
[05:44] *** greppable6 left
[05:44] *** benchable6 left
[05:44] *** tellable6 left
[05:44] *** unicodable6 left
[05:44] *** nativecallable6 left
[05:44] *** quotable6 left
[05:44] *** shareable6 left
[05:44] *** reportable6 left
[05:44] *** tellable6 joined
[05:44] *** reportable6 joined
[05:44] *** notable6 joined
[05:45] *** committable6 joined
[05:45] *** releasable6 joined
[05:45] *** sourceable6 joined
[05:45] *** nativecallable6 joined
[05:45] *** bisectable6 joined
[05:45] *** unicodable6 joined
[05:45] *** evalable6 joined
[05:45] *** quotable6 joined
[05:46] *** statisfiable6 joined
[05:46] *** coverable6 joined
[05:46] *** linkable6 joined
[05:46] *** benchable6 joined
[05:46] *** bloatable6 joined
[05:46] *** shareable6 joined
[05:47] *** greppable6 joined
[06:07] *** reportable6 left
[06:08] *** reportable6 joined
[07:43] <lizmat> Files=1353, Tests=117192, 308 wallclock secs (35.81 usr  9.58 sys + 4087.00 cusr 339.79 csys = 4472.18 CPU)

[08:04] *** frost joined
[08:15] <Geth> ¦ rakudo/lizmat-constantize-compiler-info: 257e06ab5f | (Elizabeth Mattijsen)++ | tools/cleanup-precomps.raku

[08:15] <Geth> ¦ rakudo/lizmat-constantize-compiler-info: Remove cleanup-precomps script

[08:15] <Geth> ¦ rakudo/lizmat-constantize-compiler-info: review: https://github.com/rakudo/rakudo/commit/257e06ab5f

[08:18] <patrickb> jdv: My reasoning was that if rakudo.org was compromised (someone modified the downloads) they would be able to also modify the keys also hosted there (and unless they are stupid would surely do so).

[08:20] <patrickb> jdv: So my proposal is that each uploader lists their key fingerprint on at least one other site. Since every raku dev has a GitHub account I thought the GH profile is a place everyone is able to list their fingerprint (like I did here: https://github.com/patrickbkr )

[08:23] <patrickb> jdv: I do recognize that only having keys on rakudo.org still provides some level of protection because users that already verified a download in the past would already have the keys downloaded. A malicously changed key would need to be re-downloaded and thus can cause suspicion.

[08:25] <patrickb> jdv: But all this is only my personal reasoning and I don't think I've seen this policy (listing fingerprints on third party sites) implemented on any other site. That's why I'd like to have feedback on whether we should proceed with this approach and if not how else we should do it.

[08:29] <nine> patrickb: I think there's a severe bug in your script https://github.com/rakudo/rakudo/issues/4899

[08:30] <patrickb> jdv: The PR requires all recent releasers to cooperate (put key fingerprint on GH), so it's necessary that all recent releasers (I think that's jdv, AntonOks, sena-kun and me) need to read, understand and hopefully approve the PR.

[08:30] <nine> patrickb: it's encoding music to mp3 instead of a superior codec like vorbis!

[08:30] <patrickb> nine: :-P Tell that my car radio

[08:30] <nine> Even in 2022? That's horrible

[08:31] <nine> But then, I guess a car is not a HiFi environment anyway

[08:31] <patrickb> The reason I have to have that script is because the biggest part of my collection IS encoded in opus or vorbis.

[08:33] <patrickb> WRT to more bugs: I am glad to be corrected and learn.

[08:53] <nine> patrickb: after what time can one expect that reduction in CPU usage?

[08:58] <patrickb> In that specific case the first core was droped after about 1:10

[08:59] <patrickb> In the screenshots one can see quite nicely when the cores are lost. The full width of the graph is 5 minutes.

[09:11] <nine> I've tried reproducing it with a reduced test case, but so far failed: rakudo -e 'my @input = ^1000; race for @input { my $proc = shell "dd if=/dev/urandom bs=4096 count=102400 | gzip > /dev/null", :err; $proc.err.slurp(:close) }'

[09:17] * patrickb tries

[09:36] <patrickb> nine: I think I found the sweet spot on my system.

[09:36] <patrickb> rakudo -e 'my @input = ^100000; race for @input { my $c = (($_ % 100 != 0 ?? 500 !! 0) + 100.rand).floor; my $proc = shell "dd if=/dev/urandom bs=4096 count=$c | gzip > /dev/null; echo done $_", :err; $proc.err.slurp(:close) }'

[09:38] <nine> Ah, yes, utilization goes down pretty rapidly

[09:38] <patrickb> I suspect the scheduler tries to set the no of cores to use on an estimation of no. of elements in the list and average time the last few items took. If some fast items happen to be next to each other, it reduces the number of cores.

[09:39] <patrickb> Also interesting: If one reduces the 500 to say 100, then the program doesn't start off with 32 cores but a lot fewer (~ 5 in my case).

[09:40] <nine> I see the same issue with rakudo -e 'my @input = ^100000; @input.race(:degree(32), :batch(1)).map: { my $c = (($_ % 100 != 0 ?? 500 !! 0) + 100.rand).floor; my $proc = shell "dd if=/dev/urandom bs=4096 count=$c | gzip > /dev/null; echo done $_", :err; $proc.err.slurp(:close) }'

[09:41] <patrickb> But that might actually make sense, as the time required for management could weight more than the time to do the actual work. 

[09:41] <patrickb> Interesting. So it's not only the `race for` as I originally suspected.

[09:43] <nine> for actually compiles to map, so I'd have been surprised anyway

[09:51] <nine> Btw. I don't think the scheduler ever reduces the number of worker threads.

[09:55] *** sena_kun left
[09:56] *** sena_kun joined
[10:00] <Geth> ¦ IO-Path-ChildSecure/main: 9f01c370ba | (Elizabeth Mattijsen)++ | 9 files

[10:00] <Geth> ¦ IO-Path-ChildSecure/main: 1.2

[10:00] <Geth> ¦ IO-Path-ChildSecure/main: review: https://github.com/raku-community-modules/IO-Path-ChildSecure/commit/9f01c370ba

[10:39] *** Altai-man joined
[12:07] *** reportable6 left
[12:09] *** reportable6 joined
[13:33] <Geth> ¦ WebService-GitHub/main: ba4e94c97a | (Patrick Böker)++ | 2 files

[13:33] <Geth> ¦ WebService-GitHub/main: Add a start of a documentation

[13:33] <Geth> ¦ WebService-GitHub/main: review: https://github.com/raku-community-modules/WebService-GitHub/commit/ba4e94c97a

[13:34] <Geth> ¦ WebService-GitHub/main: a94d71ccf9 | (Patrick Böker)++ | dev-scripts/generate-module

[13:34] <Geth> ¦ WebService-GitHub/main: Version 0.2.1

[13:34] <Geth> ¦ WebService-GitHub/main: review: https://github.com/raku-community-modules/WebService-GitHub/commit/a94d71ccf9

[13:39] *** frost left
[14:04] *** [Coke] left
[14:06] *** [Coke] joined
[14:08] <jdv> lizmat: are you done with your changelog edits?

[14:08] <lizmat> actually, I was just about to look at them again  :-)

[14:08] <lizmat> got distracted a bit  :-)

[14:09] <lizmat> jdv: give me 30 mins or so?

[14:10] <jdv> ok, i'll start the release in 50mins, at the top of the hour i guess

[14:10] <lizmat> ok, that should be enough  :-)

[14:10] <jdv> thanks

[14:17] *** frost joined
[14:25] *** frost left
[14:34] <lizmat> jdv: done with the changelog draft

[15:04] <Geth> ¦ rakudo/lizmat-fix-unneeded-dependency-check: 58f6bc961d | (Elizabeth Mattijsen)++ | src/core.c/CompUnit/PrecompilationRepository.pm6

[15:04] <Geth> ¦ rakudo/lizmat-fix-unneeded-dependency-check: Fix unneeded dependency check

[15:04] <Geth> ¦ rakudo/lizmat-fix-unneeded-dependency-check: 

[15:04] <Geth> ¦ rakudo/lizmat-fix-unneeded-dependency-check: Commit 199888abedfe843996 in March 2020 borked the setting of the

[15:04] <Geth> ¦ rakudo/lizmat-fix-unneeded-dependency-check: repo-id.  This caused unneeded dependency checking for installed

[15:04] <Geth> ¦ rakudo/lizmat-fix-unneeded-dependency-check: modules, and a slowdown of e.g. loading NativeCall of about 80 msecs.

[15:04] <Geth> ¦ rakudo/lizmat-fix-unneeded-dependency-check: 

[15:04] <Geth> ¦ rakudo/lizmat-fix-unneeded-dependency-check: Now, the slowdown will only occur the first time after installation,

[15:04] <Geth> ¦ rakudo/lizmat-fix-unneeded-dependency-check: when the repo-id is now updated correctly so that a subsequent loading

[15:04] <Geth> ¦ rakudo/lizmat-fix-unneeded-dependency-check: will not have to do dependency checks.

[15:04] <Geth> ¦ rakudo/lizmat-fix-unneeded-dependency-check: 

[15:04] <Geth> ¦ rakudo/lizmat-fix-unneeded-dependency-check: Spotted by nine++ after looking into https://github.com/rakudo/rakudo/issues/4900

[15:04] <Geth> ¦ rakudo/lizmat-fix-unneeded-dependency-check: review: https://github.com/rakudo/rakudo/commit/58f6bc961d

[15:04] <Geth> ¦ rakudo: lizmat++ created pull request #4901: Fix unneeded dependency check

[15:04] <Geth> ¦ rakudo: review: https://github.com/rakudo/rakudo/pull/4901

[15:09] <jdv> lizmat: ok, i guess you didn't update  the ignore and delete - i'll do it

[15:09] <lizmat> ah... sorry, I thought you wanted to do that

[15:11] <jdv> there's nothing i want to do more

[15:11] <jdv> ha

[15:14] *** carlmasak joined
[15:58] <jdv> anyway...  aws seems to not be albe to start the instance i use for this stuff...

[15:58] <jdv> of all things:)

[15:59] <jdv> "insufficient capacity" for the last hour so far.  good times.

[17:21] *** carlmasak left
[17:21] *** Altai-man left
[17:37] <jdv> well, this is swell.  i guess we'll aim for sunday or monday then.  sorry folks.

[17:47] <[Coke]> no worries, thanks for your effort!

[18:05] <[Coke]> anyone have pointers on how to run blin in a docker container against everything? (and what sort of specs I need on the machine hosting the container)

[18:07] * [Coke] wonders why AGPL was chosen.

[18:07] <[Coke]> (for Blin)

[18:09] *** reportable6 left
[18:12] *** reportable6 joined
[18:17] <jdv> it largely just works.  i run the docker way.

[18:17] <jdv> there may be a few unfixed minor issues.  been meaning to look and fix if so.

[18:18] <jdv> i normally run it on a c6i12xlarge or so

[18:19] <jdv> iirc theres 1 dep issue and i added november and test async to the skip list cause tgey always fail in random ways

[18:20] <jdv> [Coke]: ^

[18:21] <jdv> iirc the rakudo release guide has some blin resource ideas

[18:30] <Geth> ¦ rakudo: dwarring++ created pull request #4902: fix orphaned trailing Pod declarands 

[18:30] <Geth> ¦ rakudo: review: https://github.com/rakudo/rakudo/pull/4902

[20:20] *** TempIRCLogger left
[21:37] <Geth> ¦ nqp: coke self-assigned t/nqp/19-file-ops.t failure in pre-2016.01 https://github.com/Raku/nqp/issues/274

