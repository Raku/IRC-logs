[00:00] <Geth> ¬¶ nqp: b8e416f365 | MasterDuke17++ (committed using GitHub Web editor) | docs/ops.markdown

[00:00] <Geth> ¬¶ nqp: Fix typo

[00:00] <Geth> ¬¶ nqp: review: https://github.com/Raku/nqp/commit/b8e416f365

[00:18] *** lucasb left
[00:44] <Geth> ¬¶ problem-solving: vrurg assigned to jnthn Issue Provide better interface for dispatchers https://github.com/Raku/problem-solving/issues/170

[00:51] *** patrickz joined
[00:53] *** ricky007 left
[00:55] *** patrickb left
[01:05] <patrickz> .tell rba I have a change in rakubrew.org that you can pull at your convenience.

[01:05] <tellable6> patrickz, I'll pass your message to rba

[01:06] *** patrickz left
[01:07] *** rypervenche left
[01:08] <rba> patrichz: got it. time is passing by very quickly tonight...

[01:08] <tellable6> 2020-03-25T21:38:37Z #raku-dev <patrickb> rba I have a new rakubrew release: version 7. As always https://patszim.volans.uberspace.de/patcloud/index.php/s/qmkreEJLQDZcjbf Can you upload?

[01:08] <tellable6> 2020-03-25T22:45:22Z #raku-dev <patrickb> rba I just uploaded a last minute bugfix release of rakubew: version 8. Can you upload both releases (7 and 8)? Same URL. Thank you!

[01:10] *** rypervenche joined
[02:38] *** upupbb-user3 left
[03:03] *** upupbb-user3 joined
[03:10] *** upupbb-user3 left
[05:14] *** upupbb-user3 joined
[06:00] *** upupbb-user3 left
[08:47] <lizmat> Files=1306, Tests=111225, 212 wallclock secs (29.13 usr  8.10 sys + 2981.77 cusr 264.14 csys = 3283.14 CPU)

[08:48] <lizmat> that's about 100 CPU seconds *more* than last spectest, and 7 wallclock secs more

[08:48] <lizmat> is this the takenextdispatcher opt taken out?

[08:48] <lizmat> if not, something else changed badly in the past 48 hours

[08:49] <nine> No, that's not committed yet

[08:49] <lizmat> let's see what test-t has to day

[08:49] <lizmat> *say

[08:53] *** hungrydonkey joined
[08:56] <MasterDuke> could be the new freeing of stuff. some were just during --full-cleanup, but others were just plain leaks

[08:59] <MasterDuke> nine: btw, did you see any of my chat with timotimo last night? heaptrack says that one of the places where the most allocations happen when compiling rakudo is in asplice(), with the most calls happening in add() and write_string_heap() in QASTCompilerMAST.nqp

[09:01] <MasterDuke> any ideas for how to reduce the number of allocations?

[09:02] <MasterDuke> those two produce almost 600k calls to asplice() each, compared to 41k for the next biggest

[09:06] <nine> I guess instead of an array of buffers, we could write them into one large buffer

[09:10] *** upupbb-user3 joined
[09:11] <Geth> ¬¶ nqp: 7cfcb66f09 | (Daniel Green)++ | src/vm/moar/QAST/QASTCompilerMAST.nqp

[09:11] <Geth> ¬¶ nqp: Use already calculated value instead of nqp::elems

[09:11] <Geth> ¬¶ nqp: review: https://github.com/Raku/nqp/commit/7cfcb66f09

[09:22] <nine> It's hard to say if it does make a difference in actual runtime. If it does, it's quite small

[09:23] <MasterDuke> oh, you just implemented it?

[09:24] *** leont joined
[09:26] <MasterDuke> i wasn't necessarily trying to improve runtime speed, but reduce the amount of memory used/allocated (though that may help speed). i.e., heaptrack reports 24gb allocated just when compiling CORE.c, with a peak RSS of 14gb

[09:33] <Geth> ¬¶ nqp: 8170a23f51 | (Stefan Seifert)++ | src/vm/moar/QAST/QASTCompilerMAST.nqp

[09:33] <Geth> ¬¶ nqp: Save a bunch of allocations in MoarVM::StringHeap

[09:33] <Geth> ¬¶ nqp: 

[09:33] <Geth> ¬¶ nqp: All we did with the @!strings array was adding to it and collecting them into

[09:33] <Geth> ¬¶ nqp: the bytecode buffer at the end of compilation. Instead of a list of buffers, we

[09:33] <Geth> ¬¶ nqp: may as well collect them into one buffer that will later be written into the

[09:33] <Geth> ¬¶ nqp: mbc buffer in one go. This way we don't have to allocate for every single

[09:33] <Geth> ¬¶ nqp: string on the heap but can rely on the VM to grow the buffer in sensible steps.

[09:33] <Geth> ¬¶ nqp: review: https://github.com/Raku/nqp/commit/8170a23f51

[09:34] <nine> MasterDuke: can you re-test with this?

[09:35] <timotimo> nine:do you have a clue how well your "does this need utf8" check performs?

[09:35] <timotimo> also, is nqp::getcp_s actually capable of outputting negative values for synthetics?

[09:36] <timotimo> i thought synthetics were meant to never leak out of moarvm's internals

[09:38] <nine> well they do in this case

[09:39] <nine> I've had a clue as I did profile this code extensively. But I can't say for sure anymore :/ It's too long ago. I think it did still hurt

[09:40] <timotimo> OK

[09:41] <timotimo> i know liz has lots of experience rewriting raku code with nqp statements and ops to get better performance; i don't know how well it translates to actual nqp code since the semantics and performance characteristics differ quite a bit

[09:41] <nine> Too bad nqp::encode assumes it ownes the target array and simple replaces the slots (and doesn't even free a previously existing one!). Otherwise we could encode into $!strings directly, saving a lot more allocations

[09:42] <timotimo> i think i'll just look at spesh's output for the routine to see if i spot anything

[09:44] <lizmat> so, I'm starting to lean towards not having a 2020.03 release

[09:44] <timotimo> if it really turns out to take a chunk of compile time, a special "has any non-latin1/ascii codepoints" check could become a moarvm op

[09:45] <lizmat> nqp::isascii

[09:45] <lizmat> nqp::islatin

[09:46] <nine> timotimo: I'd be disappointed if we couldn't compile that code to something that performs in the same ball park as such an op

[09:46] <timotimo> at the very least we don't know how to eliminate bounds checks yet :)

[09:46] <lizmat> an op could directly go at the internal representation?  which would not be possible when written in nqp ?

[09:47] <lizmat> or are you saying that nqp::getcp_s is doing exactly that ?

[09:48] <nine> nqp::getcp_s is already just a call to MVM_string_get_grapheme_at which returns the internal int32

[09:48] <lizmat> m: use nqp; my str $a = "\r\n"; dd nqp::getcp_s($a,0)   # indeed

[09:48] <camelia> rakudo-moar 156356eae: OUTPUT: ¬´-1‚ê§¬ª

[09:49] <lizmat> ok

[09:50] *** travis-ci joined
[09:50] <travis-ci> NQP build failed. Stefan Seifert 'Save a bunch of allocations in MoarVM::StringHeap

[09:50] <travis-ci> https://travis-ci.org/Raku/nqp/builds/668306363 https://github.com/Raku/nqp/compare/7cfcb66f0935...8170a23f510c

[09:50] *** travis-ci left
[09:50] <lizmat> restarted the one failing job

[09:50] <timotimo> i haven't looked but i could imagine getcp_s will branch over and over on the string's storage type, which will be predicted very effectively, but that's still time wasted

[09:52] <nine> True. It will also check every time that the index is within the string

[09:52] <lizmat> nqp looks good, should I bump it to rakudo ?

[09:55] <nine> I don't think this change is worth a bump by itself

[09:59] <lizmat> oki

[09:59] <timotimo> you know, a made-up character class could be used for the not-ascii / not-latin1 thing; we already have the ops for the mechanism

[10:00] <timotimo> but nine was right, the code for the loop doesn't look all that bad

[10:01] <lizmat> travis is green again

[10:02] <timotimo> the add-string method doesn't inline StringHeap's add; a quick check for the presence of the string in it could be worth something

[10:02] <timotimo> i.e. a very small method that can be inlined for sure

[10:04] <timotimo> don't know how often it gets called in total

[10:08] <timotimo> string heap had 26256 attempts for 3736 strings

[10:08] <timotimo> ^- that's in World.nqp

[10:09] <timotimo> Stage mbc        : string heap had 255978 attempts for 27911 strings

[10:09] <timotimo> ^- that's the core setting

[10:09] <timotimo> m: say 27911 * 100 / 255978

[10:09] <camelia> rakudo-moar 156356eae: OUTPUT: ¬´10.9036714‚ê§¬ª

[10:10] <timotimo> so about 9 out of 10 will be rejected by the existskey check and quick lookup

[10:12] <Geth> ¬¶ nqp: samcv++ created pull request #607: Unmangle mast strings

[10:12] <Geth> ¬¶ nqp: review: https://github.com/Raku/nqp/pull/607

[10:13] <Geth> ¬¶ nqp: c09765df9a | (Stefan Seifert)++ | src/vm/moar/QAST/QASTCompilerMAST.nqp

[10:13] <Geth> ¬¶ nqp: Split MoarVM::StringHeap into an inlineable add and slow add-internal

[10:13] <Geth> ¬¶ nqp: 

[10:13] <Geth> ¬¶ nqp: Method add was too large to be inlined but 9 out of 10 calls added already

[10:13] <Geth> ¬¶ nqp: existing strings, i.e. don't need the large part of the method. Split the

[10:13] <Geth> ¬¶ nqp: method into a quick check and one that actually adds a new string.

[10:13] <Geth> ¬¶ nqp: timotimo++ for the idea!

[10:13] <Geth> ¬¶ nqp: review: https://github.com/Raku/nqp/commit/c09765df9a

[10:15] <timotimo> i may want to point out that the factor isn't quite as big in smaller programs

[10:19] <lizmat> m: dd "a" ... 10   # that feels, LTA

[10:19] <camelia> rakudo-moar 156356eae: OUTPUT: ¬´Decrement out of range‚ê§  in block <unit> at <tmp> line 1‚ê§‚ê§¬ª

[10:19] <nine> But worst case we'll get about the same performance as before, won't we? add will be inlined and we perform one call to add-internal. If add doesn't get inlined, the code's not hot enough for that to make a difference

[10:20] <timotimo> nine:quite possibly, yeah

[10:20] <nine> timotimo: speaking about memory. How would I go on about finding out why moar-ha reports a constant ~40M heap size in a program that's several 100M RSS and clearly growing without bounds?

[10:21] <timotimo> mhh, something is being allocated outside of what moar believes is its heap

[10:21] <timotimo> that's either manual mallocs that moar hasn't a clue about, or some REPR that has a wrong or missing unmanaged_size function

[10:21] <lizmat> something in libuv perhaps ?

[10:22] <lizmat> or another library that MoarVM uses?

[10:22] <timotimo> ah, i don't think i've got unmanaged size functions for any of the async task reprs

[10:23] <nine> timotimo: well according to heaptrack about half of the growing memory is allocated by the gen2 allocator and the other half by the fixed size allocator

[10:23] <timotimo> AsyncTask has another level of indirection with the ops

[10:24] <timotimo> turning on the FSA debug gives precise stack traces, because it'll malloc for every allocation rather than every time a page happens to be filled up by an allocation

[10:27] <timotimo> annoyingly - at least the last time i checked - there's no way to ask heaptrack to treat our own & special allocation functions as allocation functions to analyze

[10:30] <timotimo> on the other hand, when gen2 grows without bound, that's more interesting; the heap analyzer - sort of by neccessity - works on the heap after everything unreachable has been tossed out

[10:31] <timotimo> we never free completely empty gen2 pages currently

[10:31] <timotimo> it's possible that every time more garbage than the last time is explicitly immediately allocated into the gen2, but that seems kind of unlikely?

[10:33] <timotimo> but anything that's alive in the gen2 should definitely show up in the moar heapanalyzer

[10:34] <timotimo> and of course we have different size classes in gen2 and the fsa, so it's possible but unlikely that objects being allocated are just so diverse in their size that every size bin gets at least one page?

[10:38] <nine> The test program just does `for ^1000000 { TestMemory.new }` with TestMemory.new being Perl 5: `sub new { return bless {}; }`. Of course there's a bit of Inline::Perl5 code in between, but still it does the same thing over and over

[10:51] *** Altai-man_ joined
[11:34] <Geth> ¬¶ rakudo: a10140f189 | (Elizabeth Mattijsen)++ | src/core.c/Exception.pm6

[11:34] <Geth> ¬¶ rakudo: Make error message a bit more readable

[11:34] <Geth> ¬¶ rakudo: review: https://github.com/rakudo/rakudo/commit/a10140f189

[11:51] <MasterDuke> nine: small reduction in numbers. before: 27.2m calls to allocation functions, 29.97gb allocated, 14.0gb peak rss. after:26.6 calls to allocation functions, 23.5gb allocated, 13.8gb peak rss

[11:53] <MasterDuke> but asplice pretty much drops off the map in heaptrack. was 4.3gb allocated, now 74.2kb

[11:54] <lizmat> 25% less allocated seems like a nice win ?

[11:54] <MasterDuke> oops, sorry, looked at the wrong thing. asplice goes from 4.3gb allocated to 4.0gb allocated

[12:00] <MasterDuke> but now the number of calls to asplice is cut in half. only ~600k by add-internal(), instead of ~600k each by add() and write_string_heap()

[12:17] <nine> Yeah, won't be able to get rid of those asplice calls unless we teach nqp::encode to append to an existing buffer

[12:20] <MasterDuke> i noticed that too (encode not taking an existing buffer). think it should be just a change in the moarvm implementation? or a new flag? or even a new op?

[12:25] *** hungrydonkey left
[12:27] <nine> I think it could just be changed in MoarVM. We'd have to check if the array already has a slots allocated and copy the encoded string into that instead of using it as slots of the array

[12:28] <lizmat> I'm looking in awe of what is being done here... but is this still to get the release out ?

[12:29] <lizmat> or have we already decided that the  2020.03 release is a no-go ?

[12:29] <lizmat> if the latter, I think we need to tell sena_kun

[12:29] *** sena_kun joined
[12:29] <lizmat> fwiw, I would be in favour of skipping a release

[12:30] <nine> lizmat: this is just for fun. I've spent a lot of time on the whole takenextdispatcher situation yesterday and pretty much ran out of ideas on how to proceed.

[12:30] *** Altai-man_ left
[12:31] *** hungrydonkey joined
[12:38] <MasterDuke> nine: what should body.start get set to in the existing buffer case? it's currently explicitly set to 0

[12:39] <timotimo> MasterDuke:it'd be kept the same

[12:40] <timotimo> in order to append to the end, only the amount has to change

[12:40] <timotimo> which set_size_internal should do for us

[12:40] <MasterDuke> so still set to 0? or unchanged from whatever it was in the existing buffer?

[12:41] <MasterDuke> think encode should call set_size_internal() or just assume the receiving buffer is big enough?

[12:44] <timotimo> steal some logic from whatever append does

[12:44] <timotimo> i'm assuming by append we really do mean append, for encode i mean

[12:44] <timotimo> it'd already be a small win if we can re-use an existing buffer for encode by overwriting the entirety of it

[12:45] <timotimo> though we'll want to have something in place so that after encoding a really really long string we don't keep a huge buffer around for a couple hundred tiny strings

[12:47] <timotimo> but appending an encoding result to the end of an existing buffer is what would make the string heap relatively un-allocaty

[12:50] <MasterDuke> nqp: my $a := MAST::Bytecode.new; my $b := nqp::encode("abc", "utf8", $a); nqp::encode("123", "utf8", $b); say(nqp::elems($b)); say(nqp::decode($b, "utf8"))

[12:50] <camelia> nqp-moarvm: OUTPUT: ¬´encode requires an empty array‚ê§   at <tmp>:1  (<ephemeral file>:<mainline>)‚ê§ from gen/moar/stage2/NQPHLL.nqp:1913  (/home/camelia/rakudo-m-inst-1/share/nqp/lib/NQPHLL.moarvm:eval)‚ê§ from gen/moar/stage2/NQPHLL.nqp:2118  (/home/camelia/rakudo-m-inst-1/share/‚Ä¶¬ª

[12:51] <MasterDuke> locally i get `6‚ê§abc123`

[12:51] <timotimo> that sounds like the right semantics

[12:56] *** hungrydonkey left
[12:57] <MasterDuke> i just added `if (((MVMArray *)buf)->body.slots.any)      memmove(((MVMArray *)buf)->body.slots.i8 + ((MVMArray *)buf)->body.elems, (MVMint8 *)encoded, output_size);`. but won't that stomp over whatever random memory is after the slots?

[12:58] <timotimo> yes, you will have to set its element count before you do that

[13:02] <MasterDuke> riiiggghhhttt...forget i asked anything and just assume i did that already...

[13:08] <MasterDuke> ugh, how do it get src/strings/ops.c to know about set_elems() in 6model/reprs/VMArray.h?

[13:10] <nine> REPR(buf)->pos_funcs.set_elems

[13:12] <MasterDuke> right!

[13:13] <timotimo> reprconv.h also has MVM_repr_set_elems or something similar that does the lookup for you and is hopefully inlined

[13:16] *** upupbb-user3 left
[13:20] <MasterDuke> ok, it still needs some polishing, but let's try testing with add-internal(). so i'm thinking it's probably going to change to write directly to $!strings

[13:22] <MasterDuke> oh. but the encoded size of the string is supposed to go before the string itself...

[13:27] <nine> Yes, you'll have to skip that and fill it in later

[13:28] <MasterDuke> hm. so `setelems($!strings, <current size + the size of a uint32>); encode(..., $!strings); setelems(<back to the end of the previous string>); write_uint32(<size of new string>); setelems(<back to end of $!strings>); write_uint8(0) while $pad--` ?

[13:30] <MasterDuke> what's the size of a uint32 in elems? 1?

[13:30] <nine> setelems, encode, nqp::writeuint($!strings, $pos-where-the-size-should-be)

[13:30] <nine> 4

[13:31] <MasterDuke> ah, write_uint32_at

[13:31] <MasterDuke> doh, the size is right there in the existing code. nine++

[13:35] <jnthn> About the nextdispatcher thing: I'm getting the impression that it's tricky to back it out, and we need time to work out an optimization strategy for it (or maybe doing the whole dispatcher thing quite differently). I guess if we can't defer the nextdispatcher changes until the next release, then probably our only option is to scrub this one, because a release with such a huge performance regression is probably not helpful.

[13:35] <jnthn> I'll have time to look at how we can better do the optimization of it next week.

[13:36] <jnthn> (Though implementing it may be a bigger job.)

[13:36] <lizmat> nine vrurg timotimo Kaiepi sena_kun comments?

[13:38] <MasterDuke> cool. rakudo just built ok. doing a spectest and then i'll see what heaptrack has to say

[13:38] <sena_kun> I am in favor of delaying the release. We usually pluck out some regressions (lizmat++) so the state of the current HEAD is rather clean than not (not taking the dispatcher issue, windows segfault etc situation in account).

[13:38] <timotimo> my first thought is: if we take some pains to toss nextdispatcherfor or whatever exactly was new, and release, then people will get some bugfixes and speedups for upgrading, that they would otherwise not be able to have

[13:39] <timotimo> but since i'm not going to put the work in, i can hardly demand someone do it :)

[13:39] <nine> Whatever we do it will end up in quite a bit of work. So we need a good plan to avoid investing a lot of work in the wrong direction.

[13:41] <MasterDuke> if we reasonably believe we can get things fixed+not slower by the time of next release i'm in favor of skipping this one. if not, maybe it's worth backing out the entirety of the new dispatcher work and doing a release sooner

[13:41] <Kaiepi> i'm ok with delaying

[13:41] <MasterDuke> spectest passed

[13:41] <nine> Even backing out the changes is non-trivial

[13:41] <sena_kun> We can invest time in doing even more fixes and speedups (or look at network stuff), and do even better release, than paying double the price. (and I am the person who claimed montly releases are important, you know)

[13:42] <sena_kun> s/montly/monthly/

[13:42] <nine> sena_kun: hey, I was not just claiming but massively arguing for monthly releases :) In this case I just don't see a way to have one

[13:43] <sena_kun> :)

[13:44] <MasterDuke> what about releasing from a commit right before the dispatcher merge? might be only a few changes/improvements, but it gets something out the door

[13:44] <sena_kun> Actually, I see a way of simply taking some really old commit (before merges), maybe two weeks after 2020.02 and cherry-pick some commits there, but it'll be a minor one.

[13:47] <lizmat> all of the work I did on CompUnit can easily wait another release

[13:48] <lizmat> sena_kun: that actually sounds like a good idea

[13:48] <lizmat> the native shaped array fix could go in

[13:49] <MasterDuke> heaptrack reports pretty much everything the same

[13:49] <lizmat> most of jnthn's commits in rakudo are ok I think

[13:49] <nine> MasterDuke: but where are those asplice calls coming from now?

[13:50] <MasterDuke> haven't checked that yet

[13:50] <lizmat> sena_kun: if you have a proposal for a cut-off point, I would gladly supply the commit shas that I think could go in anyways

[13:50] <lizmat> should be around 10 I think

[13:52] <MasterDuke> hm, the 600k remaining after nine's last commits are gone

[13:53] <MasterDuke> but heaptrack still says 4b due to asplice

[13:53] <MasterDuke> guess i should record the diff of the old and new size requested

[13:54] <sena_kun> lizmat, I am kind of reluctant to think about that, because I don't like history when one has `release-point ... a looooot of things ... release-announcement`. I am not sure what the risks are here. When I did a point release for musl it took some time and I had only a single commit to backport, but here we have much more.

[13:55] <lizmat> ok, that's fine by me as well  :-)

[13:56] <sena_kun> Alas, I don't know where should I draw a line between "I'm lazy to do it after a very rough week" and "The profit is fixed and potential losses are any size we might be lucky/unlucky".

[13:57] * sena_kun tries to hack on openssl

[13:58] <nine> Note that a newly created release branch with lots of cherry picked commits would have seen ~0 testing

[14:00] * lizmat thinks she's looking at a staring contest

[14:00] <jnthn> Depends on what sena_kun thinks of it, but we could also - imagining we have a solution for the dispatcher thing within a week or so - consider a release in 2 weeks time (so make the April one a bit earlier).

[14:01] <sena_kun> jnthn, I am considering this and it looks like a rather great option (imagine nobody will fall trying to have a solution for the dispatcher within a week or so time limit).

[14:02] <sena_kun> My stance on this is that as long as you don't increase number of regressed modules over time (i.e. blin && fix it once in a week or so), you can release any week if internals are good enough.

[14:05] *** lucasb joined
[14:08] <MasterDuke> ok, most asplice calls have an increase in ssize of 0 (79k calls), next most common is 256 and 128 with 5.8k and 4.2k calls respectively

[14:09] <MasterDuke> but the biggest increases in ssize are 6037504 and 5844992, then 5844992 and 499712 and 389120 and 188416

[14:11] <MasterDuke> ok, that 6m number is suspicious. i noticed heaptrack now reports 6.54mb leaked, up from 300k before

[14:11] <timotimo> this is the "encode directly into an existing buffer" change?

[14:12] <MasterDuke> yeah, plus changing how the string heap is created

[14:12] <MasterDuke> should i gist some diffs?

[14:12] <nine> might help

[14:12] * timotimo puts matchsticks between eyelids

[14:14] <MasterDuke> https://gist.github.com/MasterDuke17/4290bdf90a53eb5ab206b6db27fdec0e

[14:15] <MasterDuke> the heaptrack was taken with those fprintfs commented out

[14:16] <nine> MasterDuke: need to free encoded after memmove

[14:19] <MasterDuke> and after all that work i did finding leaks the past couple days...

[14:21] <MasterDuke> ok, now numbers are the same, just without that 6mb leak

[14:28] <MasterDuke> the big numbers are all due to assemble_to_file

[14:28] *** Altai-man_ joined
[14:28] <timotimo> right, it's one buffer that keeps growing

[14:28] <timotimo> and in the case of the core setting it'll end up at about 15 megabyte

[14:29] <timotimo> i haven't looked; does assemble_to_file concatenate buffers and then write the result to a file? because in that case the total buffer doesn't need to be built, it could just write every part of the whole with one write call

[14:29] <timotimo> BBL

[14:31] *** sena_kun left
[14:32] <MasterDuke> https://github.com/Raku/nqp/blob/master/src/vm/moar/QAST/QASTCompilerMAST.nqp#L2651-L2653

[14:34] <lizmat> looks like lines 2648-2650 are superfluous ?

[14:35] <MasterDuke> looks like yes. but it's probably faster to assemble one big buffer in memory and write it all at once than do many small writes. guess it depends on speed/memory use

[14:36] <MasterDuke> ^^^ was to timotimo

[14:36] <lizmat> MasterDuke: yeah, got that  :-)

[14:37] <lizmat> wouldn't OS buffering take care of that for you ?

[14:37] <MasterDuke> but yeah, those look unused...

[14:37] <MasterDuke> maybe

[14:38] <MasterDuke> possibly lots more syscalls though, which might be slower even if buffered

[14:38] <MasterDuke> nine: did you do any benchmarking of the writing?

[14:38] <lizmat> .oO( only 5 Rakuers missing until /r/rakulang turns into a teapot )

[14:40] <MasterDuke> teapot?

[14:41] <lizmat> https://en.wikipedia.org/wiki/Hyper_Text_Coffee_Pot_Control_Protocol  :-)

[14:43] <MasterDuke> ah, number of readers

[14:47] <lizmat> :-)

[14:49] <nine> MasterDuke: no, I didn't try without that big buffer

[14:55] <MasterDuke> hm, might not get to it this evening, but maybe i'll try changing to write (at least somewhat) on the fly and see what the differences are

[15:09] *** synthmeat left
[15:32] *** synthmeat joined
[16:16] *** cognominal joined
[16:29] *** sena_kun joined
[16:31] *** Altai-man_ left
[16:31] *** sena_kun left
[16:54] *** sena_kun joined
[17:43] <Geth> ¬¶ rakudo: 4bbe308bef | (Ben Davies)++ | 3 files

[17:43] <Geth> ¬¶ rakudo: Make a bunch of traits around 2% faster

[17:43] <Geth> ¬¶ rakudo: 

[17:43] <Geth> ¬¶ rakudo: The performance benefits of not creating any symbols for unused named

[17:43] <Geth> ¬¶ rakudo: parameters aren't huge, but they exist.

[17:43] <Geth> ¬¶ rakudo: review: https://github.com/rakudo/rakudo/commit/4bbe308bef

[17:44] <moritz> nice trick!

[18:02] <Geth> ¬¶ nqp: a10d8a394d | (Daniel Green)++ | src/vm/moar/QAST/QASTCompilerMAST.nqp

[18:02] <Geth> ¬¶ nqp: Remove some unused variables

[18:02] <Geth> ¬¶ nqp: review: https://github.com/Raku/nqp/commit/a10d8a394d

[18:09] <nine> MasterDuke: LOL...quite obvious, isn't it :)

[18:10] <MasterDuke> lizmat++ for spotting it

[18:13] <vrurg> Unfortunately, I'm late for the discussion. But I left some thoughts on what could be done for dispatchers in https://github.com/Raku/problem-solving/issues/170

[18:16] <Geth> ¬¶ rakudo: a4fbbfa334 | (Ben Davies)++ | 4 files

[18:16] <Geth> ¬¶ rakudo: Make a minor optimization to a bunch of STORE methods

[18:16] <Geth> ¬¶ rakudo: 

[18:16] <Geth> ¬¶ rakudo: Same method of optimization as in

[18:16] <Geth> ¬¶ rakudo: 4bbe308befcfddbef5b5ca1634b524ed673449db.

[18:16] <Geth> ¬¶ rakudo: review: https://github.com/rakudo/rakudo/commit/a4fbbfa334

[18:24] <Kaiepi> those are the two hot spots i can think of offhand where that optimization would be useful, there are probably more out there

[18:28] *** Altai-man_ joined
[18:31] *** sena_kun left
[19:03] *** MasterDuke left
[19:13] *** MasterDuke joined
[19:32] <Geth> ¬¶ rakudo: 2f8538ed05 | (Ben Davies)++ | src/core.c/Parameter.pm6

[19:32] <Geth> ¬¶ rakudo: Don't loop over @!named_names for *% parameters in Parameter.raku

[19:32] <Geth> ¬¶ rakudo: 

[19:32] <Geth> ¬¶ rakudo: @!named_names should always be null for slurpy named parameters.

[19:32] <Geth> ¬¶ rakudo: review: https://github.com/rakudo/rakudo/commit/2f8538ed05

[20:29] *** sena_kun joined
[20:30] *** Altai-man_ left
[20:45] <Kaiepi> jnthn, could you review https://github.com/rakudo/rakudo/pull/3451 ?

[20:46] <Kaiepi> the pr you commented on a while ago is something i plan on revisiting later

[21:07] <jnthn> Kaiepi: Will try and do it tomorrow. Does it make any backward-incompatible changes, or just aditions?

[21:08] <Kaiepi> additions

[21:09] <Kaiepi> should be backwards compatible

[21:15] *** Kaiepi left
[21:16] *** Kaiepi joined
[21:17] *** Kaiepi left
[21:18] *** Kaiepi joined
[21:19] *** Kaiepi left
[21:20] *** Kaiepi joined
[21:21] *** Kaiepi left
[21:31] *** Kaiepi joined
[21:32] *** Kaiepi left
[21:32] *** Kaiepi joined
[21:35] *** Kaiepi left
[21:38] *** Kaiepi joined
[22:28] *** Altai-man_ joined
[22:31] *** sena_kun left
[22:35] *** lucasb left
[22:36] *** leont left
[22:55] *** Altai-man_ left
[23:14] *** rba[m] joined
[23:23] *** b2gills left
[23:28] *** b2gills joined
