[00:39] *** Kaiepi left
[00:42] *** Kaiepi joined
[00:59] *** wildtrees left
[01:03] *** softmoth joined
[01:20] *** guifa2 left
[02:18] *** guifa2 joined
[02:24] *** guifa2_ joined
[02:24] *** guifa2 left
[02:24] *** guifa2_ is now known as guifa2

[02:45] <[Coke]> greppable6: nqp::without

[02:46] <greppable6> [Coke], Found nothing!

[02:46] <[Coke]> greppable6: without

[02:48] <AlexDaniel> [Coke]: it'll take some time to gist that :D

[02:48] <[Coke]> no doubt.

[02:48] <[Coke]> just trying to figure out if the with/without opcodes are used anywhere other than the nqp test suite.

[02:49] <greppable6> [Coke], 8380 lines, 1066 modules: https://gist.github.com/f47cc0101e69eeb71fc1fafb4e7ec1ca

[02:49] <AlexDaniel> heh, it did it

[02:50] <AlexDaniel> I thought it's going to be in the sweet spot of not being enough for it to just bail but enough for it to keep trying forever

[02:58] * [Coke] wonders if there's a prebuilt recent nqp-jvm somewhere to test things on

[03:23] <[Coke]> https://news.perlfoundation.org/post/gc_proposals_2020-03 We did have a grant proposal come in for Raku for this period.

[03:56] *** evalable6 left
[03:56] *** linkable6 left
[03:56] *** linkable6 joined
[03:57] *** evalable6 joined
[06:13] *** coverable6 left
[06:13] *** releasable6 left
[06:13] *** reportable6 left
[06:13] *** committable6 left
[06:13] *** statisfiable6 left
[06:13] *** notable6 left
[06:13] *** squashable6 left
[06:13] *** quotable6 left
[06:13] *** unclechu left
[06:15] *** coverable6 joined
[06:15] *** releasable6 joined
[06:15] *** reportable6 joined
[06:15] *** committable6 joined
[06:15] *** statisfiable6 joined
[06:15] *** notable6 joined
[06:15] *** squashable6 joined
[06:15] *** quotable6 joined
[06:18] *** unclechu joined
[06:48] *** lichtkind joined
[07:16] *** ufobat__ left
[07:50] <MasterDuke> nqp-j: say("hi?")

[08:45] *** softmoth left
[09:32] *** Altai-man_ joined
[09:41] <lizmat> Files=1306, Tests=111236, 212 wallclock secs (28.87 usr  8.20 sys + 3003.32 cusr 265.36 csys = 3305.75 CPU)

[09:50] <Geth> ¦ rakudo: 77a2201e4e | (Stefan Seifert)++ | src/core.c/ThreadPoolScheduler.pm6

[09:50] <Geth> ¦ rakudo: Fix ThreadPoolScheduler only adding affinity threads in extreme cases

[09:50] <Geth> ¦ rakudo: 

[09:50] <Geth> ¦ rakudo: Swapped branches of an nqp::if caused us to only ever consider the highest

[09:50] <Geth> ¦ rakudo: threshold for adding a new affinity-worker (queue length 100).

[09:50] <Geth> ¦ rakudo: review: https://github.com/rakudo/rakudo/commit/77a2201e4e

[09:50] <Geth> ¦ rakudo: 750abe0386 | (Stefan Seifert)++ | src/core.c/ThreadPoolScheduler.pm6

[09:50] <Geth> ¦ rakudo: Only take really idle affinity workers straight away in ThreadPoolScheduler

[09:50] <Geth> ¦ rakudo: 

[09:50] <Geth> ¦ rakudo: Looking at the queue length alone doesn't tell us whether a worker is idle or

[09:50] <Geth> ¦ rakudo: not. We also have to take it's working flag into account. Otherwise we may end

[09:50] <Geth> ¦ rakudo: up picking a busy worker straight away when there's an actually idle one

[09:51] <Geth> ¦ rakudo: standing by.

[09:51] <Geth> ¦ rakudo: review: https://github.com/rakudo/rakudo/commit/750abe0386

[10:18] *** lichtkind_ joined
[10:20] *** lichtkind left
[10:25] <MasterDuke> nine: what sort of workloads will those ^^^ commits help with?

[10:26] *** Altai-man_ left
[10:26] *** sena_kun joined
[10:28] <nine> MasterDuke: workloads with lots of IO::Socket::Async, Proc::Async and IO::Notification usage. In my case two Cro applications (web frontend using a REST API providing backend)

[10:29] <MasterDuke> nice

[10:47] *** hankache joined
[10:59] *** hankache left
[11:14] *** Altai-man_ joined
[11:17] *** sena_kun left
[11:47] <nine> releasable6: status

[11:47] <releasable6> nine, Next release will happen when it's ready. 5 blockers. 166 out of 335 commits logged (⚠ 3 warnings)

[11:47] <releasable6> nine, Details: https://gist.github.com/2a42dd9d46c2b98237aa6910654629a7

[11:51] <Altai-man_> nine, looking for blockers to look at?

[11:53] <nine> Doesn't look like any of them are about something I'd work on

[11:55] <Altai-man_> I think main ones are dispatch, which is on jnthn, and windows segfault.

[11:56] <nine> Which look like they're the same

[11:58] <Altai-man_> Extremely intricate for sure.

[11:58] <MasterDuke> jnthn has said he didn't think the windows segfault was in fact related to dispatch

[11:58] <MasterDuke> iirc

[12:26] *** Kaiepi left
[12:27] *** Kaiepi joined
[12:28] *** Kaiepi left
[12:37] <lizmat> so, thinking a lot about my ... (series operator) optimizing work

[12:37] <lizmat> I've basically come to the conclusion that I don't want to mimic all of its current quirks in a new version

[12:38] <lizmat> so I'm considering now to either put this work into a module in the ecosystem

[12:38] <lizmat> or making the less quirky behaviour a 6.e thing only

[12:39] <lizmat> jnthn moritz nine vrurg opinions?

[12:41] <moritz> sounds sensible to me

[12:41] <lizmat> 6.e or module ?

[12:42] <moritz> doing it as a module first, and if it works out well, propose it for 6.e

[12:43] <lizmat> suggestions for the name of the module?  Operator::Series ?

[12:43] <moritz> Better::Series (j/k) :D

[12:44] <moritz> Operator::Series sounds good

[12:49] <lizmat> actually, making it a module, might allow us to remove ... from the core completely at some point in the future, which some of us apparently like to see  :-)

[12:49] <timotimo> we should have a namespace that's like ACME, but not for jokes. how about Serious::

[12:49] <timotimo> `use Serious::Firepower`

[12:50] <lizmat> .oO( phasers on desintegrate )

[12:50] <timotimo> `use Dys::Functional`

[12:54] <lizmat> hmmm  Sequence::Generator ?

[12:54] <Altai-man_> `Seq::Generator`?

[12:54] <Altai-man_> If it is not misleading.

[12:56] <lizmat> inspired by this from the speculcation:

[12:56] <lizmat> "The righthand first value is considered to be the endpoint or limit of the sequence that is to be generated from the lefthand side by the C<...> operator itself."

[12:56] <AlexDaniel> lizmat: why not use `experimental`?

[12:57] <lizmat> what would be the difference with that and putting it in 6.e.PREVIEW ?

[12:57] <AlexDaniel> ah no, 6.e.PREVIEW is even better

[12:59] <AlexDaniel> lizmat++ # brave soul

[13:02] <lizmat> I think I'm going to go for a separate module now for 2 reasons

[13:02] <nine> Sounds to me like you want to develop it in a module anyway, for the much shorter compile cycles alone

[13:02] <lizmat> 1. it makes developing faster (what nine said)

[13:02] <lizmat> 2. it would allow people who are stuck on 6.c or 6.d for some reason, to still use it

[13:02] <nine> I should have done that with the ThreadPoolScheduler, too :)

[13:03] <lizmat> nine: yeah, maybe that *is* a good point  :-)

[13:03] <lizmat> ah, and 3: would potentially allow export to a different operator name, for comparisons  :-)

[13:05] <nine> Oh, yes, that should make debugging a lot easier

[13:15] *** sena_kun joined
[13:16] *** Altai-man_ left
[13:18] *** Xliff joined
[13:18] <Xliff> \o

[13:19] <Xliff> Do other language have the Slip concept? As in, are there languages other than raku that can take an array and convert it to positional parameters?

[13:21] <nine> In Python it's foo(*array)

[13:21] <sena_kun> Not sure about Slip, but there are things for doing the second thing in plenty of other languages. Java has ..., python has *, etc.

[13:21] <lizmat> of course, Slip is more general than that

[13:42] <Xliff> Yes, but those sound like it's from the calling end. I was more concerned with caller.

[13:42] <Xliff> :)

[13:46] <Xliff> Can someone tell my why this is dying with an X::Assignment::RO?

[13:46] <Xliff> https://repl.it/repls/SpatialUtterDemand

[13:51] <Xliff> Ah. Nevermind.

[14:20] *** Kaiepi joined
[14:36] <Xliff> Anybody know of an algorithm that can pull the largest, most commonly used substring from a text document?

[14:38] <Xliff> Hmmm... maybe this might do. 

[14:38] <Xliff> ### /usr/include/gstreamer-1.0/gst/base/gstbasesink.h

[14:38] <Xliff> sub gst_base_sink_do_preroll (GstBaseSink $sink, GstMiniObject $obj)

[14:38] <Xliff>   returns GstFlowReturn

[14:38] <Xliff>   is native(gstreamer-base)

[14:38] <Xliff>   is export

[14:38] <Xliff> { * }

[14:38] <Xliff> sub gst_base_sink_get_blocksize (GstBaseSink $sink)

[14:38] <Xliff>   returns guint

[14:38] <Xliff>   is native(gstreamer-base)

[14:38] <Xliff>   is export

[14:38] <Xliff> { * }

[14:38] <Xliff> sub gst_base_sink_get_drop_out_of_segment (GstBaseSink $sink)

[14:38] <Xliff>   returns uint32

[14:38] <Xliff>   is native(gstreamer-base)

[14:38] <Xliff>   is export

[14:38] <Xliff> { * }

[14:38] <Xliff> sub gst_base_sink_get_last_sample (GstBaseSink $sink)

[14:38] <Xliff>   returns CArray[GstSample]

[14:38] <Xliff>   is native(gstreamer-base)

[14:38] <Xliff>   is export

[14:38] <Xliff> { * }

[14:38] <Xliff> sub gst_base_sink_get_latency (GstBaseSink $sink)

[14:39] <Xliff>   returns GstClockTime

[14:39] <Xliff>   is native(gstreamer-base)

[14:39] <Xliff>   is export

[14:39] <Xliff> { * }

[14:39] <lizmat> oops?

[14:39] <Xliff> sub gst_base_sink_get_max_bitrate (GstBaseSink $sink)

[14:39] <Xliff>   returns guint64

[14:39] <Xliff>   is native(gstreamer-base)

[14:39] <Xliff>   is export

[14:39] <Xliff> { * }

[14:39] <Xliff> sub gst_base_sink_get_max_lateness (GstBaseSink $sink)

[14:39] <Xliff>   returns gint64

[14:39] <Xliff>   is native(gstreamer-base)

[14:39] <Xliff>   is export

[14:39] <Xliff> { * }

[14:39] <Xliff> sub gst_base_sink_get_processing_deadline (GstBaseSink $sink)

[14:39] <Xliff>   returns GstClockTime

[14:39] <Xliff>   is native(gstreamer-base)

[14:39] <Xliff>   is export

[14:39] <Xliff> { * }

[14:39] <Xliff> sub gst_base_sink_get_render_delay (GstBaseSink $sink)

[14:39] <Xliff>   returns GstClockTime

[14:39] <Xliff>   is native(gstreamer-base)

[14:39] <Xliff>   is export

[14:39] <Xliff> { * }

[14:39] <Xliff> sub gst_base_sink_get_sync (GstBaseSink $sink)

[14:39] <Xliff>   returns uint32

[14:39] <Xliff>   is native(gstreamer-base)

[14:39] <Xliff>   is export

[14:40] <Xliff> { * }

[14:40] <Xliff> sub gst_base_sink_get_throttle_time (GstBaseSink $sink)

[14:40] <Xliff>   returns guint64

[14:40] <Xliff>   is native(gstreamer-base)

[14:40] <lizmat> wonders how long it will take before Xliff is bumped

[14:40] <Xliff>   is export

[14:40] <Xliff> { * }

[14:40] <Xliff> sub gst_base_sink_get_ts_offset (GstBaseSink $sink)

[14:40] <Xliff>   returns GstClockTimeDiff

[14:40] <Xliff>   is native(gstreamer-base)

[14:40] <Xliff>   is export

[14:40] <Xliff> { * }

[14:40] <Xliff> sub gst_base_sink_is_async_enabled (GstBaseSink $sink)

[14:40] <Xliff>   returns uint32

[14:40] <Xliff>   is native(gstreamer-base)

[14:40] <Xliff>   is export

[14:40] <Xliff> { * }

[14:40] <Xliff> sub gst_base_sink_is_last_sample_enabled (GstBaseSink $sink)

[14:40] <Xliff>   returns uint32

[14:40] <Xliff>   is native(gstreamer-base)

[14:40] <Xliff>   is export

[14:40] *** Xliff left
[14:42] <lizmat> bisectable6: dd PredictiveIterator

[14:42] <bisectable6> lizmat, Bisecting by exit code (old=2015.12 new=750abe0). Old exit code: 1

[14:42] <bisectable6> lizmat, bisect log: https://gist.github.com/0484b1582d5f70fe81e1245c8b47bc5b

[14:42] <bisectable6> lizmat, (2019-05-14) https://github.com/rakudo/rakudo/commit/ec97878005e2cce8288d9b1afa5fe8107b410194

[14:43] <lizmat> bisectable6: class A does PredictiveIterator { }

[14:43] <bisectable6> lizmat, Bisecting by output (old=2015.12 new=750abe0) because on both starting points the exit code is 1

[14:44] <bisectable6> lizmat, bisect log: https://gist.github.com/84ddca7d7f4485991bf210dd33ce9379

[14:44] <bisectable6> lizmat, (2018-09-09) https://github.com/rakudo/rakudo/commit/3ffacb9f74b7a30475d491a2c33ceb37262026ac

[14:45] <lizmat> hmmm... 2.5 years ago, just before 6.d 

[14:51] <guifa2> Xliff: you'd need to define largest and most common.  Those are two different ways to score that can go against each other.  "abcabcabca" -> "abc" x 3, but "a" x 4.

[14:51] <tellable6> guifa2, I'll pass your message to Xliff

[14:55] <jnthn> lizmat: Please can you make a problem solving ticket detailing the behaivors/quirks you'd like to remove? I think it's also useful to know which ones are actually spec (as in, covered by spectest in 6.d) and which are just properties of the previous implementation. Also ecosystem reliance.

[14:56] <lizmat> will do as soon as the module is fleshed out and I have an initial set of capabilities

[14:56] <jnthn> I mean, if it turns out that most things you'd like to drop have no spectests and nearly no ecosystem dependence, then the question is quite different.

[14:56] <lizmat> indeed...

[14:56] <lizmat> well, one thing I'd like to drop, is chaining

[14:56] <lizmat> m: dd 1 ... 5 ... 1

[14:56] <camelia> rakudo-moar 750abe038: OUTPUT: «(1, 2, 3, 4, 5, 4, 3, 2, 1).Seq␤»

[14:56] <lizmat> would have to be written as:

[14:56] <lizmat> m: dd 1 ... (5 ... 1)

[14:56] <camelia> rakudo-moar 750abe038: OUTPUT: «(1, 2, 3, 4, 5, 4, 3, 2, 1).Seq␤»

[14:57] <jnthn> Hm, but if the second form is well defined then so is the first, by associativity?

[14:57] <lizmat> m: dd (1 ... 5) ... 1

[14:57] <camelia> rakudo-moar 750abe038: OUTPUT: «(1,).Seq␤»

[14:57] <timotimo> right, because the last argument is a "end condition" and also a filter

[14:57] <jnthn> Or does it associate wrong and then we have a hack to deal with that?

[14:58] <timotimo> m: dd 5 ... 1 ... 5

[14:58] <camelia> rakudo-moar 750abe038: OUTPUT: «(5, 4, 3, 2, 1, 2, 3, 4, 5).Seq␤»

[14:58] <timotimo> i sure hope we have it as assoc list

[14:59] <lizmat> nqp::hash('prec', 'f=', 'assoc', 'list');

[14:59] <jnthn> Oh goodness, it comes out as &infix:<...>(1, 5, 1)

[14:59] <jnthn> Right. D'oh.

[15:00] <lizmat> m: dd 1 ... 5 ...^ 1

[15:00] <camelia> rakudo-moar 750abe038: OUTPUT: «5===SORRY!5=== Error while compiling <tmp>␤Only identical operators may be list associative; since '...' and '...^' differ, they are non-associative and you need to clarify with parentheses␤at <tmp>:1␤------> 3dd 1 ... 57⏏5 ...^ 1␤  …»

[15:01] <lizmat> m: dd 1 ...^ 5 ...^ 1   # now what does that mean?

[15:01] <camelia> rakudo-moar 750abe038: OUTPUT: «5===SORRY!5=== Error while compiling <tmp>␤Calling infix:<...^>(Int, Int, Int) will never work with signature of the proto ($, Mu, *%)␤at <tmp>:1␤------> 3dd 1 ...^ 5 7⏏5...^ 1   # now what does that mean?␤»

[15:01] <jnthn> Nothing useful. :)

[15:01] <lizmat> indeed

[15:01] <jnthn> Is 1 ... 5 ... 1 spec?

[15:01] <lizmat> it is tested for

[15:01] <jnthn> OK

[15:02] <jnthn> Yes, when I say spec that's what I mean :)

[15:02] <lizmat> not sure it is documented though  :-)

[15:02] <jnthn> Docs aren't spec.

[15:02] <lizmat> true  :-)

[15:02] <jnthn> *sigh* The number of language tickets is huge. I think I need to commit to ruling on one a week or something. It'll at lesat be something.

[15:02] <jnthn> *least

[15:03] <jnthn> And doesn't feel so overwhelming. :)

[15:03] <lizmat> ++jnthn

[15:14] *** Altai-man_ joined
[15:16] *** sena_kun left
[15:17] <lizmat> afk for a few hours&

[15:25] *** Xliff joined
[15:25] <Xliff> lizmat: HA! I bumped myself! ;D

[15:25] <tellable6> 2020-04-19T14:51:35Z #raku-dev <guifa2> Xliff: you'd need to define largest and most common.  Those are two different ways to score that can go against each other.  "abcabcabca" -> "abc" x 3, but "a" x 4.

[15:27] <Xliff> guifa: It would be longest common substring, which would never be just "a", since "abc" is longer.

[15:29] <guifa2> Ah, LCS is a different problem :-)  So you're comparing two strings ?  

[15:30] <Xliff> No. I want to extract the longest substring from a given piece of text

[15:46] <MasterDuke> Xliff: isn't that just the given piece of text itself?

[15:48] <guifa2> I think Xliff is wanting any the longest substring that's repeated at least once

[15:48] <guifa2> m: sub lcs(\t) { my Int %r; for 0 ..^ t.chars -> \f { for f ..^ t.chars -> \l { %r{t.substr: f, l}++ }; }; %r.pairs.grep(*.value > 1).sort(*.key.chars).tail.key; }; say lcs("abcdabcdabc")

[15:48] <camelia> rakudo-moar 750abe038: OUTPUT: «cdabcdabc␤»

[15:48] <guifa2> It's brute forcing the problem, but it works

[15:49] <Xliff> guifa: Yeah, but to do that for a whole piece of text gets long.

[15:49] <Xliff> And you are incorrect.

[15:49] <Xliff> I don't need the longest substring of a string. I need the logest common substring out of a piece of text

[15:50] <Xliff> Take my almost-foray-into-bumpage, earlier: https://github.com/Xliff/p6-GStreamer/blob/master/lib/GStreamer/Raw/Base/BaseSink.pm6

[15:51] <Xliff> The algorithm I want should pull gst_base_sink as one of the longest substrings.

[15:52] <MasterDuke> m: say "which is the longest".words.sort(*.char).head(1)

[15:52] <camelia> rakudo-moar 750abe038: OUTPUT: «No such method 'char' for invocant of type 'Str'. Did you mean any of these?␤    can␤    chars␤    chop␤    chr␤␤  in block <unit> at <tmp> line 1␤␤»

[15:53] <MasterDuke> m: say "which is the longest".words.sort(*.chars).head(1)

[15:53] <camelia> rakudo-moar 750abe038: OUTPUT: «(is)␤»

[15:53] <MasterDuke> m: say "which is the longest".words.sort(*.chars).tail(1)

[15:53] <camelia> rakudo-moar 750abe038: OUTPUT: «(longest)␤»

[15:55] <Xliff> Again, not the problem space!

[15:55] <MasterDuke> m: say "which is the longest of the words in the string of words".words.Bag.grep(*.value > 1).sort(*.key.chars).tail(1)

[15:55] <camelia> rakudo-moar 750abe038: OUTPUT: «(words => 2)␤»

[15:55] <Xliff> Take this for a starting point: https://repl.it/repls/SpatialUtterDemand

[15:55] <Xliff> If I start out with that list, I want to get gst_base_sink_ as the longest common substring.

[16:02] <MasterDuke> that sounds computationally expensive

[16:04] <Xliff> Yeah.

[16:06] <Geth> ¦ rakudo/rakuast: a9bde11299 | (Jonathan Worthington)++ | 6 files

[16:06] <Geth> ¦ rakudo/rakuast: Implement RakuAST support for dynamic variables

[16:06] <Geth> ¦ rakudo/rakuast: 

[16:06] <Geth> ¦ rakudo/rakuast: Both declaration and access. The lookup is done a little differently

[16:06] <Geth> ¦ rakudo/rakuast: from the current compiler, and should be a bit more efficient, at the

[16:06] <Geth> ¦ rakudo/rakuast: cost of slightly larger code size.

[16:06] <Geth> ¦ rakudo/rakuast: review: https://github.com/rakudo/rakudo/commit/a9bde11299

[16:07] <guifa2> LCS in any form is a (computationally) hard problem.  When you can set out initial conditions for it, there are ways to substantially speed it up.

[16:07] <guifa2> For instance, if we can have the assumption that the substring won't cross words boundaries, we can first divide the text up by words, which means the inner loop in my code above won't ever be run more than $largest-words.chars, probably reducing the runtime by two orders of magnitude, and then the sort() will likewise be speed up by at least an order of magnitude

[16:07] <Xliff> Yeah. I think I got it.

[16:09] <Xliff> 1) Go through list of common words (ala words that start with a prefix). 2) For first run through, return value is the current return value 3) If the next value compared with the return value has a smaller substring, then that substring is the current return value

[16:09] <Xliff> 4) return the current return value

[16:10] <Xliff> Yeah, there is a trick to it. That is prone to bugs if there are multiple substrings in the list.

[16:10] <Xliff> I don't know if I need to worry about that in the current space, though.

[16:16] <MasterDuke> Xliff: http://blogs.perl.org/users/damian_conway/2019/07/chopping-substrings.html looks like it might be useful

[16:16] <Xliff> MasterDuke++

[16:17] <Geth> ¦ Blin: 70c2390250 | (Aleks-Daniel Jakimenko-Aleksejev)++ | bin/blin.p6

[16:17] <Geth> ¦ Blin: Don't write the file on every iteration (oops!)

[16:17] <Geth> ¦ Blin: review: https://github.com/Raku/Blin/commit/70c2390250

[16:17] <AlexDaniel> Xliff: ↑ :D

[16:19] <Xliff> :p

[16:20] <Xliff> AlexDaniel++: Good catch

[16:22] <Xliff> MasterDuke: That works! Check https://repl.it/repls/SpatialUtterDemand

[16:23] *** softmoth joined
[16:24] <MasterDuke> nice

[16:32] *** Xliff left
[16:34] <Geth> ¦ Blin: c8ed4d1969 | (Aleks-Daniel Jakimenko-Aleksejev)++ | bin/blin.p6

[16:34] <Geth> ¦ Blin: Indent the :to block to make the code more readable

[16:34] <Geth> ¦ Blin: review: https://github.com/Raku/Blin/commit/c8ed4d1969

[16:43] *** lichtkind_ left
[17:15] *** sena_kun joined
[17:17] *** Altai-man_ left
[17:27] *** MasterDuke left
[17:28] * guifa2 finally got a handle on P6Regex/Grammar.nqp and got a Binex version up and running

[17:28] <guifa2> Now to tackle Actions.nqp .  Much scarier lol

[17:31] <lizmat> guifa2: care to blog about your accomplishments ?

[17:44] *** MasterDuke joined
[17:44] <[Coke]> nqp:  nqp::js

[17:44] <camelia> nqp-moarvm: OUTPUT: «Running JS NYI on MoarVM␤   at <tmp>:1  (<ephemeral file>:<mainline>)␤ from gen/moar/stage2/NQPHLL.nqp:1916  (/home/camelia/rakudo-m-inst-1/share/nqp/lib/NQPHLL.moarvm:eval)␤ from gen/moar/stage2/NQPHLL.nqp:2121  (/home/camelia/rakudo-m-inst-1/share/nqp/li…»

[17:44] <[Coke]> pmurias: ^^ ?

[17:44] <tellable6> [Coke], I'll pass your message to pmurias

[17:45] <AlexDaniel> ehhh tellable6 should probably include a link to irc logs

[17:45] <AlexDaniel> pmurias: context: https://colabti.org/irclogger/irclogger_log/raku-dev?date=2020-04-19#l329

[17:45] <tellable6> AlexDaniel, I'll pass your message to pmurias

[17:49] <[Coke]> if all an opcode does is throw a NYI exception, should we include it? (I'd vote no given that we already have a variety of implemented codes)

[17:58] *** [TuxCM] left
[18:17] *** pmurias joined
[18:17] <pmurias> [Coke]: hi

[18:17] <tellable6> 2020-04-19T17:44:51Z #raku-dev <[Coke]> pmurias: ^^ ?

[18:17] <tellable6> 2020-04-19T17:45:59Z #raku-dev <AlexDaniel> pmurias: context: https://colabti.org/irclogger/irclogger_log/raku-dev?date=2020-04-19#l329

[18:18] <pmurias> [Coke]: re nqp::js it was used as a debugging aid as that inserts a bit of js code verbatim into emitted JS code

[18:18] *** Xliff joined
[18:19] <pmurias> [Coke]: the reason it's stubbed on the moar backend is that when cross compiling parts of NQP where being compiled to moar bytecode and js code at the same time

[18:20] <pmurias> [Coke]: but I guess you could remove it at that point from moar as it was added to help debugging at an earler stage of nqp-js development

[18:21] <pmurias> although it can be usefull if you are emitting js code from an nqp running on moar

[18:26] *** pmurias left
[18:30] <guifa2> lizmat: I definitely will

[18:51] *** softmoth left
[18:51] *** softmoth joined
[18:55] <lizmat> m: dd 1 ...^ 5.5    # so how do we feel about that *not* excluding 5 ?

[18:55] <camelia> rakudo-moar 750abe038: OUTPUT: «(1, 2, 3, 4, 5).Seq␤»

[18:56] *** softmoth left
[18:57] <timotimo> MasterDuke: are you available for another core setting profile with a moarvm patch i have?

[19:00] <MasterDuke> timotimo: sure. on top of your other branch?

[19:01] <timotimo> umm i think it will be

[19:02] <timotimo> i thought it was already compiling but i just misread the output

[19:02] <timotimo> it'll take a little time still

[19:06] <timotimo> ok i just pushed it

[19:08] <MasterDuke> interesting, what do you think the effects will be?

[19:09] <timotimo> well if my hope is correct, 20x faster

[19:09] <timotimo> oh, if you could "perf record" it as well, that'd be cool

[19:14] <MasterDuke> ha, good thing i forgot to disable my swap file

[19:14] <MasterDuke> just started it

[19:14] *** Altai-man_ joined
[19:17] *** sena_kun left
[19:28] <MasterDuke> timotimo: hot damn, what did you do?!?! stage parse only took almost twice as long (i.e., 60s instead of the normal 37s) instead of the 930s last time i did the profile

[19:29] <MasterDuke> but we'll see how long it takes to write the profile now...

[19:36] <MasterDuke> oh hm, mark_call_graph_node() doesn't do anything now?

[19:37] <timotimo> that's correct

[19:48] <timotimo> because why not make the thing that did 95% of everything before do nothing at all after

[19:48] <AlexDaniel> lizmat: what if we start by saying that it should count forever

[19:49] <AlexDaniel> m: say (1, * + 1 ...^ 5.5)[^20]

[19:49] <camelia> rakudo-moar 750abe038: OUTPUT: «(1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20)␤»

[19:50] <AlexDaniel> any other behavior makes it a special case. Maybe it is justified, but if you want to end up with sane behavior it's probably better to start with no special cases and then see where we stand

[20:11] <AlexDaniel> reading the docs: “The right-hand side will have an endpoint, which can be Inf or * for "infinite" lists (that is, lazy lists whose elements are only produced on demand)”

[20:12] <AlexDaniel> wrong… as if in other cases it'll be eager or something…

[20:12] <timotimo> MasterDuke:i assume writing out the data is still enormously slow?

[20:14] <MasterDuke> slow, but definitely faster than before. currently at 2.6g written

[20:16] <MasterDuke> wow, the profiling is done! just perf writing it's data now

[20:18] <MasterDuke> so looks like it was about one hour, right?

[20:18] <timotimo> like many other things, i should have done this years ago

[20:18] <MasterDuke> nine should love this

[20:19] <MasterDuke> perf says the most expensive at 44% is VMArray_gc_mark. next at 4.2% is MVM_gc_root_add_frame_registers_to_worklist

[20:19] <timotimo> OK

[20:20] <timotimo> oh, did you get the memory usage peak, too?

[20:20] <timotimo> that shouldn't have decreased, i don't think

[20:22] <AlexDaniel> “An endpoint of * (Whatever), Inf or ∞ generates on demand an infinite sequence, with a default generator of *.succ”

[20:22] <AlexDaniel> again, wrong

[20:22] <MasterDuke> it looked about the same as before. all ram used, ~28b swap

[20:22] <AlexDaniel> as in, it depends on the elements you have on the left side

[20:22] <timotimo> right

[20:22] <MasterDuke> i didn't run that other thing. what was it?

[20:23] <timotimo> what other thing, you mean printing MVM_profiler_measure_blah in gdb?

[20:23] <timotimo> that's not quite as important this time, there was barely a change

[20:24] <MasterDuke> smem

[20:24] <MasterDuke> but yeah, didn't do either

[20:25] <AlexDaniel> lizmat: there's one interesting thing we can do. So, … first needs to decide whether it is going to call .succ or .pred, so we have the result of this comparison. If the endpoint is not Callable, can't we just keep comparing it like this?

[20:26] <timotimo> ah smem

[20:26] <timotimo> i love that tool it's so good

[20:28] <AlexDaniel> lizmat: if it's Same then it's our destination, if it's not Same and is different, then we overshot

[20:30] <nine> timotimo: so how does your speedup work?

[20:30] <timotimo> very well, thank you

[20:30] <nine> Ah, the master wants to keep his secrets :)

[20:32] <timotimo> nah, it's super simple

[20:32] <timotimo> you know how the call graph is built up of nodes that each have an MVMStaticFrame pointer?

[20:32] <timotimo> and that's how we identify what successor node to pick when we're invoking a new routine from one node

[20:33] <AlexDaniel> lizmat: though it feels like it's going to break apart either way cuz now we have to make something up if code blocks are involved

[20:33] <timotimo> (that's also how each node's "allocations" counter works)

[20:33] <timotimo> well, it turns out that if you put a pointer to a gc-managed entity in every node of a tree (plus in these alloc entries that hang off of every node) then you also have to traverse the entire tree on every GC run

[20:34] <nine> Aaah...and the array makes that so much simpler and faster

[20:34] <timotimo> because otherwise pointers get outdated and you get 1) duplicate nodes of the same staticframe, and 2) a crash at the end when you're trying to extract name and filename and such from an SF that's wrong

[20:34] <timotimo> yes, all type and staticframe pointers have moved to a flat array and there's now only an index into the array at every node

[20:34] <timotimo> no more need to traverse a tree at all

[20:35] <nine> brilliant :)

[20:35] <timotimo> the memory access pattern also shrinks drastically from leaving the tree alone unless you're actually actively moving around in it from invoking and returning

[20:36] <timotimo> though a linear search through the array isn't great, already loads slower than before, where it was literally just a pointer comparison

[20:48] <timotimo> next step could be to allocate the succ arrays in the FSA, since they are likely often small

[20:49] <timotimo> hmm. can we actually sensibly surpass 32bit counts of allocating stuff?

[20:50] <timotimo> hm. how often per second would we have to allocate an object to surpass 32bit in a week, let's say

[20:50] <timotimo> m: my $limit = 2 ** 32; my $time = 7 * 24 * 60 * 60; say $limit / $time

[20:50] <camelia> rakudo-moar 750abe038: OUTPUT: «7101.46709␤»

[20:50] <timotimo> oh, that is possible

[20:51] *** softmoth joined
[21:15] *** sena_kun joined
[21:17] *** Altai-man_ left
[21:25] *** lichtkind joined
[21:32] <timotimo> m: say (2 ** 32 * 200) / 1024 / 1024

[21:32] <camelia> rakudo-moar 750abe038: OUTPUT: «819200␤»

[21:33] <timotimo> m: say (2 ** 32 * 200) / 1024 / 1024 / 1024

[21:33] <camelia> rakudo-moar 750abe038: OUTPUT: «800␤»

[21:33] <timotimo> m: say (2 ** 16 * 200) / 1024 / 1024 / 1024

[21:33] <camelia> rakudo-moar 750abe038: OUTPUT: «0.012207␤»

[21:33] <timotimo> m: say (2 ** 16 * 200) / 1024 / 1024

[21:33] <camelia> rakudo-moar 750abe038: OUTPUT: «12.5␤»

[21:33] <timotimo> hmpf. with a 32bit index into the types array, we can handle all types if you're spending about 800 gigabytes of your memory on types

[21:34] <timotimo> but going down to 16 will only give you the ability to handle types filling up 12.5 megs

[21:34] <timotimo> now, this is only the STable itself, there'll also be at least one MVMObject that's the WHAT of it

[21:35] <timotimo> and it's unlikely that you'll have so many types of REPRs that don't have REPR_data

[21:35] <timotimo> m: say (2 ** 16 * 224) / 1024 / 1024

[21:35] <camelia> rakudo-moar 750abe038: OUTPUT: «14␤»

[21:37] <AlexDaniel> lizmat: hah, interesting case:

[21:37] <AlexDaniel> m: say (-1, 2, -4 … -128)[^20]

[21:37] <camelia> rakudo-moar 750abe038: OUTPUT: «(-1 2 -4 8 -16 32 -64 128 -256 512 -1024 2048 -4096 8192 -16384 32768 -65536 131072 -262144 524288)␤»

[21:37] <AlexDaniel> I don't know why it doesn't stop

[21:37] <AlexDaniel> I mean, sure, -128 is not in the sequence, but:

[21:37] <AlexDaniel> m: say (-1, 2, -4 … -127)[^20]

[21:37] <camelia> rakudo-moar 750abe038: OUTPUT: «(-1 2 -4 8 -16 32 -64 Nil Nil Nil Nil Nil Nil Nil Nil Nil Nil Nil Nil Nil)␤»

[21:37] <AlexDaniel> m: say (-1, 2, -4 … -129)[^20]

[21:37] <camelia> rakudo-moar 750abe038: OUTPUT: «(-1 2 -4 8 -16 32 -64 128 Nil Nil Nil Nil Nil Nil Nil Nil Nil Nil Nil Nil)␤»

[21:37] <AlexDaniel> so hmmm

[21:48] <timotimo> hmmm

[21:49] <timotimo> if we expect there's only like sixty-four nodes in the tree that exceed "a crapton of allocations ..."

[21:49] <timotimo> the counter can be half-halved into 0b11111...11xxxxx with a little index into yet another array

[22:10] <Geth> ¦ rakudo: 65fdea7dd3 | (Elizabeth Mattijsen)++ | src/core.c/Mu.pm6

[22:10] <Geth> ¦ rakudo: Make Mu.iterator use R:I.OneValue

[22:10] <Geth> ¦ rakudo: 

[22:10] <Geth> ¦ rakudo: The comment was out of date.  Needs a fix in roast where defined Mu value

[22:10] <Geth> ¦ rakudo: is improperly tested for.

[22:10] <Geth> ¦ rakudo: review: https://github.com/rakudo/rakudo/commit/65fdea7dd3

[22:11] <Geth> ¦ roast: 587e01c182 | (Elizabeth Mattijsen)++ | S32-list/iterator.t

[22:11] <Geth> ¦ roast: Properly test for Mu.new.iterator

[22:11] <Geth> ¦ roast: review: https://github.com/Raku/roast/commit/587e01c182

[22:21] <MasterDuke> timotimo: should i try again with your new commit? think it'll be noticeably faster?

[22:22] <timotimo> probably not

[22:22] <timotimo> perhaps it'll use a teensy bitty less memory

[22:43] <AlexDaniel> greppable6: \.\.\.

[22:47] *** lichtkind left
[22:47] <greppable6> AlexDaniel, 10769 lines, 650 modules: https://gist.github.com/01ad490d7f6298250eb478dbf0c8b34a

[22:54] <AlexDaniel> greppable6: \.\.\.\s*[^}]

[22:56] <greppable6> AlexDaniel, 7267 lines, 551 modules: https://gist.github.com/5ce203fae4b8fe89c128030e6986b4ab

[22:58] <AlexDaniel> hmmmmm

[22:58] <AlexDaniel> greppable6: \.\.\.\s*[^}\s]

[22:58] <AlexDaniel> can I do that?

[22:59] <greppable6> AlexDaniel, 5342 lines, 472 modules: https://gist.github.com/6bffcc951482abcbf6c5d8967666e8ec

[23:01] <AlexDaniel> that's still too many

[23:01] <AlexDaniel> greppable6: ,.*\.\.\.\s*[^}\s]

[23:02] <greppable6> AlexDaniel, 1687 lines, 233 modules: https://gist.github.com/de6e02e6c9feed182bf8270416fd1101

[23:04] <AlexDaniel> moritz: there's no way it works, right?? https://github.com/moritz/perl6-all-modules/blob/08c1a2471fcb8508881862f88cb348c464745f42/cpan/MORITZ/Math-Model/examples/spring.pl#L6

[23:04] <AlexDaniel> lizmat: ↑ food for thought :)

[23:05] <AlexDaniel> I'm also surprised how common it is

[23:05] <AlexDaniel> to do stuff like

[23:05] <AlexDaniel> `0.01, * + 0.01 ... 0.1`

[23:05] <AlexDaniel> instead of `0.01, 0.02 ... 0.1`

[23:09] <AlexDaniel> though both seem to be pretty popular

[23:14] *** Altai-man_ joined
[23:17] *** sena_kun left
[23:18] <Geth> ¦ rakudo/rakuast: e40404285a | (Jonathan Worthington)++ | t/12-rakuast/var.t

[23:18] <Geth> ¦ rakudo/rakuast: Cover default/constraint of untyped scalar decl

[23:18] <Geth> ¦ rakudo/rakuast: review: https://github.com/rakudo/rakudo/commit/e40404285a

[23:18] <Geth> ¦ rakudo/rakuast: 7be7d4cfad | (Jonathan Worthington)++ | 2 files

[23:18] <Geth> ¦ rakudo/rakuast: RakuAST handling of untyped array/hash decl

[23:18] <Geth> ¦ rakudo/rakuast: review: https://github.com/rakudo/rakudo/commit/7be7d4cfad

