[00:56] *** gfldex left
[00:56] *** gfldex joined
[01:12] *** MasterDuke joined
[01:14] <MasterDuke> this time i caught an `MoarVM panic: non-AsyncTask fetched from eventloop active work list` in t/spec/S32-io/IO-Socket-Async.t and did an `rr pack`, so hopefully it can be more easily diagnosed

[01:15] *** unicodable6__ left
[01:15] *** unicodable6 joined
[01:40] <MasterDuke> it got an MVM_REPR_ID_MVMNull instead of an MVM_REPR_ID_MVMAsyncTask

[01:46] <MasterDuke> but i know essentially nothing about the io socket async stuff, so i'll try again to catch one of the other errors. but if anybody wants the rr replay i'll save it

[02:29] *** bisectable6 left
[02:29] *** bisectable6 joined
[02:49] <MasterDuke> just caught a fail in t/spec/S02-types/mix.t

[02:49] <MasterDuke> not ok 126 - .roll(100) (2)

[02:49] <MasterDuke> # Failed test '.roll(100) (2)'

[02:49] <MasterDuke> # at t/spec/S02-types/mix.t line 283

[03:39] *** MasterDuke left
[04:48] *** kjp left
[04:48] *** kjp joined
[05:47] *** jdv left
[05:47] *** jdv joined
[06:02] *** leont left
[06:02] *** leont joined
[06:18] <lizmat> MasterDuke: don't worry about that one, it will fail about 1% of the time

[06:18] <lizmat> it's hard to write a test involving randomness

[06:19] *** sena_kun joined
[07:11] *** sena_kun left
[07:39] <nine> https://github.com/Raku/roast/commit/48466a57d0 should help with that

[07:50] <lizmat> well...  I originally chose 100 because the *failure* of the test every now and then, actually ensures the test is correct

[07:50] <lizmat> if we're going to make that chance really negligible then we might as well remove that test

[07:56] <nine> I don't understand that argument. Nothing changed with regards to the statistical analysis. It still checks whether the element that should make up ~1/3 of the result makes up less than 3/4.

[08:03] <lizmat> sorry, I mistook the test, I thought we were talking about the next block of test

[08:03] <lizmat> the my $m = {b => 1, a => 100000000000, c => -100000000000}.Mix; one

[08:04] <lizmat> :q

[08:07] <lizmat> cause if that one for some reason just starts producing "a"'s always, we will never know that we have a bug

[08:08] <lizmat> but that most definitely is not a hill I want to die on :-)

[08:32] *** Techcable left
[08:33] *** Techcable joined
[08:45] *** nine left
[08:45] *** nine joined
[08:51] *** tbrowder left
[08:51] *** tbrowder joined
[08:55] *** jjatria left
[08:55] *** jjatria joined
[09:24] <timo> we might get some benefit from checking out all the extra stuff mimalloc has to offer 

[09:25] <timo> for example, mimalloc offers "heaps", which can replace the spesh allocator, since it offers the "free all at once" feature that we made the spesh allocator for 

[09:25] *** mst left
[09:25] <timo> and we should also use the "good alloc size" and "usable size" functions for our growing arrays

[09:25] *** mst joined
[09:52] *** vrurg_ joined
[09:54] *** vrurg left
[10:41] *** greppable6 left
[10:41] *** greppable6 joined
[12:41] *** Geth left
[12:41] *** Geth joined
[13:55] <[Coke]> I am all for taking advantage of other people's code if we can.

[13:57] <timo> though i'm not sure if mimalloc can actually beat the spesh allocator, we can save a tiny bit on code size perhaps?

[13:57] <timo> since the spesh alloc uses bump-the-pointer and doesn't support freeing individual things at all, which i think the mimalloc heap api still supports

[13:57] <timo> on the other hand, we still support building moar without mimalloc

[14:23] *** notable6 left
[14:23] *** notable6 joined
[14:24] *** bloatable6 left
[14:24] *** bloatable6 joined
[17:42] *** sena_kun joined
[18:32] <timo> we can combine both; allocate the pages for the spesh allocator with a heap id, and when destroying everything we don't have to walk the linked list and call free a bunch of times, instead we just call "free the heap" once

[18:37] <lizmat> what would be the benefits?  less CPU usage, less memory ?

[18:42] <timo> could be less cpu usage, but probably not much, and it's in the spesh thread anyway, so wouldn't make programs finish much faster

[18:50] <lizmat> well, if we could get memory usage down, that would be good

[18:57] <nine> I'd be surprised if there's much to gain there though

[18:57] *** notna joined
[19:15] *** notna left
[20:09] *** bisectable6 left
[20:10] *** notable6 left
[20:10] *** greppable6 left
[20:11] *** bisectable6 joined
[20:11] *** notable6 joined
[20:11] *** unicodable6 left
[20:11] *** sourceable6 left
[20:11] *** evalable6 joined
[20:11] *** bloatable6 left
[20:12] *** greppable6 joined
[20:12] *** nativecallable6 left
[20:12] *** bisectable6__ joined
[20:12] *** linkable6 left
[20:12] *** unicodable6 joined
[20:12] *** quotable6 joined
[20:12] *** nativecallable6 joined
[20:12] *** notable6__ joined
[20:12] *** evalable6__ joined
[20:12] *** benchable6 joined
[20:13] *** tellable6 joined
[20:13] *** committable6 joined
[20:13] *** benchable6__ joined
[20:13] *** linkable6 joined
[20:13] *** notable6__ left
[20:13] *** benchable6__ left
[20:13] *** evalable6__ left
[20:13] *** bisectable6 left
[20:13] *** bisectable6__ left
[20:13] *** evalable6 left
[20:13] *** benchable6 left
[20:13] *** notable6 left
[20:13] *** quotable6 left
[20:14] *** sourceable6 joined
[20:14] *** greppable6 left
[20:14] *** tellable6 left
[20:14] *** unicodable6 left
[20:14] *** sourceable6 left
[20:14] *** coverable6 joined
[20:14] *** nativecallable6 left
[20:14] *** linkable6 left
[20:14] *** coverable6 left
[20:14] *** committable6 left
[20:15] *** greppable6 joined
[20:16] *** bloatable6 joined
[20:16] *** benchable6 joined
[20:16] *** committable6 joined
[20:17] *** evalable6 joined
[20:17] *** coverable6 joined
[20:17] *** linkable6 joined
[20:17] *** unicodable6 joined
[20:17] *** nativecallable6 joined
[20:17] *** shareable6 joined
[20:18] *** bisectable6 joined
[20:18] *** quotable6 joined
[20:18] *** sourceable6 joined
[20:18] *** tellable6 joined
[20:19] *** notable6 joined
[20:19] *** releasable6 joined
[20:34] *** sena_kun left
