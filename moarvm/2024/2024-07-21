[07:40] *** sena_kun joined
[09:49] <lizmat_> I just had a MoarVM crash on upload to zef: https://gist.github.com/lizmat/02f45bd50443fb6c43a21ba7e3dfd404

[09:49] <lizmat_> just leaving it here for possible inspiration: it did not re-occur

[09:49] *** lizmat_ left
[09:50] *** lizmat joined
[11:08] *** sena_kun left
[11:37] *** lizmat left
[12:20] *** lizmat joined
[12:21] *** lizmat_ joined
[12:24] *** lizmat left
[12:35] *** lizmat_ left
[12:35] *** lizmat joined
[14:55] *** sena_kun joined
[15:59] <timo> unfortunately, "invalid thread id in gc work pass" is impossible to debug with just a traceback, since that's some kind of memory corruption that happened earlier that's just unearthed by the next gc run

[17:37] <timo> btw for those who use "perf stat", there's a -r "repeats" argument that will run your command multiple times and give you the averages and standard deviations for all the counts (also -d once out of up to three times may be interesting sometimes??)

[17:50] <lizmat> ok, I'll remove the gist then

[18:12] *** MasterDuke joined
[18:16] <MasterDuke> caught `MoarVM panic: non-AsyncTask fetched from eventloop active work list` while running t/spec/S32-io/IO-Socket-Async.t in rr

[18:23] <MasterDuke> any suggestions for debugging?

[18:30] <MasterDuke> also caught a fail of t/spec/S12-construction/destruction.t. planned 6 tests but only ran 5, although there was no message or obvious error

[19:25] <timo> ok for the non-asynctask fetched one, you'd use "rr replay -M" to get a number for when the error happens (optional) and "rr replay -g <that number>" to reach that point

[19:25] <timo> alternatively just "rr replay" and "break MVM_panic" then "yes" if it asks to delay until a library is loaded

[19:26] <timo> then you'd go back to the spot where it popped the item off the queue

[19:26] <timo> and now i'd have to look at the code how exactly you'd add a good watchpoint for the creation of that item

[19:57] *** Nicholas left
[20:07] *** Nicholas joined
[20:58] *** sena_kun left
[21:19] <MasterDuke> yeah, i don't really know the async socket code to know what's supposed to happen

[21:26] <timo> right, i'm not sure the sockets have too much to do with it though. we'd find out what puts the broken item in there by reverse-continue-ing while watching the item

[21:26] <timo> do you have time now to go through it?

[21:26] <MasterDuke> couple minutes

[21:26] <timo> we could do a tmux share with tmate, it also has a read-only mode if you prefer

[21:27] <MasterDuke> never used tmate before, how does that work

[21:28] <timo> from your side you just "tmate" and copy what it tells you and send that to me and i'd be connected to your session

[21:32] <MasterDuke> ot, but this looks like something you'd be interested in https://pypy.org/posts/2024/07/mining-jit-traces-missing-optimizations-z3.html

[22:06] <timo> oh i like the title already

[22:06] <timo> so i've looked at the recording a little bit and here's what i think i'm seeing

[22:06] <timo> 1) we cancel the read on a socket

[22:08] <timo> in eventloop.c:56 "cancel_work", asyncsocket.c:153 "read_cancel" we call to eventloop_remove_active_work, that nulls out the "active_work" slot for work_idx 0

[22:08] <timo> that was 2)

[22:10] <timo> 3) then in asyncsocket.c:996 in "listen_cancel" we're calling uv_close with the on_listen_cancelled callback

[22:12] <timo> 4) we come back down to async_handler which uv_async_io was calling for us

[22:14] <timo> 5) in uv_run we reach uv__run_closing_handles

[22:15] <timo> 6) uv__finish_close -> uv__stream_destroy -> uv__stream_flush_write_queue -> uv__write_callbacks, then in uv__finish_close we call handle->close_cb, which finally reaches on_listen_cancelled in asyncsocket.c:989

[22:16] <timo> 7) the first thing on_listen_cancelled does is try to MVM_io_eventloop_send_cancellation_notification on MVM_io_eventloop_get_active_work, which we saw earlier was nulled out already

[22:17] <timo> i don't know terribly much about the "active work" system and what we expect to happen in what order

[22:18] <timo> i may have to go through again and make sure i'm actually looking at the same thing in each of these steps and that there's nothing unrelated in between

[22:27] <timo> not being able to "call" stuff in an rr session is all the more reason to add more stuff to the moar gdb plugins

[23:18] <timo> m: my $t = IO::Socket::Async.listen("0.0.0.0", 5000).tap -> $conn { }; my $conn = await IO::Socket::Async.connect("127.0.0.1", 5000); $conn.close; say $conn.Supply.list; $t.close;

[23:18] <camelia> rakudo-moar 6536e7568: OUTPUT: «===SORRY!=== Error while compiling <tmp>␤Unexpected block in infix position (missing statement control word before the expression?)␤at <tmp>:1␤------> ocket::Async.listen("0.0.0.0", 5000).tap⏏ -> $conn { }; my $conn = await IO::Soc…»

[23:18] <timo> m: my $t = IO::Socket::Async.listen("0.0.0.0", 5000).tap: -> $conn { }; my $conn = await IO::Socket::Async.connect("127.0.0.1", 5000); $conn.close; say $conn.Supply.list; $t.close;

[23:18] <camelia> rakudo-moar 6536e7568: OUTPUT: «Failed to resolve host name '0.0.0.0' with family 0.␤Error: Address family for hostname not supported␤  in block <unit> at <tmp> line 1␤␤»

[23:19] <timo> m: my $t = IO::Socket::Async.listen("0.0.0.0", 54345).tap: -> $conn { }; $t.close;

[23:19] <camelia> rakudo-moar 6536e7568: OUTPUT: «Failed to resolve host name '0.0.0.0' with family 0.␤Error: Address family for hostname not supported␤  in block <unit> at <tmp> line 1␤␤»

[23:19] <timo> maybe not allowed to listen on camelia?

[23:19] <timo> m: my $t = IO::Socket::Async.listen("127.0.0.1", 54345).tap: -> $conn { }; $t.close;

[23:19] <camelia> rakudo-moar 6536e7568: OUTPUT: «Failed to resolve host name '127.0.0.1' with family 0.␤Error: Address family for hostname not supported␤  in block <unit> at <tmp> line 1␤␤»

[23:19] <MasterDuke> evalable6: my $t = IO::Socket::Async.listen("127.0.0.1", 54345).tap: -> $conn { }; $t.close;

[23:19] <evalable6> MasterDuke, rakudo-moar 6536e7568: OUTPUT: «»

[23:20] <timo> evalable: my $t = IO::Socket::Async.listen("0.0.0.0", 54345).tap: -> $conn { }; $t.close;

[23:20] <evalable6> timo, rakudo-moar 6536e7568: OUTPUT: «»

[23:20] <timo> evalable: my $t = IO::Socket::Async.listen("0.0.0.0", 5000).tap: -> $conn { }; my $conn = await IO::Socket::Async.connect("127.0.0.1", 5000); $conn.close; say $conn.Supply.list; $t.close;

[23:20] <evalable6> timo, rakudo-moar 6536e7568: OUTPUT: «(exit code 1) ()␤MoarVM panic: non-AsyncTask fetched from eventloop active work list␤»

[23:20] <timo> there, this reliably reproduces the bug

[23:21] <timo> now we can pepper the code with fprintf and see if we can see something :D

