[01:47] *** ilbot3 joined
[06:12] *** brrt joined
[06:20] <nwc10> good *

[06:21] <nwc10> "brrt, tell brrt about ..."

[06:21] <brrt> good * nwc10

[06:21] <nwc10> https://morepypy.blogspot.co.at/2016/08/pypy-gets-funding-from-mozilla-for.html -- ... for Python 3.5 support

[06:21] <brrt> seen it yes

[06:21] <nwc10> oh yes, back thtere

[06:21] <brrt> i wonder how we're going to apply for a thing like that

[06:22] <nwc10> it's even on this screen :-)

[06:22] <brrt> okay, interesting little bit

[06:22] <brrt> how come infrastructure code basically has to sponsored out of charity throughout our industry

[06:23] <brrt> i mean, the number of people who'd benefit from an up-to-date pypy3 is rather large

[06:23] <nwc10> I don't have an answer to that, but a big messy example that confirms that it is a problem is OpenSSL

[06:23] <brrt> only mozilla takes up the bill

[06:24] <brrt> yes, openssl is an excellent example

[06:24] <brrt> large bigco's tend to start their own in-house variants and open-source that

[06:29] <brrt> e.g. unladen swallow, and the other python jit guys

[06:29] <brrt> whatweretheycalled

[06:30] <brrt> from dropbox

[06:30] <nwc10> pyston

[06:30] <nwc10> and Facebook with HipHop and now HHVM

[06:30] <nwc10> which has stopped blogging - http://hhvm.com/blog/

[06:31] <brrt> hmmm

[06:31] <brrt> does that mean they stopped doing things altogether?

[06:31] <brrt> maybe they're working on php7

[06:31] <brrt> support

[06:32] <nwc10> https://github.com/facebook/hhvm/commit/849695b499491b6cb7cc7654c9413ad9174f643e --  Aug 9, 2016

[06:32] <nwc10> it's dead. they stoppped working at least 12 hours ago!

[06:32] <nwc10> IIRC when they *were* still blogging they said that they would incorporate the bits of PHP 7 that didn't (er, I forget exactly) conflict with PHP 5

[06:32] <nwc10> or, I think, make some stuff switchable

[06:33] <brrt> hmm

[06:33] <nwc10> but the impression I get is that their future is with Hack

[06:33] <brrt> switches sound saner than mix-and-match

[06:34] <nwc10> I think that the answer about "everything is charity" is that most firms find it very hard to make an internal business case to pay for (more work) on something that is available to them for free (er, at no cost) and basically works well enough already

[06:34] <brrt> hmmm

[06:34] <nwc10> and it's very hard to create your own consultancy to offer such services to regular firms, and actually have a sales pitch

[06:34] <brrt> so open source is bound to remain in some low-energy state

[06:34] <nwc10> yes. ish.

[06:35] <nwc10> although it's interesting that the US Federal Government seems to be waking up to it

[06:35] <brrt> hmmm

[06:35] <brrt> interesting

[06:35] <nwc10> although you can fork gov.uk on github: https://github.com/alphagov

[06:35] <nwc10> and I think I was told that New Zealand did fork some

[06:49] * brrt hopes that won't be abused someday

[07:37] *** zakharyas joined
[08:00] *** brrt joined
[09:20] <jnthn> morning, #moarvm :)

[10:16] <jnthn> Yay, with local fixes I just had the IO::Socket::Async tests run 100 times in a row without any failure

[10:24] <nwc10> jnthn: I can run t/spec/integration/advent2013-day14.t in a loop "forever" without failing

[10:25] <nwc10> (it stops when I rebuilt rakudo in the same dir from a different window)

[10:29] <jnthn> :)

[10:30] <jnthn> What flappers/failers do you still have?

[10:31] <jnthn> The ASAN explosion in S17-lowlevel/lock.t is the only still on my "tests I know I need to look at" list

[10:31] <jnthn> *the only one

[10:35] <nwc10> of async stuff, just that

[10:35] <nwc10> t/spec/S32-str/encode.rakudo.moar is also ASAN barfage

[10:36] <nwc10> and t/spec/S16-io/eof.t makes bad assumptions about what is in /proc (on Linux)

[10:36] <nwc10> but my suggested fix in the RT ticket I opened is also daft :-)

[10:36] <jnthn> Yeah, I'm just interested in concurrency things at the moment :)

[10:37] <dalek> MoarVM: 40948f6 | jnthn++ | / (13 files):

[10:37] <dalek> MoarVM: Implement cancellation completion notification.

[10:37] <dalek> MoarVM:

[10:37] <dalek> MoarVM: So that things that want to really know when something was cancelled

[10:37] <dalek> MoarVM: are able to find out. (In the course of doing this, I noticed that we

[10:37] <dalek> MoarVM: seem to be missing a number of cancellation handlers, which will also

[10:37] <dalek> MoarVM: want looking in to.)

[10:37] <dalek> MoarVM: review: https://github.com/MoarVM/MoarVM/commit/40948f6920

[11:05] *** dalek joined
[11:07] <jnthn> Turns out the lock.t issue, after all this time, was actually an incorrect test...

[11:09] <nwc10> when the race is won/lost/$whatever, what was the failure?

[11:09] <nwc10> ASAN seems to have turned my terminal red, but failed to print out anything

[11:11] <jnthn> The test just failed due to wrong values in the array

[11:11] <nine> jnthn: does that mean that all flappers are fixed now?

[11:11] <jnthn> So far as I know.

[11:11] <nine> Yeah! jnthn++

[11:12] <nine> Need to start hacking on rakudo again. Should be even more fun now :)

[11:12] <jnthn> Hangs too :)

[11:13] <jnthn> Anothr spectest run just completed happily :)

[11:13] <nwc10> the test just failed with a SEGV: http://paste.scsys.co.uk/530610

[11:13] <nwc10> so there is something ugly in t/spec/S17-lowlevel/lock.t (as was) even if it's not testing correctly.

[11:14] <jnthn> You pulled latest roast?

[11:14] <nwc10> no, this is the one from before dalek died

[11:14] <jnthn> Ah

[11:14] <jnthn> Yes, I know there's still something wrong at the VM level

[11:14] <nwc10> but I was meaning, that version can SEGV MoarVM, and it shouldn't be possible to seg..

[11:14] <nwc10> aha OK.

[11:15] <jnthn> But the test wasn't trying to tickle that.

[11:15] <jnthn> The test was trying to do things right, but was wrong.

[11:15] <jnthn> There's already an RT to cover the underlying SEGV

[11:15] <nwc10> do we have a place for tests that try to ticket, er OK

[11:15] <jnthn> Which'll get a fix and a proper test. Maybe this afternoon :)

[11:15] <nwc10> you keep answering my questions before I finish them

[11:15] <jnthn> Lunch now... :) Will set off another spectest run whlie I nom :)

[11:28] *** JimmyZ joined
[11:29] <JimmyZ> jnthn: re 40948f69208b1f9d460883ab0089f788423db4ba, am I missing something? I never see 'cancel_notify_queue' be read

[11:35] <JimmyZ> does 'I noticed that we seem to be missing a number of cancellation handlers' explain it? :)

[11:57] <jnthn> Apparently you're missing MVM_io_eventloop_send_cancellation_notification which reads it :)

[11:57] <jnthn> Lunch spectest run also completed successfully.

[11:59] <nwc10> ASAN has not offered you an alternative lunch

[12:02] <JimmyZ> haha, i just see MVM_repr_push_o(tc, notify_queue, notify_schedulee); , not seems to process  it.

[12:03] <JimmyZ> or asynce is complicated, I can't understand it at all :(

[12:04] <JimmyZ> but I still didn't see the code that processes cancel_notify_queue

[12:04] <JimmyZ> or pop

[12:05] <JimmyZ> I only  see push :)

[12:05] <jnthn> That's 'cus Moar's job is only to push :)

[12:06] <jnthn> It's sending an async notification to be handled back in Perl 6 land

[12:06] <jnthn> So it ends up scheduled on the thread pool

[12:09] <JimmyZ> sigh, I still didn't see the way that read cancel_notify_queue ....

[12:10] <JimmyZ> even in perl 6 land

[12:14] <jnthn> It'll just be an nqp::shift in ThreadPoolScheduler.pm, iirc

[12:32] <JimmyZ> ah, find it , thanks

[12:45] <jnthn> This VMArray refactor is gonna be some work... :)

[13:05] *** unmatched} joined
[13:07] *** unmatched} joined
[14:11] <dalek> MoarVM: 2f269d8 | (Jimmy Zhuo)++ | src/io/eventloop.c:

[14:11] <dalek> MoarVM: removed unused code

[14:11] <dalek> MoarVM: review: https://github.com/MoarVM/MoarVM/commit/2f269d8a03

[14:16] <lizmat> jnthn: seems like the last updates to Moar/NQP/Rakudo borked --profile

[14:16] <lizmat> $ perl6 --profile -e 'my $a'

[14:16] <lizmat> Writing profiler output to profile-1470838613.57616.html

[14:16] <lizmat> ===SORRY!===

[14:16] <lizmat> SC not yet resolved; lookup failed

[15:47] <timotimo> yeah, i've already given a bad and a good commit for that problem

[15:48] <timotimo> hm, but not in here, it seems

[16:23] <jnthn> Time for a break, but I figure I've got the first 80% of the VMArray re-work done.

[16:27] <ilmari> now you only have the second 80% left?

[17:10] *** Zoffix joined
[17:20] <jnthn> Hopefully...don't want a third 80% :P

[17:31] <geekosaur> thought it was 90%. but then, that'd mean 10% to reach 90% + the other 90% = 100% to go >.>

[19:33] *** zakharyas joined
[20:03] *** brrt joined
[20:06] <brrt> good * #moarvm

[20:06] <timotimo> heyo brrt

[20:06] <brrt> hey timotimo

[20:06] <brrt> how's today?

[20:07] <timotimo> i feel kind of down today

[20:08] * jnthn has a totally busted MoarVM today

[20:08] <brrt> bust all the things \o/

[20:08] <timotimo> well, completely re-working the storage for VMArray would kind of do that to you :)

[20:08] <jnthn> Well, jsut VMArray :)

[20:08] <brrt> what had to change on VMArray, fwiw

[20:09] <jnthn> Its explosiveness if you mis-used it from multiple threads

[20:09] <timotimo> that's ... a question?

[20:09] <brrt> yes, a question

[20:09] <jnthn> It's not meant to be a concurrent data structure but it shouldn't be a path to SEGV the VM :)

[20:09] <brrt> i know knothing

[20:09] <timotimo> "fwiw" isn't a thing i'd end a sentence in?

[20:10] <brrt> it's not?

[20:10] <brrt> oh well

[20:10] <timotimo> it's not, no

[20:10] <timotimo> data loss is still likely when you concurrently resize the array

[20:10] <brrt> don't... do that then

[20:10] <jnthn> Yeah

[20:10] <brrt> doctor, it hurts if i do...

[20:10] <jnthn> Though I realized we can actually catch some cases of that happening and complain loudly

[20:10] <brrt> can we do that cheaply?

[20:11] <jnthn> brrt: Yeah, the pain shouldn't incldue memory corruption and a segfault though :)

[20:11] <jnthn> Relatively

[20:11] <brrt> what will it cost to the nonconcurrent case?

[20:11] <jnthn> Hard to say, but I suspect "rather little"

[20:12] <brrt> hmmm

[20:12] <jnthn> Plus we should be able to JIT array accesses pretty nicely after this too, it hink

[20:12] <brrt> good :-)

[20:12] <jnthn> *think

[20:12] <brrt> oh, that i'm interested in

[20:18] <jnthn> Anyways, I got to the point where I was too tired to hack more on the VMArray bits today, so will continue another time :)

[20:18] <brrt> oh well :-)

[20:22] <lizmat> good night, jnthn

[20:35] *** hoelzro joined
[22:04] *** stmuk_ joined
