[01:48] *** ilbot3 joined
[05:36] *** domidumont joined
[05:42] *** domidumont joined
[05:55] *** brrt joined
[06:43] *** domidumont joined
[06:49] *** brrt joined
[07:33] *** zakharyas joined
[09:09] <brrt> \o #moarvm

[09:17] <nwc10> good *, brrt

[09:22] <brrt> ohai nwc10

[10:10] *** pyrimidi_ joined
[14:29] *** hoelzro joined
[15:28] *** ggoebel joined
[16:00] *** brrt joined
[16:40] *** domidumont joined
[17:52] *** brrt joined
[18:50] <timotimo> huh

[18:51] <timotimo> i'm looking into making CStruct attribute accesses speshed

[18:51] <timotimo> it looks a bit complicated, because we're going through an extra STable per attribute

[18:52] <timotimo> but now that i'm starting to write about it here on IRC, i understand that it's all about different sized ints and such

[18:52] <timotimo> i wonder how to best spesh that ...

[18:59] <timotimo> also, i can most probably still just re-use the code from P6opaque's spesh function 1:1

[19:00] <timotimo> except i have to turn p6oget_* into get_* because we don't have a REAL_DATA indirection

[19:09] <timotimo> dang it. if i had looked at the spesh log before i would have known we don't generate getattr or bindattr for those cases, we use attrref and we don't have the infrastructure to lower those to bindattr/getattr yet

[19:12] <timotimo> yeah, and in this case i have here the attrref is immediately passed on as an argument to an invocation, so no optimization for this would happen anyway

[19:12] <timotimo> boo.

[19:19] <timotimo> though getattrref may get the same kind of optimization where it wouldn't have to go through finding the right offset each time

[19:21] <timotimo> though to be honest, that code doesn't seem to appear in the perf report at all ...

[19:21] * timotimo tries callgrind instead

[19:57] <timotimo> yeah, the CStruct object file hardly comes up at all

[20:13] *** zakharyas joined
[20:14] <timotimo> going grocery shoppign helped me figure out that getattrref can't get an optimization based on offsets

[20:26] <TimToady> you should go grocery shopping more often, or maybe less often...

[20:26] <timotimo> :)

[21:38] *** ggoebel joined
[21:38] *** stmuk joined
[21:38] *** BinGOs joined
[21:38] *** arnsholt joined
[21:38] *** krunen_ joined
[21:38] *** [Coke] joined
[21:40] *** camelia joined
[21:40] *** TimToady joined
[21:40] *** synopsebot6 joined
[21:40] *** nwc10 joined
[21:40] *** jnthn joined
[21:40] *** mst joined
[21:40] *** konobi joined
[21:40] *** _longines joined
[21:42] *** brrt joined
[21:42] *** krunen joined
[21:45] <timotimo> i'm really not very sure what's making my code so slow. 50% of run time is spent in enqueue alone

[21:45] <timotimo> (inclusive, not exclusive)

[21:46] <timotimo> (that's the sub that sorts rects into CArray queues by color)

[22:53] <dalek> MoarVM/line_based_coverage_4: 794c3c4 | (Zoffix Znet)++ | tools/parse_coverage_report.p6:

[22:53] <dalek> MoarVM/line_based_coverage_4: Include a note on the date/time when the report was generated

[22:53] <dalek> MoarVM/line_based_coverage_4: review: https://github.com/MoarVM/MoarVM/commit/794c3c49be

[22:53] <dalek> MoarVM/line_based_coverage_4: fb8f50d | jnthn++ | tools/parse_coverage_report.p6:

[22:53] <dalek> MoarVM/line_based_coverage_4: Merge pull request #411 from zoffixznet/patch-1

[22:53] <dalek> MoarVM/line_based_coverage_4:

[22:53] <dalek> MoarVM/line_based_coverage_4: Include a note on the date/time when the report was generated

[22:53] <dalek> MoarVM/line_based_coverage_4: review: https://github.com/MoarVM/MoarVM/commit/fb8f50d799

[23:44] <timotimo> our MVMInstance doesn't actually have an array (or some other collection) of threads?

[23:45] <timotimo> i suppose the profiler currently doesn't collect data from all threads because threads have to be blocked for it to be reliable?

[23:45] <timotimo> also, when threads get joined, data ought to be placed somewhere

[23:52] <timotimo> hm. it'd probably be sensible to join all threads for ending the profile ... but not all threads are cooperative about ending

[23:53] * timotimo does some more shrugging
