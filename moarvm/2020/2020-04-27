[01:21] *** sena_kun joined
[01:22] *** Altai-man_ left
[02:24] *** farcas1982regreg joined
[02:26] *** squashable6 left
[02:30] *** bloatable6 joined
[02:30] *** sourceable6 joined
[02:32] *** bloatable6 left
[02:32] *** statisfiable6 joined
[02:32] *** committable6 joined
[02:32] *** quotable6 joined
[02:32] *** greppable6 joined
[02:32] *** releasable6 joined
[02:32] *** unicodable6 joined
[02:32] *** notable6 joined
[02:32] *** evalable6 joined
[02:32] *** nativecallable6 joined
[02:32] *** tellable6 joined
[02:32] *** benchable6 joined
[02:32] *** reportable6 joined
[02:32] *** greppable6 left
[02:33] *** reportable6 left
[02:33] *** linkable6 joined
[02:33] *** reportable6 joined
[02:33] *** committable6 left
[02:33] *** bloatable6 joined
[02:33] *** squashable6 joined
[02:33] *** coverable6 joined
[02:33] *** greppable6 joined
[02:34] *** committable6 joined
[02:34] *** shareable6 joined
[03:13] *** pamplemousse left
[03:19] *** Altai-man_ joined
[03:22] *** sena_kun left
[05:13] *** sourceable6 left
[05:13] *** committable6 left
[05:13] *** squashable6 left
[05:13] *** bloatable6 left
[05:13] *** reportable6 left
[05:13] *** notable6 left
[05:13] *** coverable6 left
[05:13] *** greppable6 left
[05:13] *** statisfiable6 left
[05:13] *** quotable6 left
[05:13] *** releasable6 left
[05:13] *** unicodable6 left
[05:13] *** evalable6 left
[05:13] *** nativecallable6 left
[05:13] *** tellable6 left
[05:13] *** benchable6 left
[05:13] *** linkable6 left
[05:13] *** shareable6 left
[05:20] *** sena_kun joined
[05:22] *** Altai-man_ left
[05:23] *** shareable6 joined
[05:23] *** coverable6 joined
[05:24] *** committable6 joined
[05:24] *** notable6 joined
[05:24] *** quotable6 joined
[05:24] *** sourceable6 joined
[05:24] *** bisectable6 joined
[05:25] *** reportable6 joined
[05:25] *** releasable6 joined
[05:25] *** bloatable6 joined
[05:25] *** statisfiable6 joined
[05:25] *** benchable6 joined
[05:25] *** nativecallable6 joined
[05:26] *** squashable6 joined
[05:26] *** linkable6 joined
[06:31] *** committable6 left
[06:32] *** shareable6 left
[06:32] *** bisectable6 left
[06:32] *** shareable6 joined
[06:34] *** bisectable6 joined
[06:34] *** committable6 joined
[07:19] *** Altai-man_ joined
[07:22] *** sena_kun left
[07:24] *** MasterDuke joined
[07:52] *** zakharyas joined
[08:45] <nwc10> good *, #moarvm

[09:20] *** sena_kun joined
[09:22] *** Altai-man_ left
[11:19] *** Altai-man_ joined
[11:22] *** sena_kun left
[11:30] <MasterDuke> why would https://gist.github.com/MasterDuke17/1a307815a1ee24dcfafb99821fe36c89 cause a  sigabort  and `corrupted size vs. prev_size` when doing `raku -e ''`?

[11:31] <timotimo> what does valgrind say?

[11:31] <MasterDuke> bin is 15 (so one of the ones i've changed) and size is 272

[11:31] <jnthn> I don't think you changed it to actually allocate the page memory?

[11:31] <jnthn> Oh, you did

[11:32] <MasterDuke> gist updated

[11:32] <jnthn> oh, but

[11:32] <jnthn> your alloc limit is probably bogus?

[11:32] <timotimo> yeah the limit is far too long

[11:32] <timotimo> you're allowing fsa to write far, far outside of its pages

[11:33] <MasterDuke> oh?

[11:33] <MasterDuke> should it be unchanged?

[11:33] <timotimo> what exactly were you going for?

[11:33] <timotimo> i may have to explain what alloc_limit means

[11:34] <MasterDuke> starting with more pages for some bins (vs starting with bigger pages for some bins)

[11:34] <jnthn> Well, the allocator's design assumes that you are either allocating from the free list or, failing that, the latest added page

[11:34] <timotimo> the alloc limit is a memory address, not a total amount

[11:34] <jnthn> and yes, what timotimo said

[11:34] <timotimo> when the alloc limit address is reached, a new page gets created

[11:34] <jnthn> I don't really see how this change helps much, though, because you're making the same number of mallocs

[11:35] <timotimo> i think i had suggested making a single bigger page first rather than making loads of pages

[11:36] <jnthn> Yes, that would probably both help more *and* fit better with the overall allocator design

[11:36] <MasterDuke> yeah, but we won't have to go through alloc_slow_path a lot

[11:36] <MasterDuke> i'm just trying to measure a bunch of different options

[11:36] <timotimo> it's possible that setting up multiple empty pages will just get the allocator to skip all but the first page

[11:36] <jnthn> Or the last one, but yeah.

[11:37] <MasterDuke> jnthn: i have some measurements from implementing what timotimo++ suggested, thought i'd also try this way

[11:37] <MasterDuke> but i guess it's not (easily) possible

[11:38] <jnthn> No, it goes against the grain of the allocator design a bit too much

[11:40] <MasterDuke> afk for a bit. https://gist.github.com/MasterDuke17/0cde60931db68b66fc3cf889c817ef53 has measurements for compiling CORE.c, https://gist.github.com/MasterDuke17/bf4bddee8e928526817ef2d8e892e481 for `raku -e ''`

[11:51] *** pamplemousse joined
[11:52] <timotimo> another thing we can do is serve all the bins from a single malloc (which then would probably become an mmap)

[11:53] <timotimo> ah, this "just" runs when the first bin is requested

[11:57] <nine> Unless we're doing hundreds of mallocs here, I'd be sursprised if we could get a measurable speedup

[11:58] <timotimo> but we are

[11:58] <timotimo>      36 adding a page of size 17408 for bin = 15

[11:58] <timotimo>      37 adding a page of size 8192 for bin = 6

[11:58] <timotimo>     314 adding a page of size 7168 for bin = 5

[11:58] <timotimo> this is running raku -e '' on master

[11:59] <timotimo>      39 adding a page for bin = 1

[11:59] <timotimo>      56 adding a page for bin = 8

[11:59] <timotimo>     404 adding a page for bin = 2

[11:59] <timotimo>     406 adding a page for bin = 5

[11:59] <timotimo> and this is with MD's patch to have VMArray use FSA

[12:13] <MasterDuke> and it's not just the mallocs, it's trying and failing to go through the fast path and having to go through the slow path

[12:13] <MasterDuke> i measured ~600k fewer instructions for `raku -e ''`

[12:38] <timotimo> 0.01s system for raku -e ''; we should strive for that number to get to 0, though i wouldn't expect us to reach a point where the 0.01s are the majority of time spent

[12:49] * jnthn digs back into the dispatcher design work

[12:49] <timotimo> and a whole bunch of the sys calls have always been brk

[12:51] <timotimo> interesting, we actually shrink a couple of times, too

[12:55] <timotimo> a plot of the brk numbers shows that the reductions are pretty much inconsequential :D

[12:57] <timotimo> that's with out the MD patch,t hough

[13:20] *** sena_kun joined
[13:22] *** Altai-man_ left
[13:25] *** Altai-man_ joined
[13:28] *** sena_kun left
[13:34] <MasterDuke> do people have an opinion on just making the first page (much?) bigger vs making the first page bigger and/or making subsequent pages bigger also?

[13:34] <MasterDuke> should it just statically be for the bins that we see commonly allocated in some representative code?

[13:35] <MasterDuke> or maybe dynamically resize the page size as more (or fewer) are requested?

[13:35] <timotimo> difficult to say without a crystal ball

[13:36] <MasterDuke> hm, maybe if i first write a program that will tell if another program will ever halt...

[13:36] <timotimo> you can record a "trace" of all allocations for all size classes and then simulate what different algorithms would decide, and then find how much wasted space you would be allocating with any given algo

[13:38] <MasterDuke> so model on some sort of theoretical knapsack and see how efficiently i can put different sized items in?

[13:38] <MasterDuke> these are both pretty easy problems, should only take a day or two

[13:39] <timotimo> i'd not call it that, honestly

[13:39] <timotimo> since things of the same size always go in their corresponding bins

[13:39] <timotimo> the decisions would only be what sizes you'd put for the bins

[13:39] <MasterDuke> yep, just joking around 

[13:39] <timotimo> like, one approach would allocate exactly the maximum amount of memory used throughout the whole program at the start and then never have to do anything again

[13:47] <lizmat> I wonder if memory usage information couldn't be kept somewhere, to give hints to any allocation scheme during runtime

[13:47] <lizmat> a bit like spesh, I guess, but then over several runs

[13:48] <lizmat> just brainstorming, and no I'm not suggesting you should drink bleach

[14:02] <jnthn> While it doesn't tend to be done cross-process, there's all kinds of presizing, pretenuring, and pretransitioning optimizations that are done in this kind of area (not in MoarVM yet, but they are known/described techniques) :)

[14:03] <MasterDuke> yeah, don't think i'm quite up for that level of work just yet. just some new simple heuristics

[15:02] <lizmat> and another Rakudo Weekly News hits the Net: https://rakudoweekly.blog/2020/04/27/2020-17-spring-cleanup/

[15:05] <jnthn> Goodnes, I always figured that deferral was slow, but if you compare callsame against self.Parentclass::meth (the latter is already well optimized thanks to a spesh plugin) then it's more than 65x slower!

[15:07] <jnthn> lizmat++ # weekly

[15:08] <MasterDuke> wow. but you think the redesign will end up with it closer to the same speed?

[15:11] <jnthn> Dunno yet :)

[15:11] <jnthn> I'm quite happy with the non-resume-related bits of the design by now

[15:12] <jnthn> Still trying to find something I like for the resume case

[15:18] <MasterDuke> resuming exceptions?

[15:18] <jnthn> No

[15:18] <jnthn> Resuming dispatches

[15:19] <jnthn> The thing that's going on with callsame et al.

[15:21] *** sena_kun joined
[15:22] <MasterDuke> ah

[15:22] *** Altai-man_ left
[15:44] *** pamplemousse left
[16:11] <jnthn> Finally, I think I found something that'll work...

[16:26] *** sena_kun left
[16:26] *** dogbert11 left
[16:26] *** vesper11 left
[16:26] *** krunen left
[16:26] *** synthmeat left
[16:28] *** synthmeat joined
[16:29] *** vesper11 joined
[16:31] *** sena_kun joined
[16:31] *** dogbert11 joined
[16:32] *** krunen joined
[16:32] *** DrEeevil joined
[16:39] <timotimo> cool!

[16:45] *** ChanServ joined
[16:45] *** cherryh.freenode.net sets mode: +o ChanServ

[17:05] *** shareable6 left
[17:09] *** shareable6 joined
[17:13] *** zakharyas left
[17:18] <jnthn> Here's my design notes so far, for the curious: https://gist.github.com/jnthn/e81634dec57acdea87fcb2b92c722959

[17:19] *** Altai-man_ joined
[17:20] <jnthn> Home time &

[17:26] *** krunen left
[17:26] *** DrEeevil left
[17:26] *** sena_kun left
[17:26] *** dogbert11 left
[17:29] *** krunen joined
[17:29] *** DrEeevil joined
[17:29] *** dogbert11 joined
[17:30] <nine> jnthn: ah, I can see how that will help with the dynamically discovered methods in Inline::Perl5::ClassHOW :)

[17:31] <vrurg> Is it possible for moar to know which module is being deserialized? I'm trying to dig into https://github.com/MoarVM/MoarVM/issues/1276 

[17:32] <Altai-man_> vrurg, hi. Are you available for a bit?

[17:33] <vrurg> Altai-man_: I think, yes, if nothing happens

[17:33] <nine> vrurg: yes, look at the MVMCompUnit's file_name

[17:34] <vrurg> nine: thanks!

[17:35] <Altai-man_> vrurg, can you please say what do you think about the next release proposal (https://gist.github.com/Altai-man/433f331428595e32fc06c2e255533eb3)? If the solution suggested won't work for you, we can discuss it.

[17:36] <vrurg> Altai-man_: gimme a few minutes.

[17:37] <timotimo> i wonder if there needs to be any special consideration for slurpy arguments that can help us deal with extremely long "arguments" / argument lists when trying to slurpy a very long array

[17:39] *** ChanServ left
[17:42] *** ChanServ joined
[17:42] *** cherryh.freenode.net sets mode: +o ChanServ

[17:42] *** MasterDuke left
[17:43] <[Coke]> what do the trunc_xx opcodes do?

[17:46] <nine> Truncate e.g. int64 -> int16

[17:46] <nine> The opposite of extend

[17:49] <nine> jnthn: oh, I just realized that this will finally get us super fast multi dispatch on constants like multi foo(1) { }; multi foo(2) { } by way of nqp::dispguardobj

[18:10] *** MasterDuke joined
[18:20] <[Coke]> nine: would it be correct to describe trunc/extend as casting? 

[18:57] *** farcas1982regreg left
[19:20] *** sena_kun joined
[19:22] *** Altai-man_ left
[19:34] <[Coke]> m: use nqp; dd trunc_i8 123456789

[19:34] <camelia> rakudo-moar 561076437: OUTPUT: «5===SORRY!5=== Error while compiling <tmp>␤Undeclared routine:␤    trunc_i8 used at line 1. Did you mean 'truncate'?␤␤»

[19:34] <[Coke]> m: use nqp; dd nqp::trunc_i8 123456789

[19:34] <camelia> rakudo-moar 561076437: OUTPUT: «===SORRY!===␤No registered operation handler for 'trunc_i8'␤»

[19:35] <Geth> ¦ MoarVM: MasterDuke17++ created pull request #1277: Make the first page for some FSA bins bigger

[19:35] <Geth> ¦ MoarVM: review: https://github.com/MoarVM/MoarVM/pull/1277

[19:35] <MasterDuke> m: use nqp; dd nqp::trunc_i8(123456789)

[19:35] <camelia> rakudo-moar 561076437: OUTPUT: «===SORRY!===␤No registered operation handler for 'trunc_i8'␤»

[19:36] <MasterDuke> the above PR is to give something concrete to discuss, i'm open to changes

[19:37] <[Coke]> Wonder if the test is misreading the nqp source and those aren't really opcodes.

[19:41] <[Coke]> (or if they are QAST only)

[19:42] <MasterDuke> https://github.com/MoarVM/MoarVM/blob/master/src/core/interp.c#L255

[19:43] <nine> Could be they're opcodes but not actually mapped to NQP ops

[19:44] <nine> They are used by the QAST compiler's coercions: https://github.com/Raku/nqp/blob/master/src/vm/moar/QAST/QASTCompilerMAST.nqp#L529

[19:56] *** Kaiepi left
[19:58] *** Kaiepi joined
[20:10] <[Coke]> I'll see if I can get them working in a QAST example.

[21:08] <timotimo> you probably need to set a :returns(int16) in a qast node or something like that

[21:12] *** patrickb joined
[21:19] *** Altai-man_ joined
[21:22] *** sena_kun left
[22:04] *** patrickb left
[22:59] <Geth> ¦ MoarVM: MasterDuke17++ created pull request #1278: Fix some compiler warnings

[22:59] <Geth> ¦ MoarVM: review: https://github.com/MoarVM/MoarVM/pull/1278

[23:12] *** Altai-man_ left
