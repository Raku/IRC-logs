[00:02] *** evalable6 left
[00:02] *** committable6 left
[00:02] *** linkable6 left
[00:03] *** evalable6 joined
[00:03] *** committable6 joined
[00:04] *** linkable6 joined
[00:30] *** frost-lab left
[01:07] *** dogbert11 joined
[01:10] *** dogbert17 left
[02:10] *** quotable6 left
[02:10] *** notable6 left
[02:10] *** squashable6 left
[02:10] *** bisectable6 left
[02:10] *** committable6 left
[02:10] *** bloatable6 left
[02:10] *** evalable6 left
[02:10] *** linkable6 left
[02:10] *** nativecallable6 left
[02:10] *** releasable6 left
[02:10] *** benchable6 left
[02:10] *** greppable6 left
[02:10] *** coverable6 left
[02:10] *** sourceable6 left
[02:10] *** shareable6 left
[02:10] *** unicodable6 left
[02:10] *** tellable6 left
[02:10] *** statisfiable6 left
[02:11] *** sourceable6 joined
[02:11] *** greppable6 joined
[02:11] *** nativecallable6 joined
[02:11] *** bloatable6 joined
[02:12] *** tellable6 joined
[02:12] *** notable6 joined
[02:12] *** releasable6 joined
[02:12] *** squashable6 joined
[02:12] *** coverable6 joined
[02:12] *** linkable6 joined
[02:12] *** committable6 joined
[02:13] *** shareable6 joined
[02:13] *** benchable6 joined
[02:13] *** quotable6 joined
[02:13] *** statisfiable6 joined
[02:13] *** bisectable6 joined
[02:14] *** evalable6 joined
[02:14] *** unicodable6 joined
[02:41] *** lucasb left
[03:13] *** leont left
[05:36] *** MasterDuke left
[07:40] *** domidumont joined
[07:43] *** domidumont left
[07:49] *** domidumont joined
[07:56] *** domidumont left
[07:58] *** domidumont joined
[08:03] <timotimo> finally back at my regular desktop system ...

[08:05] <timotimo> ... wow, one of the drives on the raid isn't showing up, eh? that's great

[08:09] *** sena_kun joined
[08:14] *** dumarchie joined
[08:14] <dumarchie> Isn't https://github.com/MoarVM/MoarVM/blob/82a34e25a8f66847a1d895214488264618f4dd82/src/6model/reprs/MVMMultiCache.c#L160-L164 a classical example of a data race?

[08:16] *** sivoais left
[08:16] *** camelia left
[08:20] *** Geth left
[08:20] *** Geth joined
[08:21] *** domidumont left
[08:22] *** sivoais joined
[08:22] *** camelia joined
[08:28] *** domidumont joined
[08:36] <nine> timotimo: can you re-add it?

[08:37] <nine> I've had devices fall out of the RAID twice in the past couple of weeks. But a simple add and they're up again

[08:52] *** zakharyas joined
[09:08] *** Altai-man joined
[09:10] *** sena_kun left
[09:22] *** MasterDuke joined
[09:53] <MasterDuke> timotimo: ping re https://github.com/MoarVM/MoarVM/pull/1399

[10:02] <Geth> Â¦ MoarVM/modern-raku-script-extensions: a7efa956f9 | (Elizabeth Mattijsen)++ | 6 files

[10:02] <Geth> Â¦ MoarVM/modern-raku-script-extensions: Fix some .p6 -> .raku changes that were missed

[10:02] <Geth> Â¦ MoarVM/modern-raku-script-extensions: 

[10:02] <Geth> Â¦ MoarVM/modern-raku-script-extensions: Nine++ for the spot

[10:02] <Geth> Â¦ MoarVM/modern-raku-script-extensions: review: https://github.com/MoarVM/MoarVM/commit/a7efa956f9

[10:05] <dumarchie> MasterDuke, can you have a look at https://github.com/MoarVM/MoarVM/issues/1237#issuecomment-744273918 ?

[10:08] <MasterDuke> i'm not sure what's the race there?

[10:09] <MasterDuke> but i suspect you'll really need nine, jnthn, timotimo, or nwc10 to take a look

[10:09] <dumarchie> Two threads see the cache is not properly initialized. Both assign a new object. Then later one of them finds out the object is different than expected.

[10:11] <MasterDuke> hm

[10:15] <dumarchie> I'm not a C programmer, but maybe even the pointer itself could become corrupted?

[10:17] <MasterDuke> there is a mutex that's used for some multicache operations. e.g., https://github.com/MoarVM/MoarVM/blob/82a34e25a8f66847a1d895214488264618f4dd82/src/6model/reprs/MVMMultiCache.c#L209

[10:18] <MasterDuke> you could try wrapping that init in a lock/unlock

[10:22] <dumarchie> I guess it's safe to use the mutex_multi_cache_add for initialization as well. I'll give it a try. It's probably more interesting than writing a hello_world.c :)

[10:23] <dumarchie> Otoh, do we want a lock/unlock for every MVM_multi_cache_add ?

[10:23] <jnthn> iirc, the idea was that threads may race to install the multi cache, and whoever loses just ends up with that being GC'd later

[10:23] <tellable6> hey jnthn, you have a message: https://gist.github.com/9b64d0622fc34b57dbfae790e6562af7

[10:26] <dumarchie> jnthn, that makes sense to me. But does the cur_node check in line 258 make sense from that perspective?

[10:28] *** frost-lab joined
[10:29] *** Kaeipi joined
[10:29] <dumarchie> line 257 that is

[10:29] *** Kaiepi left
[10:34] <MasterDuke> dumarchie: do you still have the problem with a current rakudo? according to one of your recent comments, you still have a rakudo from 2020.10, even though your moarvm is from 2020.11

[10:36] <dumarchie> All my recent debugging was with Rakudo v2020.10-275-gc63f078a2

[10:36] <MasterDuke> there were some changes to cas between now and then, but i think they were just optimizations and shouldn't have changed how it works

[10:39] <dumarchie> I don't see them in https://github.com/dumarchie/rakudo/compare/master...rakudo:master

[10:40] <MasterDuke> https://github.com/rakudo/rakudo/commit/1ccfb095d19e47fff8a864836312b603dac6f8e3

[10:41] <MasterDuke> oh ha. https://github.com/rakudo/rakudo/commit/96ab6eba55243670f5cf4b9be3ae0bba491a4663 was by you

[10:42] <dumarchie> :)  I checked and they are part of the Rakudo I built

[10:43] <MasterDuke> but it says v2020.10-275-gc63f078a2 ? something's off then

[10:47] <MasterDuke> did you pull tags?

[10:48] <dumarchie> iirc I just did a `git pull upstream master`

[10:49] <dumarchie> That should pull tags, right?

[10:49] <MasterDuke> nope. need to do a round of pull/push with `--tags`

[10:50] <dumarchie> Oh... maybe a `--rebase` does it as well? That's what I normally use.

[10:50] <MasterDuke> but this does make it interesting that i can't repro. i thought you had an old rakudo, but if not than i wonder why i don't get the panic

[10:51] <MasterDuke> what's your OS and hardware?

[10:52] <dumarchie> Windows 10. Let me figure out how to get CPU info.

[10:53] <dumarchie> Intel Core i5-6200U

[10:54] <dumarchie> I guess most devs have an AMD?

[10:56] <MasterDuke> well, i know nine and i do

[10:56] <dumarchie> A colleague of mine also failed to repro on Linux on AMD.

[10:56] <MasterDuke> yeah, i'm on linux also

[10:58] <dumarchie> But note that I can't consistently repro. Only once in a while I have the "Switching to Thread" followed by a panic.

[10:59] <MasterDuke> what compiler are you using?

[10:59] <MasterDuke> i usually build with gcc, but i'll try switching to clang

[11:00] <dumarchie> For my latest build I used gcc provided by MinGW

[11:02] *** tib left
[11:06] <MasterDuke> been running in a loop for a couple minutes, still no panic

[11:12] <dumarchie> Maybe you can speed it up with `benchmark/stack.raku 100` to limit the number of values pushed and popped per run.

[11:23] <MasterDuke> couple minutes of that each with gcc and clang, no panic

[11:24] <dumarchie> You also didn't see the "Switching to Thread"?

[11:28] <MasterDuke> nope

[11:50] <dumarchie> Can you limit the number of CPU cores MoarVM uses?

[11:55] <MasterDuke> i think so

[12:02] <dumarchie> Maybe it helps if you limit them to 4 or 2.

[12:02] *** linkable6 left
[12:02] *** evalable6 left
[12:02] <MasterDuke> been running various configurations with 2 cores, no dice so far

[12:03] *** linkable6 joined
[12:04] <MasterDuke> how long does it usually take for you to get an error?

[12:05] *** evalable6 joined
[12:10] <dumarchie> It varies between 1 and about 20 runs.

[12:11] <MasterDuke> i've probably done over 1k across the different configurations. seems like it might be a windows thing

[12:12] <MasterDuke> i think [Coke] runs on windows, maybe he can repro to confirm

[12:13] <MasterDuke> afk for a bit

[13:02] *** lizmat_ joined
[13:04] *** lizmat left
[13:09] *** sena_kun joined
[13:10] *** Altai-man left
[13:12] *** frost-lab left
[13:20] *** zakharyas left
[13:28] *** leont joined
[14:00] *** domidumont1 joined
[14:02] *** dumarchie left
[14:02] *** domidumont left
[14:02] <nine> dumarchie: ignore the "Switching to Thread". That's just gdb saying that it switches to the thread that called abort() as that's most likely what you as a user want to look at

[14:02] <tellable6> nine, I'll pass your message to dumarchie

[14:06] *** lucasb joined
[14:08] <nine> Naively putting that allocation insided the locked area can cause a deadlock: allocation may trigger garbage collection. If another thread is waiting for that mutex, it won't be able to enter the GC, so GCing threads are waiting for that thread to join and that thread is waiting for the multi cache mutex to become available

[14:11] *** lizmat_ is now known as lizmat

[14:29] *** zakharyas joined
[14:36] *** dumarchie joined
[14:37] <dumarchie> nine, good point I guess

[14:37] <tellable6> 2020-12-14T14:02:39Z #moarvm <nine> dumarchie: ignore the "Switching to Thread". That's just gdb saying that it switches to the thread that called abort() as that's most likely what you as a user want to look at

[14:44] *** zakharyas1 joined
[14:47] *** zakharyas left
[14:58] <dumarchie> Maybe it would be better to obtain and manipulate a private pointer to the `cache` (i.e. the `cache_obj->body`) instead of manipulating and checking the `cache_obj` itself?

[15:00] <MasterDuke> dumarchie: is valgrind available for windows?

[15:02] <nine> dumarchie: I don't see what that would change

[15:03] <nine> also that's exactly what happens in line 165

[15:09] <nine> dumarchie: did you notice that the message says cur_node != 0, re-check == 0000000000000000? In my book, 0000000000000000 is decidedly 0. Previous messages in the same GH issue said <nil> which should be quite 0, too

[15:09] <nine> dumarchie: what does gdb say that cur_node actually is?

[15:11] <dumarchie> How do I ask gdb?

[15:12] <nine> p cur_node

[15:12] <dumarchie> Let met try to trigger another panic.

[15:12] <nine> in that call frame. From MVM_panic you'll have to do up

[15:13] <dumarchie> You mean `(gdb) up` and then `(gdb) p cur_node` ?

[15:13] <nine> yes

[15:18] <MasterDuke> might need to recompile with `--optimize=0`

[15:20] <dumarchie> Just MoarVM, I suppose. How do I do that efficiently?

[15:21] <nine> Just run MoarVM's Configure.pl manually and do a make install. No need to touch nqp or rakudo

[15:28] <dumarchie> Running `nqp\MoarVM>perl Configure.pl --optimize=0 --debug=3`

[15:34] *** MasterDuke left
[15:34] *** Kaeipi left
[15:35] *** Kaeipi joined
[15:35] *** Kaeipi left
[15:36] *** Kaeipi joined
[15:39] <[Coke]> 

[15:41] *** MasterDuke joined
[15:42] <dumarchie> OK, I hit the breakpoint and did `bt`. Should I also do `f 1`?

[15:44] <dumarchie> With just `up` and `p cur_node` I get `$1 = <optimized out>`

[15:44] <MasterDuke> that should be the same as `up`, so yeah

[15:45] <MasterDuke> you did `make install` after the configure?

[15:47] <dumarchie> Yes, both in `nqp\MoarVM` and in the main `rakudo` directory. But I see that *install\bin\moar.dll* was not changed...

[15:47] <nine> dumarchie: did you still have the debugger running when doing make install?

[15:48] <nine> That could prevent make install from replacing the file

[15:48] <dumarchie> No, I'm using just one command prompt.

[15:53] <Geth> Â¦ MoarVM/try_fix_multi_cache_add: f19e0e9f67 | (Stefan Seifert)++ | 2 files

[15:53] <Geth> Â¦ MoarVM/try_fix_multi_cache_add: Try to fix "Corrupt multi dispatch cache: cur_node != 0, re-check == 0"

[15:53] <Geth> Â¦ MoarVM/try_fix_multi_cache_add: 

[15:53] <Geth> Â¦ MoarVM/try_fix_multi_cache_add: MVM_multi_cache_find did a couple more checks when looking for a multi

[15:53] <Geth> Â¦ MoarVM/try_fix_multi_cache_add: candidate than MVM_multi_cache_add, i.e. it was more picky. This could lead to

[15:53] <Geth> Â¦ MoarVM/try_fix_multi_cache_add: the check for a pre-existing candidate to not find one and the code for finding

[15:53] <Geth> Â¦ MoarVM/try_fix_multi_cache_add: the right tree node to extend getting surprised by finding a matching candidate

[15:53] <Geth> Â¦ MoarVM/try_fix_multi_cache_add: after all.

[15:53] <Geth> Â¦ MoarVM/try_fix_multi_cache_add: review: https://github.com/MoarVM/MoarVM/commit/f19e0e9f67

[15:53] <nine> dumarchie: can you please try this patch?

[15:56] <dumarchie> It looks like the first `gmake install` from *nqp\MoarVM>* installed in *nqp\MoarVM\install\bin\* and the second `gmake install` thought that was OK...

[15:56] <nine> dumarchie: oh, you have to specify the --prefix to Configure.pl

[15:57] <nine> Same prefix as for rakudo

[15:57] <dumarchie> The other files in the main *install\bin\* _were_ updated ...

[15:57] <nine> by the make install in rakudo

[15:59] <dumarchie> I guess so. To apply your patch I can just do a `git pull` in *nqp\MoarVM\* ?

[15:59] <nine> and `git checkout try_fix_multi_cache_add`

[16:00] <nine> then make install. After you ran Configure.pl again with a correct --prefix

[16:02] <dumarchie> That would be `--prefix="c:\raku\rakudo\install\" in my case, I guess.

[16:02] <nine> looks good

[16:05] <dumarchie> Do you prefer the first or the last compile error?

[16:06] <dumarchie> First: src\gen\config.c:238: error: unterminated argument list invoking macro "MVMROOT"

[16:06] <nine> There shouldn't be any :D

[16:06] <nine> what the? I was nowhere near that code

[16:06] <dumarchie> src\gen\config.c:26:5: error: 'MVMROOT' undeclared (first use in this function); did you mean 'MVMIOOps'?

[16:07] <dumarchie> src\gen\config.c:26:12: error: expected ';' at end of input

[16:07] <dumarchie>      MVMROOT(tc, config, {

[16:08] <dumarchie>             ^

[16:08] <nine> There's something seriously wrong.

[16:08] <nine> Can you please try a `git clean -xfd` followed by `perl Configure.-l --prefix="c:\raku\rakudo\install"` and `make install`?

[16:09] <dumarchie> OK

[16:10] <nine> If that alone doesn't help, then maybe deleting the c:\raku\rakudo\install completely before that will

[16:10] <dumarchie> I did the `git clean -xfd` and `perl Configure.pl --optimize=0 --debug=3 --prefix="c:\raku\rakudo\install\".

[16:11] <nine> No need for --optimize=0 --debug=3

[16:11] <dumarchie> Just to be sure: does it matter where I do the `gmake install`?

[16:11] <nine> in the MoarVM directory

[16:14] <dumarchie> Same error. I will delete both c:\raku\rakudo\install and c:\raku\rakudo\nqp\MoarVM\install and try again.

[16:17] <dumarchie> B.t.w. `git status` tells me "modified:   3rdparty/libuv (untracked content)". Does that matter?

[16:20] <dumarchie> config.o is now compiled fine, so I guess not

[16:21] <nine> step by step :)

[16:25] <dumarchie> Okay, compiled the moar binaries. I guess I now have to do another `gmake install` in c:\raku\rakudo\

[16:26] <nine> no

[16:26] <nine> or yes, since you deleted install previously

[16:26] <nine> in that case you also need a make install in nqp

[16:27] <dumarchie> Indeed, thanks :)

[16:28] <dumarchie> +++ Rakudo installed succesfully!

[16:30] <dumarchie> 5th run without debugger:

[16:30] <dumarchie> MoarVM panic: Corrupt multi dispatch cache: cur_node != 0, re-check == 000000000892F3C0

[16:30] <dumarchie> Would you like me to try again with gdb?

[16:32] <nine> Well....this is interesting. This is the first time that re-check actually isn't 0

[16:32] <nine> please, gdb would be nice

[16:32] <nine> Actually it would have been a good idea to just extend the error message to include cur_node's actual value

[16:32] <nine> Then one wouldn't need the debugger for that

[16:38] <dumarchie> Shall I wait for you commit?

[16:40] <Geth> Â¦ MoarVM/try_fix_multi_cache_add: a344c580ed | (Stefan Seifert)++ | src/6model/reprs/MVMMultiCache.c

[16:40] <Geth> Â¦ MoarVM/try_fix_multi_cache_add: Add some more diagnostics to "Corrupt multi dispatch cache" error

[16:40] <Geth> Â¦ MoarVM/try_fix_multi_cache_add: review: https://github.com/MoarVM/MoarVM/commit/a344c580ed

[16:40] <nine> there ^^^

[16:42] <MasterDuke> -cur_node? why the minus?

[16:46] <nine> Because leaf nodes (containing the index into the results array) in the multi dispatch cache tree are marked by being negative

[16:46] <MasterDuke> ah

[16:54] <dumarchie> Hmm, now it takes more time to reproduce the panic.

[16:56] <MasterDuke> but it does still panic?

[16:56] <dumarchie> Not yet

[16:56] *** dumarchie left
[16:57] *** dumarchie joined
[16:58] <dumarchie> Finally: MoarVM panic: Corrupt multi dispatch cache: cur_node != 0: -1 (000000000891F3C0), re-check == 000000000891F3C0

[16:59] <nine> Makes me wonder: what if there are actually 2 bugs here and I did fix the one?

[17:03] <dumarchie> In line 210, the `MVM_multi_cache_find` could operate on a `cache_obj` that is modified by a thread that has not reached line 209 yet, I think

[17:04] <nine> How would that be modified?

[17:04] <dumarchie> Line 162.

[17:04] <nine> cache_obj is a local variable

[17:05] <nine> line 162 allocates a new object that other threads do not have any reference to

[17:06] <dumarchie> I'm not going to argue with you :)  

[17:06] <dumarchie> I don't know C. I just saw the parameter declared as `MVMObject *cache_obj`

[17:08] *** Altai-man joined
[17:08] <dumarchie> I guess that dereferences the pointer?

[17:10] *** sena_kun left
[17:11] <nine> that declares a pointer. cache_obj is just a pointer. dereferencing is via ->

[17:13] <nine> I only see 2 ways how a second check could find a candidate where the first one didn't: either the cache changed, or the thing we're looking for changed between the first and the second find

[17:14] <nine> The cache is protected by the mutex. Unless there's some other code that modifies the cache without taking the mutex, I don't see how the cache could change

[17:14] <MasterDuke> there is a MVM_MULTICACHE_DEBUG that enables a `dump_cache`. dump at every find?

[17:15] <dumarchie> Afk to buy food.

[17:49] *** vrurg left
[18:04] *** domidumont1 left
[18:08] <nine> Following the hypthesis that the thing we're looking for changes mid-way: we're looking for an MVMCallCapture. That's a set of arguments. The arguments to cas are: (Mu $target is rw, Mu \expected, Mu \value)

[18:12] *** vrurg joined
[18:13] <nine> What if $target gets changed by a different thread between us checking for that cas candidate for the first time and the second time?

[18:17] *** vrurg left
[18:38] <timotimo> yooooo

[18:39] <timotimo> i was without a usable desktop machine for a couple of days :|

[18:40] <nine> ouch

[18:43] <timotimo> upgraded my media file storage from a 12tb raid0 to a 14tb raid1

[18:43] <timotimo> three disks to two disks

[18:57] <nine> Can't confirm my hypothesis of the changing capture

[19:09] <lizmat> and yet another Rakudo Weekly News hits the Net: https://rakudoweekly.blog/2020/12/14/2020-50-new-on-wikipedia/

[19:30] *** zakharyas1 left
[19:31] <dumarchie> nine: in my code the type of value contained in `$target` changes from `:U` to `:D`

[19:33] *** MasterDuke left
[19:46] <dumarchie> Furthermore the panic appears to be triggered by `prefix:<â>`, but maybe that first sees a `:U` and then a `:D` because another thread already performed a `cas`.

[20:00] <nine> yes, that's what I figured, but I still cannot reproduce it even with creating those exact circumstances

[20:02] *** dumarchie left
[20:17] *** vrurg joined
[20:18] *** patrickb joined
[20:21] *** vrurg left
[21:09] *** sena_kun joined
[21:10] *** Altai-man left
[21:18] *** dumarchie joined
[21:20] <dumarchie> nine: I enabled MVM_MULTICACHE_DEBUG and created a gist with the output of the last four additions before a panic: https://gist.github.com/dumarchie/1dec8e931fb3f0b846562c71276ae3f4

[21:20] <dumarchie> The first and last tree dump appear to be the same cache. Maybe the difference between them tells you something.

[21:35] *** patrickb left
[21:44] *** zakharyas joined
[21:53] *** zakharyas left
[21:56] *** vrurg joined
[22:01] *** vrurg left
[22:04] *** vrurg joined
[22:09] *** vrurg left
[22:12] *** sena_kun left
[22:19] *** MasterDuke joined
[23:28] *** vrurg joined
[23:33] *** vrurg left
[23:56] *** vrurg joined
