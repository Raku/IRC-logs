[00:06] <Geth> ¦ MoarVM: MasterDuke17++ created pull request #1264: Free malloced data before leaving the function

[00:06] <Geth> ¦ MoarVM: review: https://github.com/MoarVM/MoarVM/pull/1264

[00:47] <Geth> ¦ MoarVM: patrickbkr++ created pull request #1265: Always handle proc exec arguments verbatim on Windows

[00:47] <Geth> ¦ MoarVM: review: https://github.com/MoarVM/MoarVM/pull/1265

[00:53] *** patrickb61 joined
[00:55] *** patrickb left
[01:05] *** patrickb61 left
[06:04] *** reportable6 joined
[08:28] *** AlexDaniel left
[08:55] <Geth> ¦ MoarVM: 24f663cf51 | (Daniel Green)++ | src/6model/serialization.c

[08:55] <Geth> ¦ MoarVM: Free malloced data before leaving the function

[08:55] <Geth> ¦ MoarVM: review: https://github.com/MoarVM/MoarVM/commit/24f663cf51

[08:55] <Geth> ¦ MoarVM: 4f08d803f4 | niner++ (committed using GitHub Web editor) | src/6model/serialization.c

[08:55] <Geth> ¦ MoarVM: Merge pull request #1264 from MasterDuke17/cleanup_serialization_output

[08:55] <Geth> ¦ MoarVM: 

[08:55] <Geth> ¦ MoarVM: Free malloced data before leaving the function

[08:55] <Geth> ¦ MoarVM: review: https://github.com/MoarVM/MoarVM/commit/4f08d803f4

[10:04] *** sena_kun joined
[10:51] *** Altai-man_ joined
[10:53] *** sena_kun left
[11:42] *** lizmat joined
[11:58] *** lizmat left
[12:07] *** lizmat joined
[12:18] *** zakharyas joined
[12:29] <lizmat> would now be a good time to do a bump ?

[12:36] <Altai-man_> lizmat, good enough

[12:36] <Altai-man_> some nice fixes are there, at the very least

[12:42] <lizmat> ok, will do

[12:52] *** sena_kun joined
[12:53] *** Altai-man_ left
[12:57] *** zakharyas left
[13:19] <MasterDuke> jnthn, nine, timotimo, brrt: sena_kun thinks https://github.com/MoarVM/MoarVM/commit/162b68b6b676318901c7baf6fd4c4955710b6f7d is responsible for the appveyor failures. i don't have a windows machine to test on (or the confidence i could diagnose even if so). anybody mind giving that commit a second look?

[13:20] <sena_kun> I'll now explain my reasoning for this suspect...

[13:20] <nine> MasterDuke: I've given it a close look and the expr JITed versions, too and couldn't find anything wrong

[13:20] <MasterDuke> hm

[13:21] <MasterDuke> and thanks

[13:22] <sena_kun> the bump which bring in the issue was from 44-g04005cf43 to 47-g3c3ad0678, so on 040005cf43 it worked, but on 3c3ad0678 already not. In between there are three commits: 2252a95df953d7182e2a380dffdfd50e55380bab which is a revert of some fix, 162b68b6b676318901c7baf6fd4c4955710b6f7d which does something with asm and the third is its merge commit.

[13:22] <linkable6> (2020-03-16) https://github.com/MoarVM/MoarVM/commit/3c3ad0678a Merge pull request #1259 from MasterDuke17/jit_nextdispatcherfor

[13:22] <linkable6> (2020-03-15) https://github.com/MoarVM/MoarVM/commit/162b68b6b6 JIT nextdispatcherfor

[13:23] <jnthn> MasterDuke: Probably I can try a Windows build and see if I can reproduce it a bit later today

[13:23] <sena_kun> what I also found suspicious is that this commit added not only case MVM_OP_nextdispatcherfor but also MVM_OP_takenextdispatcher, so maybe it's MVM_OP_takenextdispatcher introduced before who is guilty.

[13:23] <jnthn> That's also possible

[13:24] <jnthn> If it only breaks on Windows I kinda suspect a calling conventions violation (they're different)

[13:24] <sena_kun> the "easy" solution is to just revert them and see if that helps, but I am reluctant to do this because we'll need to do not just a revert, but revert, bump, bump, and if that's pointless then it's for nothing.

[13:25] <sena_kun> And revert it back and bump and bump, then maybe just trying to look for a real reason is cheaper instead of doing a hurry workaround (as we are not in a real critical hurry).

[13:26] <jnthn> Well.

[13:26] <jnthn> Updating submodules .................................... List form of pipe open not implemented 

[13:26] <jnthn> MoarVM won't even configure on my machine.

[13:27] <jnthn> (On Windows, that is.)

[13:28] <lizmat> yikes

[13:29] <jnthn> So much more "let's leave a build to happen while I lunch"...

[13:29] <jnthn> Well, bbl

[13:29] <nine> BUT

[13:30] <nine> There is an error in the lego JIT implementation for MVM_OP_takenextdispatcher which gets unlocked by https://github.com/MoarVM/MoarVM/commit/162b68b6b676318901c7baf6fd4c4955710b6f7d#diff-df7f32b20e74e35d9db8f2f3a89c6299R1816

[13:30] <dogbert17> FWIW, the DU error uncovered by nwc10++ yesterday is caused by https://github.com/MoarVM/MoarVM/commit/6b31c93c8768cb758407a380e616af870f5c3ae2

[13:31] <dogbert17> perhaps that is what nine is saying above :)

[14:43] <Geth> ¦ MoarVM: 3438ad2a40 | (Stefan Seifert)++ | src/jit/x64/emit.dasc

[14:43] <Geth> ¦ MoarVM: Fix copy pasta in lego JIT implementation of takenextdispatcher

[14:43] <Geth> ¦ MoarVM: review: https://github.com/MoarVM/MoarVM/commit/3438ad2a40

[14:47] <lizmat> nine++

[14:47] <sena_kun> lizmat, bump time?

[14:47] <lizmat> fine by me  :-)

[14:47] <lizmat> nine?

[14:47] <nine> sure

[14:48] <nine> Lets hope this fixes the Windows issue

[14:48] <lizmat> ok, coming up

[14:48] <lizmat> nine: feels like the WIndows issue is a problem in the build process

[14:48] <sena_kun> lizmat, no

[14:48] <lizmat> no?  ok, in that case... whee!

[14:48] <sena_kun> lizmat, jnthn has said it's likely a segfault looking at different logs of failures

[14:51] *** Altai-man_ joined
[14:53] *** sena_kun left
[15:03] <Altai-man_> lizmat, do you plan to bump nqp after travis green or?

[15:03] <lizmat> sorry, got distracted while spectesting  :-)

[15:04] <lizmat> bumped now

[15:04] <Altai-man_> lizmat++

[15:04] <Altai-man_> when probably we'll be able to have a release tonight, or maybe closer to tomorrow as blin is required

[15:05] <lizmat> looking forward to it!

[15:05] <Altai-man_> s/when/then/

[16:00] <dogbert17> unsurprisingly the DU error is still present

[16:52] *** sena_kun joined
[16:53] *** Altai-man_ left
[18:02] *** zakharyas joined
[18:51] *** Altai-man_ joined
[18:54] *** sena_kun left
[20:28] *** lucasb joined
[20:52] *** sena_kun joined
[20:54] *** Altai-man_ left
[22:24] *** patrickb joined
[22:51] *** Altai-man_ joined
[22:54] *** sena_kun left
[23:17] *** AlexDaniel joined
[23:17] *** AlexDaniel left
[23:17] *** AlexDaniel joined
[23:21] *** zakharyas left
[23:22] *** Kaiepi left
[23:23] *** Kaiepi joined
[23:27] <MasterDuke> i'm curious, what's the optimization possible here? https://github.com/MoarVM/MoarVM/blob/master/src/6model/reprs/VMArray.c#L922-L923

[23:28] *** Altai-man_ left
[23:29] <timotimo> can use memcpy if the sizes and signednesses match, can use a tight loop that likely gets compiled to vectorized instructions if it's two MVMArray objects

[23:30] <timotimo> since if the other object is an MVMArray too, you can access the other object's body->elems

[23:33] <MasterDuke> hm, still have to copy either way?

[23:34] <timotimo> i don't think you can get around it, yeah

[23:37] <MasterDuke> a large chunk of the allocations that happen during a rakudo compile are from asplice

[23:38] <timotimo> do you have stack traces to go with it, or at least the line numbers / filenames or whatever to go with them?

[23:38] <MasterDuke> 592750    at gen/moar/stage2/QAST.nqp:7015  (/home/dan/Source/perl6/install/share/nqp/lib/QAST.moarvm:add)

[23:38] <MasterDuke> 593411    at gen/moar/stage2/QAST.nqp:7271  (/home/dan/Source/perl6/install/share/nqp/lib/QAST.moarvm:write_string_heap)

[23:39] <MasterDuke> https://github.com/Raku/nqp/blob/master/src/vm/moar/QAST/QASTCompilerMAST.nqp#L2294-L2325

[23:40] <MasterDuke> https://github.com/Raku/nqp/blob/master/src/vm/moar/QAST/QASTCompilerMAST.nqp#L2550-L2553

[23:44] <MasterDuke> i can send you the heaptrack file if you'd like

[23:45] <MasterDuke> that has the moarvm stack traces

[23:46] <timotimo> moarvm stacktraces ar not quite as interesting i think?

[23:47] <timotimo> if you're only recording asplice calls

[23:47] <timotimo> huh that add function is interesting; checking whether a string is utf8 by going char by char

[23:48] <MasterDuke> i only recorded asplice calls after seeing it at the top of the heaptrack data

[23:48] <timotimo> what are those numbers for then? for the add and write_string_heap functions?

[23:48] <MasterDuke> oh, number of times called

[23:49] <MasterDuke> i added `fprintf(stderr, "%s\n", MVM_exception_backtrace_line(tc, tc->cur_frame, 0, *(tc->interp_cur_op)));` right before the set_size_internal() in asplice()

[23:50] <timotimo> i assume this comes from write_* method calls?

[23:50] <timotimo> i mean, those already pre-size the buffer, so the set_size_internal ought to not do anything in many of these cases?

[23:51] <MasterDuke> well, heaptrack says 4.3gb allocated by set_size_internal in asplice when building CORE.c

[23:52] <timotimo> but we're not outputting only the calls that actually do an allocation in set_size_internal

[23:52] <timotimo> oh, or is that inside a conditional?

[23:53] <timotimo> that is line 990 of VMArray.c?

[23:53] <MasterDuke> yep

[23:53] <timotimo> hm actually

[23:54] <timotimo> we can totally have a variant of set_size_internal that doesn't have to worry about nulling out slots

[23:54] <timotimo> that we can use when we know we're going to write over them anyway

[23:54] <timotimo> the initial setelems for $foo and then 0 will have nulled everything out already

[23:54] <timotimo> then we're just pasting data in there from start to end

[23:55] <timotimo> write_buf at least could always use that, asplice perhaps as well

[23:57] <timotimo> it'll not be terribly much efficiency we'd win, but *shrugs* every little bit helps right?

