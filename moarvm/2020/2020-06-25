[00:01] *** sena_kun joined
[00:03] *** Altai-man_ left
[00:33] <timotimo> in that case, i wonder if compiling what deopt has to do to ops that can then be jitted rather than interpreted would be doable without making a big mess out of things

[02:01] *** Altai-man_ joined
[02:03] *** sena_kun left
[04:02] *** sena_kun joined
[04:03] *** Altai-man_ left
[04:53] *** sena_kun left
[04:54] *** sena_kun joined
[06:01] *** Altai-man_ joined
[06:04] *** sena_kun left
[06:55] <nwc10> good *, #moarvm 

[06:57] *** Kaeipi left
[07:00] *** Kaiepi joined
[07:47] *** zakharyas joined
[08:02] *** sena_kun joined
[08:03] *** Altai-man_ left
[10:01] *** Altai-man_ joined
[10:03] *** sena_kun left
[11:35] *** zakharyas left
[11:44] *** patrickb joined
[12:01] *** sena_kun joined
[12:03] *** Altai-man_ left
[12:42] *** zakharyas joined
[12:45] *** Kaiepi left
[12:48] *** Kaiepi joined
[13:02] *** Kaeipi joined
[13:06] *** Kaiepi left
[13:13] *** Kaeipi left
[13:46] <jnthn> Ah....the revenge of lazy deopt

[13:46] <jnthn> So at the point we find an exception handler...the frame we find it in may still be specialized

[13:47] <jnthn> But if there's lazy deopt, then by the time we actually reach it, then it won't be any more

[13:49] <jnthn> And ouch ouch ouch, it ain't just JIT

[13:50] <jnthn> It'll also have the specialized bytecode address wrong too

[13:50] <jnthn> Well, have the specialized address, not the deopt'd one

[13:58] *** lucasb joined
[14:01] *** Altai-man_ joined
[14:03] *** sena_kun left
[14:49] <nine> We serialize method caches? That's a bit surprising...

[14:54] <jnthn> Yes, though they're going away with new-disp

[14:55] *** Kaiepi joined
[14:55] <jnthn> *sigh* I really don't get why the clever fix I thought of for the unwind issue causes all kinds of madness...

[14:58] <nine> the usual small detail throwing a wrench...

[15:27] *** Kaiepi left
[15:43] <Geth> ¦ MoarVM/new-disp: 4579f66547 | (Jonathan Worthington)++ | src/core/frame.c

[15:43] <Geth> ¦ MoarVM/new-disp: Have frame unwind use the return mechanism

[15:43] <Geth> ¦ MoarVM/new-disp: 

[15:43] <Geth> ¦ MoarVM/new-disp: Since we perform lazy deopt on returning into a frame, we can't rely on

[15:43] <Geth> ¦ MoarVM/new-disp: the JIT or bytecode address we discovered when locating an exception

[15:43] <Geth> ¦ MoarVM/new-disp: handler still being meaningful after that has happened. So instead, set

[15:43] <Geth> ¦ MoarVM/new-disp: the return address in the caller to be those things, and then rely on

[15:43] <Geth> ¦ MoarVM/new-disp: the usual frame removal process to do the right thing, including in the

[15:43] <Geth> ¦ MoarVM/new-disp: lazy deopt case.

[15:43] <Geth> ¦ MoarVM/new-disp: review: https://github.com/MoarVM/MoarVM/commit/4579f66547

[15:43] <jnthn> Well, that's one SEGV down

[15:44] <jnthn> grmbl, I still have failures when running `make test` that I can't reproduce when I run the things alone

[15:46] <[Coke]> does the error in make test go away with TEST_JOBS=1 ?

[15:46] <jnthn> Didn't try that yet

[15:48] <MasterDuke> jnthn: fwiw, i get those on master. i.e., random fails in spectests, that i can't repro even when running that file in a loop for a lot of iterations. and it seems completely random which test file

[15:49] <jnthn> This is just test, not spectest

[15:50] *** Kaiepi joined
[15:50] <MasterDuke> ah. don't remember it happening in test on master

[15:51] <jnthn> Yeah, this is certainly a regression of sorts

[15:54] <jnthn> Despite the mass of spectest fails (most 'cus I didn't wire up multi dispatch to the new dispatcher yet, it seems), I only see two segfaults

[15:54] <jnthn> The current one is...because we have no caller...wat

[15:55] <nwc10> ASAN makes no comment on the NQP test suite

[15:56] <jnthn> I observe that to be stable also

[15:56] <jnthn> Is it happy with the Rakudo build?

[15:58] <jnthn> Hm, this one is a real curiosity

[16:01] *** sena_kun joined
[16:03] <nwc10> jnthn: it's working on that

[16:03] *** Altai-man_ left
[16:34] *** Kaiepi left
[16:36] <nwc10> jnthn: build is OK, many rakduo tests fail

[16:36] *** zakharyas1 joined
[16:38] *** zakharyas left
[16:40] <nwc10> tried one: Use of Nil in string context

[16:43] <jnthn> How many is many?

[16:44] <nwc10> 54 i think

[16:45] <jnthn> Oh, tests, not test files

[16:45] <nwc10> no, 54 test files

[16:45] <nwc10> out of 108

[16:45] <nwc10> er, at least

[16:46] <jnthn> OK, I see a single digit number

[16:46] <nwc10> I realise now that tmux can get keen and skip output

[16:46] <nwc10> I have spesh

[16:46] <nwc10> and all the pain enabled in the environment

[16:46] <jnthn> Me too, though only MVM_SPESH_BLOCKING=1

[16:46] <nwc10> oh wait, am I supposed to update NQP?

[16:47] <jnthn> If you don't have c0d4f2f289 then you'll probably have a bad time

[16:48] <jnthn> Or at least, a less good one

[16:48] <nwc10> no, I think I had that

[16:59] <jnthn> *sigh* this is so odd, it's somehow ending up in the same callframe after a return, rather than going back a frame...

[16:59] <jnthn> Well, tomorrow

[17:19] *** MasterDuke left
[17:54] *** zakharyas1 left
[18:01] *** Altai-man_ joined
[18:03] *** sena_kun left
[19:45] *** zakharyas joined
[20:01] *** sena_kun joined
[20:03] *** Altai-man_ left
[20:16] <timotimo> tbh sounds like maybe rr + watchpoints could give valuable insight here

[20:27] *** zakharyas left
[20:28] <nwc10> jnthn: in src/gc/roots.c, shouln't the code that iterates over tc->instance->sc_weakhash hold the tc->instance->mutex_sc_registry while doing this?

[20:28] <timotimo> perhaps because it's GC, and all threads are supposed to be In It Together™, you don't need to?

[20:29] <nwc10> yes, that might be the answer

[20:29] <timotimo> if only one thread is responsible for marking the weakhash, that ought to be fine

[20:29] <timotimo> i do believe our marking and sweeping is in consecutive stages, i.e. every thread syncs up before all of them go to the second phase?

[20:29] <timotimo> actually i'm basing this on ... nothing whatsoever

[20:30] <timotimo> also, i wonder if it'd be interesting to store what percentage of time is spent in mark vs sweep in the profiler

[20:30] <nwc10> I don't know enough. Nothing you have said contracts my undestanding

[20:30] <timotimo> that's a good way to say that :D :D

[20:30] <nwc10> but I was thinking something like your " i do believe our marking and sweeping " ...

[20:30] <nwc10> and thinking - but hey, can't one thread be doign the marking, including that hash, whilst some other thread decides to mutate it.

[20:33] <timotimo> btw i just had a thought

[20:33] <timotimo> then i forgot it again

[20:33] <timotimo> but i'm at least slightly certain it was a good thought

[20:36] <jnthn> nwc10: I don't think so 'cus the GC is "stop the world"?

[20:36] <nwc10> OK. I didn't know what got stopped when.

[20:37] <nwc10> I hope that I didh't wake you up :-)

[20:39] <jnthn> No, have been cooking/eating :)

[20:39] <timotimo> hey jnthn are all commits needed to see the "return to the same callframe" issue already pushed? if so, which test file is it?

[20:41] <jnthn> Yes, they are, but unfortunately I forget the test file :/

[20:41] <jnthn> (I'm on my home machine now, it's on my office machine)

[20:41] <jnthn> But basically run make spectest with MVM_SPESH_BLOCKING=1 and I only saw 2 SEGVs and it's one of them

[20:41] <timotimo> does the crash look spectacular enough that i may be able to tell relatively quickly?

[20:41] <timotimo> ah, good

[20:42] <jnthn> It's one about recursion and native arrays, iirc

[20:42] <jnthn> An integration test

[20:44] <timotimo> jnthn: were you ever interested in timing mark vs sweep on a per-thread basis for the profiler?

[20:44] <jnthn> I don't think so... :)

[20:45] <timotimo> though i think if you have telemeh active you can see that

[20:45] <timotimo> i totally need to write a telemeh-to-log-timeline translator so i actually have something to look at the output with

[20:45] <jnthn> I ain't really done much with the GC in a long time, because I'm not often seeing it be The Bottleneck

[20:45] <timotimo> or someone could put tracy into moarvm and replace telemeh with it

[20:45] <jnthn> (I'm sure there are cases where it can be)

[20:46] <timotimo> back when marking had to walk the entirety of the profile data tree, it could take rather A Time

[20:49] <timotimo> i assume to get there i'd turn off spesh during core setting compilation?

[20:49] <timotimo> and maybe all of rakudo compilation actually

[20:53] <timotimo> just a little regret from not really having worked on the dispatcher program to spesh bytecode writer 

[20:56] <timotimo> Stage parse      : 202.543

[20:56] <timotimo> woof!

[20:59] <lizmat> meow!

[21:19] <timotimo> "t/spec/integration/deep-recursion-initing-native-array.t" that sounds good

[21:39] <timotimo> slightly amused that this code results in two different specializations of init-array; one where the first argument is a scalar, the other where the second argument is a scalar

[21:39] <timotimo> because on each branch it has the - 1 on a different argument

[21:56] <timotimo> ah, bytecodedump is very unhappy with sp_dispatch_* having .s instead of .d

[22:01] *** Altai-man_ joined
[22:03] *** sena_kun left
[22:12] *** nebuchadnezzar left
[22:13] <Geth> ¦ MoarVM/new-disp: cc8634af49 | (Timo Paulssen)++ | src/core/bytecodedump.c

[22:13] <Geth> ¦ MoarVM/new-disp: teach bytecodedump about sp_dispatch_* ops

[22:13] <Geth> ¦ MoarVM/new-disp: 

[22:13] <Geth> ¦ MoarVM/new-disp: will really want to factor this out ...

[22:13] <Geth> ¦ MoarVM/new-disp: review: https://github.com/MoarVM/MoarVM/commit/cc8634af49

[22:13] <timotimo> now is cooking and eating time, but having this fixed will surely help someone at some point

[22:13] <timotimo> (also throws out debug output from bytecode dump)

[22:19] <timotimo> this is perhaps not important, but somehow all comments that optimize_disp writes to the spesh graph have order 0, even though it's supposed to be using a counter on the spesh graph that gets incremented every time

[22:24] <timotimo> i'm finding myself just looking at disp stuff in general rather than hunting the segfault

[22:30] <timotimo> i'm not sure if this is relevant, but i see the sp_dispatch_o of infix:<+> have an annotation "logged: bytecode offset 182" but in the facts i see that at offset 184 it has "505 x spesh plugin guard index 0"

[22:30] <timotimo> ("spesh" just because it's re-using the same kind of entry for spesh guard hits and dispatch guard outcomes)

[22:48] *** lucasb left
[23:02] <timotimo>       # [002] Deemed polymorphic

[23:02] <timotimo>       sp_dispatch_o     r8(2), lits(raku-rv-typecheck), callsite(0x12b1240, 2 arg, 2 pos, nonflattening, interned), sslot(3), litui32(1),   r6(3),   r5(9)

[23:05] <timotimo> when removing 2 from the bytecode address in interp.c; which makes sense, since reading the opcode advances the cur_op by 2 already, right?

[23:06] <jnthn> Yes

[23:07] <timotimo> want me to push that?

[23:11] <timotimo> of course, apart from outputting whether it has never been dispatched or is deemed poly- or monomorphic it's not doing anything so far

[23:35] *** nebuchadnezzar joined
[23:47] *** patrickb left
