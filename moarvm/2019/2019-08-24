[00:31] *** lucasb left
[00:44] <timotimo> SACRED EXCREMENT

[00:44] <timotimo> Data::MessagePack is now an extreme amount of times faster than it was before my changes

[00:44] <timotimo> oh, it looks like it's now also wrong

[00:52] <timotimo> now it is correct, and much slower than when it was wrong, but it's still a whole lot faster than before i changed anything

[01:10] <japhb> timotimo: oscillating towards the win?

[01:10] <timotimo> seems so

[01:10] <timotimo> i could go for another "faster" though

[01:23] <timotimo> so it's currently at 2 minutes for 1.8M file

[01:24] <timotimo> 39 entries in that file

[04:14] *** pamplemousse joined
[06:10] *** pamplemousse left
[06:21] *** Voldenet left
[06:26] *** Voldenet joined
[06:26] *** Voldenet left
[06:26] *** Voldenet joined
[07:07] <MasterDuke> without knowing anything about Data::MessagePack that seems slow. how long do other langs take to process the same file?

[07:22] *** Voldenet left
[07:27] *** Voldenet joined
[07:27] *** Voldenet left
[07:27] *** Voldenet joined
[09:09] *** Ven`` joined
[09:28] *** Ven`` left
[10:48] *** lucasb joined
[11:09] *** squashable6 left
[11:14] *** squashable6 joined
[12:51] *** evalable6 left
[12:52] *** evalable6 joined
[13:03] <MasterDuke> man, a whole bunch of our 3rdparty repos have newer versions available upstream, e.g., dyncall, libatomicops, libtommath, libuv

[13:21] *** Ven`` joined
[13:26] <timotimo> updating those will feel line an explosion of freshness

[13:27] <timotimo> 93 (  1 pgs): 96256 - 95504 - 1504 == -752 ( -0.78125% )

[13:27] <timotimo> well, this is working great, clearly!

[13:28] <MasterDuke> what do those numbers represent?

[13:36] <timotimo> first is the bin index, second is the number of pages in the bin

[13:36] <timotimo> then comes the total size of the pages minus how much space is left at the end

[13:37] <timotimo> actually, no, not the "space left at the end" bit

[13:38] <timotimo> ...

[13:38] <timotimo> i made these names confusing

[13:40] <MasterDuke> do you want space left at the end?

[13:43] <timotimo> it's not really preventable

[13:58] <Ven``> if you add a new opcode to moarvm, how do you usually test it? also lay the groundwork in nqp/rakudo necessary?

[14:00] <timotimo> the tests for opcodes go into nqp

[14:00] <timotimo> moar relies on nqp for the vast majority of its testing needs

[14:00] <timotimo> well, that and rakudo, but a little more indirectly over there

[14:06] <Ven``> timotimo: yeah, the actual tests for sure, I meant more for experimentation

[14:16] <Kaiepi> so because of some of the changes i'm making to make it so ipv6 addresses don't get used when they're not supposed to be able to, it's now possible to close async udp sockets in the middle of attempting to do a write, and just checking if the handle's closing before each attempt to write isn't good enough to prevent that from happening

[14:17] <Kaiepi> i tried using a semaphore to fix this, but idt i can just go and destroy it on close with threads waiting on it, i need something else on top of that or to go about this entirely differently. how do i make stuff like this thread-safe?

[14:20] *** chloekek joined
[14:22] <timotimo> Kaiepi: maybe a mechanism like "free at safepoint" can help with this issue

[14:27] <Kaiepi> like, keeping whether or not the handle wants to be closed somewhere in state and adding a callback that gets called after each write that checks if there are any threads waiting on the semaphore before destroying it when it wants to close?

[14:28] <Kaiepi> or rather, handles the actual closing

[14:42] *** chloekek left
[14:42] <Kaiepi> however it should work, uv_once_t looks like it'd be useful here

[14:56] *** Ven`` left
[14:59] *** Ven`` joined
[15:06] *** Ven`` left
[16:38] *** domidumont joined
[18:29] *** domidumont left
[19:45] *** chloekek joined
[20:22] *** pamplemousse joined
[20:52] *** Kaiepi left
[20:55] *** Kaiepi joined
[21:18] *** lucasb left
[21:34] *** pamplemousse left
[21:43] *** pamplemousse joined
[22:11] *** pamplemousse left
[22:19] <Geth> Â¦ MoarVM: MasterDuke17++ created pull request #1162: Jit some num ops

[22:19] <Geth> Â¦ MoarVM: review: https://github.com/MoarVM/MoarVM/pull/1162

[22:23] *** chloekek left
[22:49] *** pamplemousse joined
[23:08] <jnthn> Kaiepi: There's only one thread that does all the async I/O (all things to do are marshalled there, and all results are delivered into the queue of the thread pool scheduler). So you don't need any locking there, just to make sure all requests are properly sent to the I/O worker.

[23:08] <tellable6> https://gist.github.com/db92a797bfb3de8a0247576d2205d248

[23:17] <Kaiepi> oh wow i'm fucking dumb, i was getting a double free during cleanup after writing that made me think that but it was because i was freeing the request and its data in the wrong order

[23:17] <Kaiepi> closing doesn't even free the same request in the first place

[23:24] <Kaiepi> but knowing that i/o can only be done from one thread is really helpful, thanks jnthn

[23:30] <jnthn> Kaiepi: This only applies to the async I/O stuff, but the non-async I/O also wraps a mutex acquisition around the operations, so you're already safe in the I/O vtable handlers too

[23:33] <Kaiepi> yeah, i saw how some parts of sync i/o use mutexes and assumed it was possible with async i/o as well

[23:34] <Kaiepi> btw what's the logic in what sync i/o calls require calls to MVM_gc_mark_thread_blocked/MVM_gc_mark_thread_unblocked and which don't? because there are a bunch that don't atm

[23:35] <jnthn> If it can block, it should have the calls

[23:36] <jnthn> Where by block we really mean "is I/O bound"

[23:37] <jnthn> Rest time; 'night

[23:47] *** pamplemousse left
