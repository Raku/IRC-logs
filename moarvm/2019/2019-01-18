[02:00] *** timotimo left
[02:08] *** timotimo joined
[02:08] *** p6bannerbot sets mode: +v timotimo

[02:08] *** lizmat left
[02:50] <AlexDaniel> dogbert11: maybe you should try reproducing this: https://github.com/rakudo/rakudo/issues/2576

[02:50] <AlexDaniel> my gut feeling tells me it's not just on mips

[03:04] *** lizmat joined
[03:04] *** p6bannerbot sets mode: +v lizmat

[03:44] *** squashable6 left
[03:48] *** squashable6 joined
[03:49] *** p6bannerbot sets mode: +v squashable6

[06:01] *** robertle left
[07:03] *** domidumont joined
[07:03] *** p6bannerbot sets mode: +v domidumont

[07:43] <Geth> Â¦ MoarVM: 2bc5ce7b84 | (Stefan Seifert)++ | src/6model/reprs/CStruct.c

[07:43] <Geth> Â¦ MoarVM: Fix segfaults related to corrupted CStruct STables

[07:43] <Geth> Â¦ MoarVM:

[07:43] <Geth> Â¦ MoarVM: The fix is basically a port of commit 6733e927ceb1aeff0daf23a58c43730cb2c38d77

[07:43] <Geth> Â¦ MoarVM: from CUnion to CStruct. It fixes composed CStruct STables showing up with a

[07:43] <Geth> Â¦ MoarVM: NULL REPR_data pointer and garbage in sc_forward_u caused by an unfortunately

[07:43] <Geth> Â¦ MoarVM: timed GC run.

[07:43] <Geth> Â¦ MoarVM: Fixes GH #2564 (segfault in t/04-nativecall/06-struct.t)

[07:44] <Geth> Â¦ MoarVM: review: https://github.com/MoarVM/MoarVM/commit/2bc5ce7b84

[08:02] *** robertle joined
[08:03] *** p6bannerbot sets mode: +v robertle

[09:34] <Geth> Â¦ MoarVM: c001ce23ca | (Timo Paulssen)++ | src/math/bigintops.c

[09:34] <Geth> Â¦ MoarVM: fix declaration after code (thanks @dod38fr)

[09:34] <Geth> Â¦ MoarVM: review: https://github.com/MoarVM/MoarVM/commit/c001ce23ca

[10:07] *** zakharyas joined
[10:08] *** p6bannerbot sets mode: +v zakharyas

[10:38] *** brrt joined
[10:39] *** p6bannerbot sets mode: +v brrt

[11:06] *** domidumont left
[11:34] *** brrt left
[11:34] *** brrt joined
[11:35] *** p6bannerbot sets mode: +v brrt

[11:59] *** brrt left
[12:02] *** robertle left
[12:04] *** robertle joined
[12:05] *** p6bannerbot sets mode: +v robertle

[12:38] *** robertle left
[12:45] *** dogbert2_ joined
[12:45] *** p6bannerbot sets mode: +v dogbert2_

[12:46] *** domidumont joined
[12:47] *** p6bannerbot sets mode: +v domidumont

[13:12] *** robertle joined
[13:12] *** p6bannerbot sets mode: +v robertle

[13:34] *** brrt joined
[13:35] *** p6bannerbot sets mode: +v brrt

[13:41] *** brrt left
[14:05] *** zakharyas left
[15:11] *** brrt joined
[15:11] *** p6bannerbot sets mode: +v brrt

[15:38] *** brrt` joined
[15:39] *** p6bannerbot sets mode: +v brrt`

[15:39] *** brrt left
[15:40] *** lizmat left
[16:29] *** domidumont left
[16:30] *** MasterDuke joined
[16:30] *** p6bannerbot sets mode: +v MasterDuke

[16:30] *** MasterDuke left
[16:30] *** MasterDuke joined
[16:30] *** herbert.freenode.net sets mode: +v MasterDuke

[16:30] *** p6bannerbot sets mode: +v MasterDuke

[16:31] *** brrt` left
[16:31] *** robertle left
[17:00] <MasterDuke> timotimo: huh, looks like you can't use %p in profile filenames to get the pid in there, only in the MVM_*_LOGs

[17:03] <timotimo> i should probably implement that!

[17:04] <MasterDuke> that would be nice

[17:07] <timotimo> if you could somehow target a very specific invocation to have profiling enabled ...

[17:07] <timotimo> also, ATM you can't have "give me sql, but randomly generated filename", which the %p thing would fix

[17:08] <MasterDuke> yep

[17:12] <nine> I don't get this. I have a branch for Inline::Perl5 that removes some no longer used arguments which gets some functions below speshs arguments limit. The profiler shows much better numbers for this branch but the actual run time is some 15 % higher.

[17:17] <MasterDuke> nine: does MVM_SPESH_BLOCKING make a difference?

[17:20] <nine> MasterDuke: none that I can see

[17:27] <MasterDuke> how are you measuring? with something like 'now - INIT now' or with /usr/bin/time? maybe compilation is taking longer?

[17:27] <nine> /usr/bin/time

[17:28] <MasterDuke> --stagestats show anything interesting?

[17:28] <nine> Startup overhead is virtually identical (measured by using only 1 CSV row as data)

[17:29] <MasterDuke> weird indeed

[17:29] <nine> Btw. what's the reason why a speshresolve can remain in speshed code?

[17:30] <MasterDuke> perf record?

[17:30] <MasterDuke> heh...i've been trying to track down the answer to that exact question for at least a week now

[17:31] <nine> gotten any closer? Is that an intended situation or is something wrong?

[17:31] <MasterDuke> nine: i the case i've been looking at, https://github.com/MoarVM/MoarVM/blob/master/src/spesh/optimize.c#L2126 is relevant

[17:31] <MasterDuke> *in the

[17:32] <MasterDuke> but that's where my understanding ends. i was chatting with timotimo about it some, you and him taking would probably make more progress than i have/would

[17:34] *** domidumont joined
[17:35] <MasterDuke> fwiw, it's from this Str.index multi https://github.com/rakudo/rakudo/blob/master/src/core/Str.pm6#L272

[17:35] *** p6bannerbot sets mode: +v domidumont

[17:36] *** lizmat joined
[17:36] *** p6bannerbot sets mode: +v lizmat

[17:39] <nine> MasterDuke: same line is hit in my case

[17:40] <nine> MasterDuke: your case also on a decontrv?

[17:44] <MasterDuke> nine: don't remember

[17:45] <MasterDuke> this is the code i'm testing with: 'my $a; $a = $_.Str.index("0", 0) for 1 .. 2_000_000; say $a'

[18:19] *** robertle joined
[18:19] *** p6bannerbot sets mode: +v robertle

[18:48] *** [Coke] left
[18:48] *** [Coke] joined
[18:48] *** [Coke] left
[18:48] *** [Coke] joined
[18:48] *** p6bannerbot sets mode: +v [Coke]

[18:49] *** p6bannerbot sets mode: +v [Coke]

[20:11] <nine> MasterDuke: I firmly believe that that return there is just wrong. I've replaced it and everything seems to still work and I see performance improvements in csv-ip5xs.pl

[20:13] <dogbert11> this can't be good: MoarVM panic: Tried to garbage-collect a locked mutex

[20:17] <nine> jnthn: does this patch make sense? https://gist.github.com/niner/7f146f55c52d43608952f6bf16867711

[20:24] *** domidumont left
[20:24] <nine> Another real conundrum: in my supposedly faster branch enabling the profiler speeds things up from ~24 to ~21s!

[20:25] <lizmat> how is that a conundrum ?  :-)

[20:25] <timotimo> the profiler is supposed to not make things go faster %)

[20:25] <lizmat> aaaahhh... ok

[20:25] *** AlexDaniel left
[20:26] <lizmat> well, the solution is easy then: always run with the profiler  :-)

[20:26] *** AlexDaniel joined
[20:26] *** p6bannerbot sets mode: +v AlexDaniel

[20:26] <nine> Btw. the profiles do show sensible numbers. 28s for the unoptimized version vs. 20s for the optimized one. It's just the real wall clock times that make no sense.

[20:27] <lizmat> how about CPU usage ?

[20:27] <nine> Pretty much the same. It's just ~200-300ms of spesh time

[20:28] <timotimo> could be slower to compile i guess?

[20:30] <nine> All numbers are better in the optimized branch. Less runtime according to the profiler, millions of frames saved, much less GC, 2/3 of deopts (~2 million!) gone, 99.97 % JIT instead of 97.54 %, > 10 million fewer allocates.

[20:30] <nine> But wall clock time is slower

[20:30] <nine> timotimo: I checked. Their compile time is virtually identical

[20:32] <timotimo> millions of lines saved sounds like perhaps a lot less profiler overhead

[20:36] *** zakharyas joined
[20:36] *** p6bannerbot sets mode: +v zakharyas

[20:36] <nine> timotimo: btw. what do you think about this? https://gist.github.com/niner/7f146f55c52d43608952f6bf16867711

[20:37] <timotimo> i'm not entirely sure what that entails

[20:41] <dogbert11> here's a gist of what gdb could uncover wrt the failure in t/spec/integration/eval-and-threads.t, https://gist.github.com/dogbert17/8d0ed18404c2400283504d7b8949c545

[20:42] *** MasterDuke left
[20:44] <timotimo> shall i hack a totally bad patch that records when a mutex was last successfully locked?

[20:45] <nine> timotimo: go for it!

[20:46] <dogbert11> if it could help us pinpoint the problem why not :)

[20:46] *** zakharyas left
[20:51] <timotimo> .o( pingpoint )

[20:57] <AlexDaniel> dogbert11: I think there was an issue that you had to file? :)

[20:57] <AlexDaniel> dogbert11: we're running out of blockers :)

[21:07] <dogbert11> AlexDaniel: if we disregard the problem above we have the t/spec/S04-exceptions/fail.t problem

[21:08] <dogbert11> releasable: next

[21:08] <releasable6> dogbert11, Next release in â‰ˆ21 hours. 3 blockers. 112 out of 238 commits logged (âš  41 warnings)

[21:08] <releasable6> dogbert11, Details: https://gist.github.com/cd1a6f4147484b45ad08222f934101fc

[21:09] <timotimo> my silly patch is ... pretty explosive

[21:11] <timotimo> if you're okay with a moarvm that leaks memory like a ripped sieve, you can have it, though

[21:13] <timotimo> huh. it seems to work now?

[21:13] <timotimo> maybe recompiling rakudo was the magic ingredient ...

[21:14] <timotimo> OH COME ON :(

[21:14] <timotimo> Failed to open file /home/timo/perl6/install/share/perl6/vendor/version: No space left on device

[21:15] <timotimo> oh, silly me

[21:17] <timotimo> of course now it doesn't reproduce any more :|

[21:18] <dogbert11> AlexDaniel: R#2623

[21:20] <dogbert11> hmm, an explosive patch

[21:23] <Geth> Â¦ MoarVM/pinpoint_garbage-collected_locked_mutex: 409fef09bd | (Timo Paulssen)++ | 4 files

[21:23] <Geth> Â¦ MoarVM/pinpoint_garbage-collected_locked_mutex: dump a trace when mutex gets GC'd while locked

[21:23] <Geth> Â¦ MoarVM/pinpoint_garbage-collected_locked_mutex:

[21:23] <Geth> Â¦ MoarVM/pinpoint_garbage-collected_locked_mutex: more precisely, the backtrace is taken when the

[21:23] <Geth> Â¦ MoarVM/pinpoint_garbage-collected_locked_mutex: lock gets acquired (and cleared when it gets

[21:23] <Geth> Â¦ MoarVM/pinpoint_garbage-collected_locked_mutex: unlocked)

[21:23] <Geth> Â¦ MoarVM/pinpoint_garbage-collected_locked_mutex: review: https://github.com/MoarVM/MoarVM/commit/409fef09bd

[21:23] <timotimo> i couldn't get it to reproduce any more with this applied

[21:23] <timotimo> but if it does crash it could maybe give an interesting piece of info

[21:24] <dogbert11> so I switch to this branch and rerun ?

[21:25] <timotimo> switch or cherry-pick or whatever

[21:26] <dogbert11> let's see if it happens again ...

[21:28] <dogbert11> baaam

[21:28] <timotimo> oh?

[21:28] <dogbert11> should I run MVM_dump_backtrace now?

[21:28] <timotimo> that's sheepish

[21:28] <timotimo> no, it's supposed to output the backtrace by itself

[21:28] <dogbert11> aha, the I should continue in gdb I guess

[21:29] <dogbert11> *then

[21:29] <timotimo> depends on where your breakpoint lives?

[21:29] <timotimo> you can just as well "print rm->body.backtrace"

[21:29] <dogbert11> in MVM_panic

[21:29] <timotimo> then it should already have outputted it

[21:29] <dogbert11> no, the backtrace is here in fron of my blind eyes, sec

[21:29] <timotimo> haha

[21:30] <timotimo> well, anyway, that backtrace tells you where the lock was last locked successfully

[21:30] <timotimo> sadly, the backtrace from where the panic happens doesn't correspond to anything

[21:31] <dogbert11> https://gist.github.com/dogbert17/8d0ed18404c2400283504d7b8949c545

[21:31] <dogbert11> check the comment

[21:31] <timotimo> i see it

[21:32] <timotimo> hm, is that the buffertocu that locks the lock?

[21:33] <dogbert11> buffertocu?

[21:33] <timotimo> nqp::buffertocu

[21:33] <dogbert11> ah

[21:34] <timotimo> is the only line in assemble_and_load

[21:35] <timotimo> well, i suppose we might also need a gdb-level backtrace?!

[21:35] <dogbert11> wasn't nine working on this recently?

[21:36] <timotimo> yeah, but not sure if he touched that C part of the code specifically

[21:36] <dogbert11> perhaps I'm jumping to incorrect conclusions

[21:37] <timotimo> i can't find the actual corresponding lock call :)

[21:40] <timotimo> could perhaps be MVM_bytecode_finish_frame

[21:41] <dogbert11> there's mutex stuff happening there

[22:00] <timotimo> so yeah anyway some mutex is locked and then whatever's being done ends in not unlocking the mutex and the reference disappears

[22:00] <timotimo> that's probably what happens

[22:01] <dogbert11> trying to repro again but so far nothing

[22:01] <timotimo> damn it :|

[22:02] <dogbert11> I just wanted to see if I get the same stacktrace

[22:02] <timotimo> OK

[22:03] <dogbert11> I'm running it in the background so perhaps it will trigger again

[22:04] <timotimo> best of luck!

[22:07] <dogbert11> in the meantime, libuv 1.25.0 is out. https://github.com/libuv/libuv/blob/v1.x/ChangeLog

[22:09] *** synopsebot joined
[22:10] <dogbert11> and the bug triggered again, the first line is still 'gen/moar/stage2/QAST.nqp:7325  (/home/dogbert/repos/rakudo/install/share/nqp/lib/QAST.moarvm:assemble_and_load)'

[22:10] *** p6bannerbot sets mode: +v synopsebot

[22:10] <AlexDaniel> R#2623

[22:10] <synopsebot> R#2623 [open]: https://github.com/rakudo/rakudo/issues/2623 Intermittent failure when running t/spec/S04-exceptions/fail.t

[22:11] <timotimo> it's not completely unlikely that reducing the nursery size will trigger it more often

[22:11] *** Kaiepi left
[22:11] <dogbert11> and suddenly the number of blockers have risen to 4

[22:11] <AlexDaniel> hahaha

[22:12] <AlexDaniel> dogbert11: no strong opinion on that one, but this kind of issues causes all sorts of troubles in the end

[22:12] *** Kaiepi joined
[22:12] <AlexDaniel> e.g. failures in buildd

[22:12] <AlexDaniel> I think robertle++ changed the way the package is built

[22:12] <AlexDaniel> so that it doesn't use spesh on mips, I think? I'm not sure

[22:12] *** p6bannerbot sets mode: +v Kaiepi

[22:13] <AlexDaniel> but sweeping under the rug is not a great strategy for the long run

[22:13] <dogbert11> how true

[22:14] <dogbert11> I might have to report this mutex problem as well

[22:14] <AlexDaniel> I've been running t/06-telemetry/01-basic.t in a loop for like half a day

[22:14] <AlexDaniel> and no failure so far

[22:15] <dogbert11> I couldn't reproduce that one either

[22:17] <AlexDaniel> btw nine++ jnthn++ lizmat++ # working on blockers

[22:17] <AlexDaniel> our heroes :)

[22:17] <dogbert11> indeed

[22:56] <jnthn> nine: Yes, I believe it does; nice find

[23:40] *** MasterDuke joined
[23:40] *** p6bannerbot sets mode: +v MasterDuke

[23:40] *** MasterDuke left
[23:40] *** MasterDuke joined
[23:40] *** herbert.freenode.net sets mode: +v MasterDuke

[23:40] *** p6bannerbot sets mode: +v MasterDuke

[23:44] <MasterDuke> nine++ that makes my test case much better. i tried some random changes, but had no idea what i was doing and none of mine made anything better

