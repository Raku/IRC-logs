[00:05] *** anatofuz left
[00:06] *** anatofuz joined
[00:11] *** anatofuz left
[00:18] *** anatofuz joined
[00:22] *** anatofuz left
[00:23] *** anatofuz joined
[00:25] *** anatofuz left
[00:25] *** anatofuz joined
[00:48] *** lucasb left
[00:49] *** anatofuz left
[01:09] <Geth_> ¬¶ MoarVM/more_efficient_serialization: 0b47ff9ec3 | (Timo Paulssen)++ | src/6model/serialization.c

[01:09] <Geth_> ¬¶ MoarVM/more_efficient_serialization: serialization: smaller storage for empty arrays/hashes

[01:09] <Geth_> ¬¶ MoarVM/more_efficient_serialization: review: https://github.com/MoarVM/MoarVM/commit/0b47ff9ec3

[01:11] <timotimo> that's the 0.075% win or whatever that was

[01:13] *** anatofuz joined
[01:34] *** tellable6 left
[01:34] *** anatofuz left
[01:38] *** tellable6 joined
[01:54] *** anatofuz joined
[01:57] *** anatofuz left
[02:11] *** anatofuz joined
[02:14] *** anatofuz left
[02:19] *** anatofuz joined
[02:21] *** anatofuz left
[02:37] *** anatofuz joined
[02:41] *** anatofuz left
[02:43] *** anatofuz joined
[03:00] *** anatofuz left
[03:14] *** anatofuz joined
[03:18] *** anatofuz left
[03:20] *** anatofuz joined
[03:31] *** anatofuz left
[03:32] *** anatofuz joined
[03:37] *** anatofuz left
[03:51] *** anatofuz joined
[03:55] *** anatofuz left
[03:57] *** anatofuz joined
[04:03] *** anatofuz left
[04:12] *** anatofuz joined
[04:17] *** anatofuz left
[04:41] *** anatofuz joined
[04:45] *** anatofuz left
[04:54] *** anatofuz joined
[04:59] *** anatofuz left
[05:08] *** anatofuz joined
[05:12] *** anatofuz left
[05:22] *** anatofuz joined
[05:27] *** anatofuz left
[05:30] *** anatofuz joined
[05:37] *** sena_kun joined
[05:41] *** anatofuz left
[05:47] *** anatofuz joined
[06:03] *** squashable6 left
[06:06] *** squashable6 joined
[06:11] *** anatofuz left
[06:14] *** domidumont joined
[06:15] *** anatofuz joined
[06:15] *** domidumont left
[06:16] *** domidumont joined
[06:23] *** domidumont left
[06:29] *** anatofuz left
[06:40] *** domidumont joined
[06:40] *** anatofuz joined
[06:41] *** anatofuz left
[06:41] *** anatofuz joined
[06:43] *** anatofuz left
[06:45] *** anatofuz joined
[06:45] *** anatofuz left
[07:11] *** anatofuz joined
[07:12] *** anatofuz left
[07:28] *** anatofuz joined
[07:33] *** anatofuz left
[07:39] *** anatofuz joined
[07:44] *** anatofuz left
[08:08] *** sena_kun left
[08:11] *** zakharyas joined
[08:52] *** anatofuz joined
[08:55] *** sena_kun joined
[09:05] *** anatofuz left
[09:05] *** anatofuz joined
[09:06] *** anatofuz left
[09:07] *** anatofuz joined
[09:27] *** MasterDuke left
[09:47] *** anatofuz left
[09:47] *** anatofuz joined
[09:49] *** anatofuz left
[09:49] *** anatofuz joined
[09:52] *** anatofuz left
[10:01] *** anatofuz joined
[10:08] *** domidumont left
[10:17] <nine> Ok, I'm starting to become desparate here. To recap: I have an easily reproducible case of a mixin's STable pointing at a corrupt HOW. The STable in question seems to be freed by the GC but reappears later in an invokee. I can only reproduce the issue if multiple threads create similar mixins. But even completely deactivating the mixin cache has no effect.

[10:20] <lizmat> so, a race condition on creating identical subtypes ?

[10:21] <lizmat> perhaps deactivating the mixin cache exacerbates the problem ?

[10:21] <nine> Apparently. But I can't find anything that a race could be on. The mixin cache was the obvious candidate for a shared data structure that gets modified in the process, but it doesn't look like it's at fault

[10:22] <nine> Deactivating the mixin cache seems to have no effect whatsoever, except for the expected slow down of course

[10:24] <lizmat> hmmm... if it slows down, but then does not make the problem go away or be worse, then maybe it isn't a race condition after all ?

[10:24] <lizmat> maybe the problem is then *accessing* the same subtype from different threads?

[10:25] <jnthn> I wonder if it's anything to do with the parameterization handling at the VM level

[10:28] <nine> jnthn: what would that be?

[10:32] *** Guest69469 joined
[10:34] <jnthn> nine: The use of setparameterizer in MixinCacheHOW

[10:35] <jnthn> Though you mentioned you disabled that and still saw the issue.

[10:35] <jnthn> So maybe not.

[10:35] <jnthn> Did you have an isolated example?

[10:35] <jnthn> Is this the segv you did on the evalbot yesterday?

[10:36] <nine> that's the one, yes

[10:37] <nine> Ok, to be clear, what I disabled was this code: https://github.com/MoarVM/MoarVM/blob/master/src/6model/parametric.c#L63

[10:37] <nine> I didn't know that there was a higher level cache, too

[10:38] <jnthn> Ah, well, it mostly just wraps around the thing you mentioned

[10:44] <nine> I have found quite a number of issues while looking including GC issues with find_method and friends and a deadlock but not what I was actually looking for

[10:45] <nine> Odd...  turning this https://github.com/rakudo/rakudo/blob/master/src/Perl6/Metamodel/Mixins.nqp#L41 into my $mixin_type := 0 && self.is_composed($obj) to never use the cache breaks the build: array[int] *must* be parameterized

[10:51] <nine> So instead I create a new cache object for every call. That gets me far enough through the build to run my test case and it still fails

[11:30] <timotimo> nine: does the problem also happen when valgrind is active? because there's a what-points-here command you can send to it via its gdbserver interface

[11:31] <timotimo> if you know the address of the object that had been freed, you can check what has pointers at it

[11:31] <Guest69469> nine: I guess that the GC stacktraces were of no use then?

[11:39] <nine> Guest69469: unfortunately not :/

[11:40] <nine> timotimo: oh, valgrind definitely reports them as Invalid reads

[11:40] <nine> timotimo: how does the gdbserver interface thing work?

[11:42] *** zakharyas left
[11:45] <timotimo> valgrind --vgdb=yes --vgdb-error=0 prog

[11:45] <timotimo> gdb prog

[11:45] <timotimo> (gdb) target remote | vgdb

[11:45] <timotimo> i don't think i've tried this yet tbh

[11:46] <timotimo> http://valgrind.org/docs/manual/manual-core-adv.html#manual-core-adv.gdbserver-simple

[11:47] <timotimo> i recommend configuring moarvm with --valgrind if you haven't yet

[11:55] <timotimo> cool, the valgrind gdbserver supports read and write watchpoints; any number of them, and with no length limit

[12:03] *** domidumont joined
[12:05] *** domidumont left
[12:06] *** domidumont joined
[12:32] *** lucasb joined
[12:49] <nine> I do get a list of memory loations pointing at the STable in question. All but one of them are on different threads' stacks

[12:53] <timotimo> on stacks, huh

[12:53] <timotimo> that's not fun

[12:53] <timotimo> have you been able to find which stack frames correspond to the locations?

[12:56] <nine> in frame #5, created by MVM_interp_run (interp.c:104)

[13:17] *** anatofuz left
[13:27] *** zakharyas joined
[13:36] *** domidumont left
[13:45] <nine> Oh, this may be a deciding hint: the STable is destroyed by MVM_gc_gen2_destroy during a thread's destruction!

[13:50] <nine> Yes, I totally fail to reproduce the issue using the thread pool.

[13:55] <timotimo> oooh

[13:56] <timotimo> i'm now using the uppermost 4 bits of the discriminator as "extra data" storage space

[13:56] <nine> So it may either be some bug in thread cleanup. Or maybe still a missing write barrier as without the gen2roots entry, the pointer will not be updated.

[13:56] <timotimo> on thread destruction we're actually trying to copy everything out to another thread

[13:56] <nine> But....where are pointers updated? I see code for copying memory and for taking over the gen2roots, but not for actually updating the roots

[13:57] <nine> else { /* note: we don't have tests that exercise this path yet. */

[13:57] *** domidumont joined
[13:58] <nine> Or do those objects actually stay where they are and the memory areas are taken over by the receiving thread?

[14:00] <timotimo> not sure

[14:00] <timotimo> i would assume one way to do it - if not the way we do it - is to set a forwarder on all of the objects and then run a GC

[14:01] <timotimo> but simply merging the gen2 pages to another gen2 allocator could also work

[14:01] <timotimo> gen2 are per thread, right?

[14:01] <nine> I guess so

[14:01] <nine> Anyway, have to leave for a couple hours

[14:02] <timotimo> good hunting though

[14:03] *** zakharyas left
[14:28] <timotimo> m: say 14630232 - 14594536; say 54592 - 54440; say 120792 - 120584

[14:28] <camelia> rakudo-moar 17d036ad4: OUTPUT: ¬´35696‚ê§152‚ê§208‚ê§¬ª

[14:28] <timotimo> savings for core.c, core.d, and core.e settings

[14:29] <timotimo> m: sub infix:<->($a, $b) { $a * 100 / $b }; say 14630232 - 14594536; say 54592 - 54440; say 120792 - 120584

[14:29] <camelia> rakudo-moar 17d036ad4: OUTPUT: ¬´100.24458469‚ê§100.279206‚ê§100.172494‚ê§¬ª

[14:29] <timotimo> m: sub infix:<->($b, $a) { $a * 100 / $b }; say 14630232 - 14594536; say 54592 - 54440; say 120792 - 120584

[14:29] <camelia> rakudo-moar 17d036ad4: OUTPUT: ¬´99.75601207‚ê§99.721571‚ê§99.827803‚ê§¬ª

[14:29] <timotimo> m: sub infix:<->($b, $a) { 100 - $a * 100 / $b }; say 14630232 - 14594536; say 54592 - 54440; say 120792 - 120584

[14:29] <timotimo> oops

[14:30] <timotimo> oh well

[14:34] *** camelia left
[14:36] *** camelia joined
[14:37] <camelia> rakudo-moar 17d036ad4: OUTPUT: ¬´(timeout)¬ª

[14:42] <timotimo> that took a while

[14:57] <Guest69469> m: is-prime(-1)

[14:57] <camelia> rakudo-moar 17d036ad4: OUTPUT: ¬´WARNINGS for <tmp>:‚ê§Useless use of "is-prime(-1)" in expression "is-prime(-1)" in sink context (line 1)‚ê§Invalid number of rounds (100), valid range is 0..256‚ê§‚ê§  in block <unit> at <tmp> line 1‚ê§‚ê§¬ª

[15:08] *** domidumont left
[15:19] *** domidumont joined
[15:35] *** MasterDuke joined
[15:38] *** domidumont left
[15:57] *** Kaiepi left
[15:58] *** Kaiepi joined
[16:11] *** domidumont joined
[16:26] *** domidumont left
[16:28] *** domidumont joined
[16:37] *** domidumont left
[16:53] *** robertle joined
[16:57] <timotimo> urgh, the serialization doesn't work out right any more with my changes :|

[17:43] *** nebuchad` joined
[17:44] *** nebuchadnezzar left
[17:44] *** nebuchad` is now known as nebuchadnezzar

[18:12] <Geth_> ¬¶ MoarVM/more_efficient_serialization: 20a340be9a | (Timo Paulssen)++ | 2 files

[18:12] <Geth_> ¬¶ MoarVM/more_efficient_serialization: store 4 extra data bits in the discriminator

[18:12] <Geth_> ¬¶ MoarVM/more_efficient_serialization:

[18:12] <Geth_> ¬¶ MoarVM/more_efficient_serialization: currently used for small integers (-1 through 14), the

[18:12] <Geth_> ¬¶ MoarVM/more_efficient_serialization: size of object/int/str arrays, and hashes (only the ones

[18:12] <Geth_> ¬¶ MoarVM/more_efficient_serialization: directly implemented in the serializer code)

[18:12] <Geth_> ¬¶ MoarVM/more_efficient_serialization: review: https://github.com/MoarVM/MoarVM/commit/20a340be9a

[18:12] <Geth_> ¬¶ MoarVM/more_efficient_serialization: 3aa0169786 | (Timo Paulssen)++ | src/6model/serialization.c

[18:12] <Geth_> ¬¶ MoarVM/more_efficient_serialization: fix an error message (that should never happen)

[18:12] <Geth_> ¬¶ MoarVM/more_efficient_serialization: review: https://github.com/MoarVM/MoarVM/commit/3aa0169786

[18:12] <Geth_> ¬¶ MoarVM/more_efficient_serialization: 93900418c9 | (Timo Paulssen)++ | 3 files

[18:12] <Geth_> ¬¶ MoarVM/more_efficient_serialization: attempt to refactor discrim extra data; broken

[18:12] <Geth_> ¬¶ MoarVM/more_efficient_serialization: review: https://github.com/MoarVM/MoarVM/commit/93900418c9

[20:00] <timotimo> i'm not entirely sure if the heap dump is correct here, the code refs array in an MVMCompUnitpoints at some BOOTCode that points, via being something's Caller, to data that's been created during the current run

[20:10] *** domidumont joined
[20:11] *** domidumont left
[20:30] *** sena_kun left
[20:43] <timotimo> https://gist.github.com/timo/d587372f665624a6743dceecffe4f391 - does this look reasonable?

[20:45] *** Ven`` joined
[20:51] <timotimo>       Permanent Roots (Permanent Roots:0) (Frame) (374912)

[20:51] <timotimo> okay *this* definitely doesn't look right

[21:09] *** MasterDuke left
[21:31] <samcv> when is it estimated for MoarVM release?

[21:33] *** anatofuz joined
[22:10] <AlexDaniel> samcv: oh I don't know‚Ä¶ there are a bit too many blockers right now

[22:16] *** anatofuz left
[22:17] *** Ven`` left
[22:30] *** anatofuz joined
[22:34] *** anatofuz left
[22:56] *** anatofuz joined
[23:20] *** Kaiepi left
[23:23] *** Kaiepi joined
[23:41] *** anatofuz left
[23:51] *** anatofuz joined
[23:52] *** lucasb left
[23:55] *** anatofuz left
