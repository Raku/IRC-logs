[00:00] *** lizmat joined
[00:12] *** reportable6 joined
[00:27] *** Kaiepi joined
[01:09] *** bisectable6 joined
[01:57] *** ilbot3 joined
[02:10] *** yoleaux joined
[03:58] *** greppable6 joined
[04:04] *** ilmari_ joined
[04:05] *** ilmari joined
[04:44] *** Voldenet joined
[04:50] *** Voldenet joined
[04:57] *** ilmari[m] joined
[05:24] *** wictory[m] joined
[05:44] *** eater joined
[05:45] *** AlexDaniel` joined
[06:21] *** lizmat joined
[06:35] *** domidumont joined
[06:42] *** domidumont joined
[06:44] *** robertle joined
[07:16] *** AlexDaniel joined
[08:22] <nwc10> good *, #moarvm

[08:27] *** AlexDaniel joined
[09:34] *** zakharyas joined
[09:37] *** lizmat joined
[10:51] <jnthn> o/

[10:57] <dogbert2_> good afternoon jnthn

[11:01] <dogbert2_> it's quite silent/empty here so far today

[11:07] <dogbert2_> as always I'm trying to lure someone to take a look at some suspicious gdb output

[11:07] <nwc10> I think folks only work mornings here :-)

[11:08] * jnthn lost most of his morning to visitng the dentist, alas

[11:08] <dogbert2_> where I live visiting the dentist => cavity in the wallet

[11:15] <robertle> hi jnthn, I was wondering whether you could give me some tips about R#1711? note that this is different from earlier big-endian issues insofar that a) it is intermittent and not immediately reproducible, and b) that the error message always says "offset 0, instruction 0" (zeroes)

[11:15] <synopsebot> R#1711 [open]: https://github.com/rakudo/rakudo/issues/1711 [severe] intermittent "Bytecode validation error at offset 0, instruction 0" on s390x/mips

[11:16] <robertle> I am wondering whether there is anything I can do to shed some light on it. MVM_SPESH_DISABLE does not prevent it,

[11:16] <jnthn> hah, that was going to be my first question

[11:16] <robertle> I thought so ;)

[11:16] <jnthn> Does valgrind work on such platforms and does it show any memory errors when this happens?

[11:17] <robertle> I have not tried, but it is quite intermittent. I need to run the test suite in a loop for a long time before getting any failures. I guess valgrind will slow that down even more. but I can try...

[11:18] <robertle> I find the zero offset interesting, isn't it?

[11:18] <timotimo> ISTR bytecode validation only runs when we load a piece of bytecode for the first time; since we do that lazily, maybe we can try turning off lazy deserialization and see if that makes the problem appear more often

[11:18] <yoleaux> 09:39Z <lizmat> timotimo: updated

[11:19] <timotimo> there might be a #define for that purpose

[11:19] <jnthn> We run bytecode on first call to it

[11:19] <jnthn> uh

[11:19] <jnthn> *validate bytecode

[11:19] <jnthn> It's orthogonal to lazy deserialization

[11:19] <timotimo> OK, so there's also no eager bytecode validation

[11:19] <jnthn> Yes, it's...interesting...that it's wrong at the start

[11:19] <jnthn> timotimo: no, never has been

[11:20] <jnthn> robertle: Are the tests that if fails ones involving threads?

[11:20] <robertle> I did try some other SPESH environment variables to make it happen without any effect...

[11:20] <robertle> jnthn: I do believe so, lt me check.

[11:21] <robertle> not all, but there may be some noise in there. but the mayority looks like the last comment in the ticket, so definitely threads

[11:23] *** lizmat joined
[11:24] <jnthn> m: say 1039.base(16)

[11:24] <camelia> rakudo-moar ceeb3a00d: OUTPUT: «40F␤»

[11:24] <jnthn> m: say 40704.base(16)

[11:24] <camelia> rakudo-moar ceeb3a00d: OUTPUT: «9F00␤»

[11:25] <jnthn> m: say 0x9F

[11:25] <camelia> rakudo-moar ceeb3a00d: OUTPUT: «159␤»

[11:36] * dogbert2_ clickbaits https://github.com/rakudo/rakudo/issues/1709

[11:38] *** domidumont joined
[11:51] <nwc10> robertle:

[11:51] <nwc10> #define MSG(val, msg) "Bytecode validation error at offset %" PRIu32 \

[11:51] <nwc10> ", instruction %" PRIu32 ":\n" msg, \

[11:51] <nwc10> (MVMuint32)((val)->cur_op - (val)->bc_start), (val)->cur_instr

[11:51] <nwc10> that's missing a cast on the second argument

[11:52] <nwc10> robertle: the machine you have is 64 bit big endian?

[11:52] <nwc10> er, technically third argument - the argument to the second formatting string

[11:52] <nwc10> but that doesn't easily explain why it's two zeros. I would have expected 2 missing casts, to give 2 zeros

[11:53] <nwc10> but what does the macro PRIu32 end up as on that machine?

[11:56] <ilmari> nwc10: cur_instr is an MVMuint32, so doesn't need a cast, does it?

[11:57] *** lizmat joined
[11:57] <ilmari> and why doesn't that use PRiu8 instead of casting?

[11:58] *** scovit joined
[12:02] <robertle> some of them are 64bit yes, so would fixing this cast help debug this because we get better messages?

[12:03] <ilmari> cur_op and bc_start are both MVMuint8, and cur_instr is MVMuint32

[12:04] <nwc10> ilmari: IIRC the rules of varargs in C is that everything integer is promoted to a long (and everything float is promoted to a double)

[12:04] <nwc10> (in C++ char is not promoted)

[12:04] <nwc10> hence if the format string says "32 bit" and the value is 32 bit, the promotion bites

[12:05] <nwc10> but if you're little endian, you usually don't notice

[12:05] <ilmari> ah

[12:05] <nwc10> note "IIRC"

[12:05] <evalable6> nwc10, rakudo-moar ceeb3a00d: OUTPUT: «IIRC␤»

[12:05] <nwc10> ooh

[12:05] <nwc10> note "Lunch"

[12:05] <evalable6> nwc10, rakudo-moar ceeb3a00d: OUTPUT: «Lunch␤»

[12:05] <nwc10> robertle: I don't know for sure about "get better messages" but I think that fixing it (locally) and trying again might help

[12:06] <nwc10> but if so, I'm a bit unsure as to why, as it ought only to fix the second 0 you see output

[12:06] <ilmari> so changing the format string to PRIu64 might help

[12:06] <lizmat> say so True

[12:07] <robertle> ok, will do

[12:08] <lizmat> hmmm....

[12:08] <lizmat> evalable6: say so True

[12:08] <evalable6> lizmat, rakudo-moar ceeb3a00d: OUTPUT: «True␤»

[12:09] <AlexDaniel> lizmat: it looks a bit too much like normal text, so it doesn't eval it

[12:09] <AlexDaniel> say so True;

[12:09] <AlexDaniel> say so True; # foo

[12:09] <evalable6> AlexDaniel, rakudo-moar ceeb3a00d: OUTPUT: «True␤»

[12:09] <AlexDaniel> maybe it should

[12:12] <lizmat> I sorta assumed it would try to EVAL and if that worked, show the result

[12:27] *** lizmat joined
[14:00] *** zakharyas joined
[14:08] <dogbert2_> hmm, SEGV, 0xb7a9e14b in check_lex (tc=0xa81bb68, f=0xaedc4418, idx=2) at src/core/interp.c:29.  line 29   ? f->spesh_cand->lexical_types[idx]

[14:18] <jnthn> hm, ain't check_lex the thing turned on by MVM_GC_DEBUG=2 ?

[14:33] <dogbert2_> interesting, I had indeed turned on MVM_GC_DEBUG=2

[14:34] <dogbert2_> perhaps that was a mistake in this particular case

[14:46] *** Kaiepi joined
[15:15] *** zakharyas joined
[16:14] *** domidumont joined
[17:38] *** brrt joined
[17:47] <brrt> \o

[18:22] *** AlexDaniel joined
[18:59] <timotimo> i wonder if we could do better with the unicode property values hashes that we generate in MVM_unicode_init; it looks like it allocates about 230 kilobytes

[19:00] <timotimo> that's spread over 4.1k individual "blocks" (according to valgrind; not sure if each individual allocation call contributes one "block")

[19:05] *** FROGGS joined
[19:23] *** zakharyas joined
[20:08] <timotimo> jnthn: is there a ticket associated with the profiler crash you had the other day with EVAL in a loop?

[20:12] <jnthn> timotimo: I didn't create one, and I guess you'd know if there was an existing one that it falls under :)

[20:12] <timotimo> um

[20:12] * timotimo shuffles through some heaps of paper on his desk

[20:19] *** lizmat joined
[20:34] <timotimo> somehow near the end of this heap snapshot, the indices get off the rails, or it forgets to write a marker in the right spot

[20:34] <timotimo> the index does point at a position where the nature of data changes visibly in the hexdump

[20:38] *** committable6 joined
[20:38] *** evalable6 joined
[20:38] *** quotable6 joined
[20:38] *** nativecallable6 joined
[20:38] *** notable6 joined
[20:38] *** bloatable6 joined
[20:38] *** coverable6 joined
[20:38] *** unicodable6 joined
[20:38] *** greppable6 joined
[20:38] *** bisectable6 joined
[20:38] *** reportable6 joined
[20:38] *** benchable6 joined
[20:38] *** shareable6 joined
[20:38] *** statisfiable6 joined
[20:38] *** squashable6 joined
[20:39] *** undersightable6 joined
[20:39] *** releasable6 joined
[20:47] *** Kaiepi joined
[20:55] <timotimo> looks like R#1713 is leaking frames from TPS, Promise, HyperPipeline, Lock, Supply, and Channel

[20:55] <synopsebot> R#1713 [open]: https://github.com/rakudo/rakudo/issues/1713 [leak] Easily reproducible memory leak

[20:58] <timotimo> oh, huh

[20:58] <timotimo> here's a ThreadPoolScheduler::Queue (Object) that has a gazillion Block objects in it

[20:59] <timotimo> 86230 through 90704, though i didn't check to see if every number in between was in there

[21:01] <timotimo> so, just lack of a backpressure method?

[21:17] <jnthn> hyper pipelines do have a limit on number of items in flight

[21:17] <timotimo> that's interesting

[21:17] <jnthn> Basically, 2 * degree * batch size

[21:17] <jnthn> Is the upper limit

[21:18] <jnthn> Though for things actually being processed somewhere that factor of 2 goes away

[21:21] <jnthn> The code in question is doing .batch(1), however

[21:21] <jnthn> So that can't be to blame

[21:21] <jnthn> Which raises the question, where the heck are all the things in the queue coming from?

[21:21] <timotimo> does .List actually eagerly evaluate?

[21:22] <jnthn> ah, hmmmm! :)

[21:22] <jnthn> No

[21:22] <jnthn> Does it stop leaking if writing .eager?

[21:22] <timotimo> it's still running ...

[21:22] <timotimo> 8.5k Promise and Lock::ConditionVariable objects alive in the last snapshot

[21:24] <jnthn> Hmm, that seems not right

[21:28] <timotimo> memory use seems to be rising still

[21:37] <timotimo> the rise isn't as clear-cut, it seems to be rather noisy

[21:37] <timotimo> perhaps it depends on how the scheduler decides to spawn new workers or something

[21:38] <timotimo> i had 151k for 2k times, 150k for 3.5k times, 128k for 4k times, 150k for 5k times

[21:38] <timotimo> (that should be k²)

[21:52] <timotimo> haha

[21:52] <timotimo> perl6-m -e 'for ^7500 -> $go { (^100).race(batch=>1).map({ say "$go at $_" if $_ %% 20;  $_ }).List }'

[21:52] <timotimo> perl6-m -e 'for ^7500 -> $go { (^100).race(batch=>1).map({ say "$go at $_" if $_ %% 20;  $_ }).eager.elems }'

[21:53] <timotimo> guess how their output differs

[21:53] <jnthn> Guessing the second works out rather better?

[21:55] <jnthn> As in, displays $go in ascending order?

[21:58] <timotimo> yup

[21:58] <timotimo> the first one displays a looooong parade of 0s after "at" :)

[21:58] <timotimo> so, my thought process goes like this:

[21:58] <timotimo> we create about 10 thousand iterators that could at any point racemap through these lists

[21:58] <timotimo> but nobody ever asks for the list to be iterated over

[21:59] <timotimo> so they just hang out, chilling in preparation for real work, in the thread pool scheduler's queue

[21:59] <timotimo> but work never comes

[22:20] <jnthn> In theory, it was meant to work such that work had to be asked for, and everything would go away if the iterator also did

[22:20] <jnthn> But yeah, sounds like that ain't happening

[22:28] <timotimo> oh, do we have a DESTROY in play somewhere?

[22:37] <timotimo> i'd expect something like that would have to be in HyperToIterator and RaceToIterator or something?

[22:42] <MasterDuke> oh cool, looks like you've made some good findings

[22:47] <timotimo> this is at least 25% user error :P

[22:56] <MasterDuke> heh, i'm not even sure where the code example came from

[23:03] <dogbert17> it came from an RT :)

[23:04] <timotimo> was the RT also about excessive memory usage?

[23:06] <dogbert17> no, it was when I and AlexDaniel tried to wrote a test for the original problem that the memory 'problem' was noticed

[23:06] <timotimo> you know ... the debugserver could be used to build a grammar debugger that doesn't need the grammar to be recompiled

[23:06] <timotimo> i.e. it could be used on the rakudo grammar, too

[23:06] <timotimo> OK

[23:06] <timotimo> i'd say that the situation that appears in the ticket itself shouldn't happen in real world code, as it doesn't actually do the work you're hoping it'd do

[23:07] <timotimo> so it should be easy to spot that it's not doing what you want, and fixing it to actually consume the list will free memory appropriately

[23:07] <dogbert17> ah, RT #127974

[23:07] <synopsebot> RT#127974 [open]: https://rt.perl.org/Ticket/Display.html?id=127974 [CONC] sprintf() not threadsafe/reentrant if the format tokens use explicit indices

[23:09] <timotimo> wow, this is old

[23:10] <timotimo> oh, but it was already fixed

[23:10] <timotimo> so that's nice

[23:10] <dogbert17> sounds good, perhaps the issue should be closed ... unless AlexDaniel is of a different opinion

[23:10] <dogbert17> yeah, it was the when trying to write a test for the RT that the 'problem' cropped up

[23:11] <dogbert17> s/the when/when/

[23:13] <dogbert17> MasterDuke: OT next gen Ryzen processors will be released on Thursday

[23:14] <dogbert17> slightly less OT, https://blogs.oracle.com/developers/announcing-graalvm

[23:19] <timotimo> huh, wasn't graalvm announced already a year or two ago?

[23:25] <timotimo> ok, so, actually: building a grammar debugger with the debug remote feature isn't possible. but building a Grammar::Tracer replacement should be

[23:38] <MasterDuke> dogbert17: yeah, they're so very tempting. but ram prices are still kind of crazy

[23:43] <MasterDuke> timotimo: is it possible to make it so the memory doesn't grow like that even if you aren't doing any work?

[23:44] *** eater joined
[23:48] <timotimo> i imagine so

[23:51] <timotimo> trying a naive patch

[23:54] <timotimo> it was not enough

[23:56] <timotimo> damn, one of the workers in HyperPipeline actually holds on to the HyperToIterator object

[23:56] <timotimo> until we have weak references, we can't get this done automatically, i don't think

[23:58] <timotimo> how come, though, that it doesn't get sunk?

[23:59] <timotimo> ok, being the last statement in the for loop prevents it from being sunk. do we want that, though?
