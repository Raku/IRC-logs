[01:40] *** benchable6 joined
[01:57] *** ilbot3 joined
[02:49] *** greppable6 joined
[03:44] <Geth> ¦ MoarVM/grisu: af2eb8a7f7 | (Zoffix Znet)++ | src/math/grisu.c

[03:44] <Geth> ¦ MoarVM/grisu: More post-Grisu3 Num renderer polishing

[03:44] <Geth> ¦ MoarVM/grisu:

[03:44] <Geth> ¦ MoarVM/grisu: - Fix a couple of rendering issues introed in

[03:44] <Geth> ¦ MoarVM/grisu:     earlier mod[^1]

[03:44] <Geth> ¦ MoarVM/grisu: - Add more cases of handling decimal positions

[03:44] <Geth> ¦ MoarVM/grisu:     like we had before Grisu3 stuff.

[03:44] <Geth> ¦ MoarVM/grisu:

[03:44] <Geth> ¦ MoarVM/grisu: [1] https://github.com/MoarVM/MoarVM/commit/8841c4241b

[03:44] <Geth> ¦ MoarVM/grisu: review: https://github.com/MoarVM/MoarVM/commit/af2eb8a7f7

[04:06] <Geth> ¦ MoarVM: zoffixznet++ created pull request #823: Stringify Num using Grisu3 algo / Generalize mp_get_double() routine

[04:06] <Geth> ¦ MoarVM: review: https://github.com/MoarVM/MoarVM/pull/823

[04:06] <Geth> ¦ MoarVM/master: 5 commits pushed by (Zoffix Znet)++

[04:06] <Geth> ¦ MoarVM/master: 067c059410 | Stringify Num using Grisu3 algo

[04:06] <Geth> ¦ MoarVM/master: 8841c4241b | Mod post-Grisu3 Num renderer…

[04:06] <Geth> ¦ MoarVM/master: 49b7389058 | Generalize mp_get_double()

[04:06] <Geth> ¦ MoarVM/master: af2eb8a7f7 | More post-Grisu3 Num renderer polishing

[04:06] <Geth> ¦ MoarVM/master: b8d70829f6 | Merge pull request #823 from MoarVM/grisu

[04:06] <Geth> ¦ MoarVM/master: review: https://github.com/MoarVM/MoarVM/compare/b5b69681f62b...b8d70829f6f4

[05:09] <Geth> ¦ MoarVM: Kaiepi++ created pull request #824: Allow NativeCall support for wchar_t

[05:09] <Geth> ¦ MoarVM: review: https://github.com/MoarVM/MoarVM/pull/824

[05:32] *** SourceBaby joined
[05:35] *** SourceBaby joined
[05:41] *** SourceBaby joined
[05:42] *** SourceBaby joined
[05:44] *** SourceBaby joined
[06:43] <samcv> so I've decided after much thinking that while i'll retain the shiftjis name in the shiftjis decode/encode functions, the name of the encoding is going to be referenced as windows

[06:44] <samcv> windows-932 since that's the official name of the extensions that it uses, and make sure we don't encounter issues if or when i add support for the baseline shiftjis standard

[06:44] <samcv> though maybe not the most common name it's know by, it's accurate

[07:00] <Geth> ¦ MoarVM: 4d3fc2818d | (Zoffix Znet)++ | src/math/bigintops.c

[07:00] <Geth> ¦ MoarVM: Fix handling of denormals in nqp::div_In

[07:00] <Geth> ¦ MoarVM:

[07:00] <Geth> ¦ MoarVM: Makes  3.1x faster: nqp::div_In with 2**1020 Ints

[07:00] <Geth> ¦ MoarVM:       19.3x faster: nqp::div_In with 2**1020100020 Ints

[07:00] <Geth> ¦ MoarVM: Fixes RT#130155: https://rt.perl.org/Ticket/Display.html?id=130155

[07:01] <Geth> ¦ MoarVM: Fixes RT#130154: https://rt.perl.org/Ticket/Display.html?id=130154

[07:01] <Geth> ¦ MoarVM: Fixes RT#130153: https://rt.perl.org/Ticket/Display.html?id=130153

[07:01] <Geth> ¦ MoarVM: <…commit message has 19 more lines…>

[07:01] <synopsebot> RT#130155 [new]: https://rt.perl.org/Ticket/Display.html?id=130155 [BUG] Rat operations give bogus underflow

[07:01] <Geth> ¦ MoarVM: review: https://github.com/MoarVM/MoarVM/commit/4d3fc2818d

[07:01] <synopsebot> RT#130154 [new]: https://rt.perl.org/Ticket/Display.html?id=130154 [BUG] Int/Int gives bogus underflow

[07:01] <synopsebot> RT#130153 [new]: https://rt.perl.org/Ticket/Display.html?id=130153 [9999][BUG] Int**Int yields bogus overflow

[07:01] *** evalable6 joined
[07:08] <Geth> ¦ MoarVM: a5ed7ea5ed | (Zoffix Znet)++ | src/math/bigintops.c

[07:08] <Geth> ¦ MoarVM: Tweak naming of double mantissa size define

[07:08] <Geth> ¦ MoarVM:

[07:08] <Geth> ¦ MoarVM: Tis teh bits; ain't digits

[07:08] <Geth> ¦ MoarVM: review: https://github.com/MoarVM/MoarVM/commit/a5ed7ea5ed

[07:31] <Geth> ¦ MoarVM/master: 5 commits pushed by (Samantha McVey)++

[07:31] <Geth> ¦ MoarVM/master: e8fdb35e5f | Add shiftjis decode and decodestream support

[07:31] <Geth> ¦ MoarVM/master: d44d7d1741 | Ensure a failed JIS result is distinct from 0

[07:31] <Geth> ¦ MoarVM/master: 62cfb2f765 | Make sure to throw in shiftjis decode if byte after lead

[07:31] <Geth> ¦ MoarVM/master: edfde24b06 | Use windows-932 instead of shiftjis for enc name

[07:31] <Geth> ¦ MoarVM/master: d322741cea | Merge pull request #820 from samcv/shiftjis

[07:31] <Geth> ¦ MoarVM/master: review: https://github.com/MoarVM/MoarVM/compare/a5ed7ea5edaf...d322741ceaf7

[08:06] *** domidumont joined
[08:12] *** domidumont joined
[08:32] *** lizmat joined
[09:48] *** evalable6 joined
[10:18] <MasterDuke> timotimo: here's another --profile segv. `$ = (1,2,3,4,5).max for ^100_000`

[10:32] <MasterDuke> backtrace (minux ~87k lines) https://gist.github.com/MasterDuke17/a0cede29b793d46ceb580a445d90da23

[10:34] *** Ven`` joined
[10:57] *** Ven`` joined
[11:29] <timotimo> 87k lines? holy crap. maybe that's actually a stack overflow?

[11:30] <MasterDuke> it was just repeats of `#87351 0x00007ffff76c955c in dump_call_graph_node (tc=tc@entry=0x555555758c60, pds=pds@entry=0x7fffffffd540, pcn=0x5555557bb610) at src/profiler/instrument.c:420`

[11:30] <timotimo> yeah

[11:30] <timotimo> not the same addresses though, right?

[11:31] <MasterDuke> correct, different addresses for pcn

[11:33] <timotimo> i have no explanation for this deep of a call graph yet

[11:36] <MasterDuke> runs just fine with MVM_SPESH_INLINE_DISABLE=1

[11:38] <timotimo> i wonder if my recent inline fix was just bogus?

[11:41] <MasterDuke> you definitely fixed something, right? maybe there's just more to fix that was uncovered by that change

[11:41] <timotimo> the fix got in right *after* the release, though, right?

[11:43] <MasterDuke> i think. fwiw, i'm at HEAD

[11:52] <timotimo> ok so at least it's actually a stack overflow, not some other kind of crash

[11:59] <timotimo> so, the good news is, i should probably be able to reduce the stack size of dump_call_graph_node a little

[12:00] <MasterDuke> so we get more iterations in before it overflows?

[12:02] <timotimo> yeah

[12:02] <timotimo> it should definitely not grow that big, though

[12:05] <timotimo> actually, perhaps i can't do better than 144 bytes

[12:06] <timotimo> wait, did i just confuse bits and bytes again %)

[12:19] <timotimo> OK, the one 144 frame is now two frames, the one that'll remain on the stack more often is 80, the other one is 128

[12:19] <timotimo> not quite as good as i had hoped, maybe i can do something about it yet.

[12:21] <MasterDuke> nice

[12:23] <timotimo> cool, 64 and 128 now

[12:24] <MasterDuke> nicer

[12:26] <timotimo> it now reaches the "write profile file" stage

[12:27] *** benchable6 joined
[12:28] <timotimo> just 160 megs!

[12:28] <MasterDuke> does it actually write it out?

[12:28] <timotimo> yup

[12:29] <MasterDuke> good deal

[12:31] <Geth> ¦ MoarVM/profile_dump_less_stack_usage: 90b05c81b9 | (Timo Paulssen)++ | src/profiler/instrument.c

[12:31] <Geth> ¦ MoarVM/profile_dump_less_stack_usage: profile: extract recursion loop for smaller stack frames

[12:31] <Geth> ¦ MoarVM/profile_dump_less_stack_usage:

[12:31] <Geth> ¦ MoarVM/profile_dump_less_stack_usage: dumping call graphs used to put 144 bytes onto the stack for

[12:31] <Geth> ¦ MoarVM/profile_dump_less_stack_usage: every slice of recursion, now it'll deposit just 64 bytes for

[12:31] <Geth> ¦ MoarVM/profile_dump_less_stack_usage: every slice and put a 128 byte frame on top to do most of the

[12:31] <Geth> ¦ MoarVM/profile_dump_less_stack_usage: actual work.

[12:31] <Geth> ¦ MoarVM/profile_dump_less_stack_usage: review: https://github.com/MoarVM/MoarVM/commit/90b05c81b9

[12:32] <timotimo> MasterDuke: could you have a look at what the stack actually looks like? i'll try to enjoy the last bit of sun on the balcony

[12:33] <MasterDuke> what do you mean?

[12:33] <MasterDuke> with your branch?

[12:36] <MasterDuke> timotimo: gist updated with output of `info frame` at the segv

[12:52] <timotimo> oh, i meant the profiled call stack

[12:57] <timotimo> so we have almost 100k of each pull-one, infix:<cmp>, infix:«>», return, max, iterator-and-first, is-lazy, iterator, ReifiedList, one anonymous routine, new, and SET-SELF, the last 4 out of Rakudo/Iterator

[12:57] <timotimo> and almost 100k of the -e frame

[12:58] <timotimo> it seems to think that the -e frame is calling itself over and over

[13:10] <MasterDuke> huh. think it's a problem in how the profiler is recording the data? or in how spesh is doing inlining?

[13:11] <timotimo> not sure yet

[14:02] *** AlexDaniel joined
[14:22] *** greppable6 joined
[15:00] *** robertle joined
[15:58] <timotimo> what was the second-to-last profile example that segfaulted? :|

[15:58] *** committable6 joined
[15:59] <dogbert11> second to last?

[15:59] <dogbert11> from me or MasterDuke?

[15:59] <timotimo> i'm not entirely sure :|

[16:01] <dogbert11> perhaps this https://gist.github.com/dogbert17/750ffbf9fe7022ef091e8172b70779d8

[16:03] <timotimo> let's see

[16:04] <dogbert11> do you have a new theory?

[16:04] <timotimo> that used to segfault?

[16:04] <dogbert11> yes

[16:04] <timotimo> doesn't any more :)

[16:04] <dogbert11> timotimo++

[16:05] <dogbert11> it worked if you turned off inlining

[16:05] <timotimo> same as the one today, then

[16:06] <dogbert11> perhaps your fix works there as well

[16:06] <timotimo> it did

[16:06] <dogbert11> cooool

[16:06] <timotimo> but it just makes it non-explosive

[16:06] <timotimo> the call graph being so huge is still bogus

[16:06] <dogbert11> aha, one mystery left then

[16:07] <dogbert11> I have one small script where the profile get 150 megs

[16:08] <timotimo> that's likely the same underlying issue

[16:09] *** benchable6 joined
[16:10] <dogbert11> if it helps, I can tune a script so that the profile, albeit buggy, is quite small

[16:11] <timotimo> 93.2% (5054407876196421ms)

[16:11] <timotimo> *sigh*

[16:11] <timotimo> Infinity% (14.95ms)

[16:12] <timotimo> oops, did i say it doesn't crash any more

[16:12] <timotimo> it just doesn't crash reliably

[16:22] <dogbert11> FWIW if I run the profile under valgrind it does not SEGV

[16:24] <dogbert11> and the generated profile doesn't look bugus

[16:27] <timotimo> the call graph is suspiciously deep

[16:28] <dogbert11> my profilare you referring to the long spike on that page

[16:29] <dogbert11> wanted to write; are you referring to the long spike on that page

[16:29] <timotimo> yeah

[16:29] <timotimo> got something to show you

[16:29] <timotimo> once i figure out how to work this GUI here

[16:30] <dogbert11> clicking it gives 'push-at-least SETTING::src/core/Iterator.pm6:49 '

[16:30] <timotimo> https://i.imgur.com/htmC15q.png

[16:30] <timotimo> see how the structure is suspiciously similar?

[16:31] <dogbert11> yes

[16:32] <timotimo> i think we're accidentally forgetting to handle a prof_exit and thus recursing too deep

[16:32] <dogbert11> the allocations page also looks a bit strange

[16:34] <dogbert11> there are many lines mentioning the same thing, e.g. Rakudo::Iterator::CountOnlyBoolOnlyDelegate is mentioned 40-50 times

[16:34] <timotimo> that does seem wrong, yeah

[16:34] <dogbert11> instead of being aggregated

[16:36] <dogbert11> same with '<anon|23>+{Rakudo::Iterator::CountOnlyBoolOnlyDelegate[<anon|33>]}' and 'Rakudo::Iterator::CountOnlyBoolOnlyDelegate[<anon|33>]'

[16:42] <timotimo> it's small enough that i can do it without --minimal, i.e. get the function names in the graph, too

[16:43] <dogbert11> does that give you any new clues?

[16:43] <timotimo> let's see

[17:31] <timotimo> hm, so, the implementation of List:D's ACCEPTS is deeply recursive

[17:32] <timotimo> this list's accepts is called from junction's ACCEPTS

[17:33] <timotimo> let's just say that at some point this code would have a stack trace that'd have a hundred or so ACCEPTS in a row

[17:38] <timotimo> just replacing the ~~ there with eq makes the profile really tiny

[17:39] <timotimo> i think we see that one type so many times because we actually create many types of that name

[17:41] <dogbert11> shouldn't they be aggregated?

[17:42] <timotimo> not if they are different actual types

[17:44] <MasterDuke> so does anything very recursive blow up profiles?

[17:47] *** FROGGS_ joined
[17:47] <timotimo> yep

[17:50] <timotimo> thing is, we create one role for every time we mix in the CountOnlyBoolOnlyDelegate

[17:50] <timotimo> which is a role that just forwards a call to bool-only and a call to count-only to another iter object's method of the same name

[17:50] <timotimo> i'd think it'd be better to have an attribute mixed in by the role that stores this delegate target

[17:51] <timotimo> i'm not sure why the name shows up as anon|23 and anon|33, like, how do these numbers get so short?

[17:54] <MasterDuke> that would just help/fix that particular bit of code, right?

[17:55] <timotimo> i expect this causes a bit of performance degradation in everything that uses this particular piece of rakudo iterator tech

[17:55] <MasterDuke> i'm kind of impressed, `sub f($n) { $n * ($n < 2 ?? $n !! f($n - 1)) }; say f(40_000)` only created a 19mb profile

[17:55] <timotimo> also, it's not the cause for the huge profile; the use of ~~ against lists/junctions is

[17:55] <MasterDuke> ah

[17:57] <dogbert11> the program actually generates a MoarVM panic if MVM_GC_DEBUG=2

[17:57] <dogbert11> message is 'MoarVM panic: Invalid assignment (maybe of heap frame to stack frame?)'

[17:57] <timotimo> yeah, i was hunting that yesterday

[17:57] <timotimo> dinner proparation time now, though

[18:11] <timotimo> No such method '!set-delegate-target' for invocant of type '<anon|23>+{Rakudo::Iterator::CountOnlyBoolOnlyDelegate}'. Did you mean '!set-delegate-target'?

[18:11] <timotimo> :|

[18:16] <timotimo> got it down

[18:16] <timotimo> one Rakudo::Iterator::CountOnlyBoolOnlyDelegate

[18:16] <timotimo> one <anon|23>+{Rakudo::Iterator::CountOnlyBoolOnlyDelegate}

[18:24] <timotimo> so ...

[18:24] <timotimo> mixin goes from 21.6ms inclusive time down to 2.77ms inclusive time

[18:24] <timotimo> with the same amount of calls to it :)

[18:24] <timotimo> notably because generate_mixin now gets called only once, rather than the 39 times that mixin itself is called

[18:25] <timotimo> same for set_is_mixin and setup_mixin_cache

[18:25] <timotimo> oh, wow

[18:25] <timotimo> from 2 gc runs down to 1

[18:27] <dogbert11> what are you up to ?

[18:28] <timotimo> i'm not sure if these two profiles were measuring the same code

[18:36] *** robertle joined
[18:52] *** Kaiepi joined
[19:15] <timotimo> i'll probably change it back a tiny bit so that it's actually parameterized on something, but only on the target iterator's type

[19:16] <timotimo> that could give us better specializations, i think

[19:17] <timotimo> though we may not be calling bool-only or count-only a million times in regular code

[19:22] *** bisectable6 joined
[20:12] <Geth> ¦ MoarVM: 67e5093f0e | (Timo Paulssen)++ | src/debug/debugserver.c

[20:12] <Geth> ¦ MoarVM: only suspend on actually must-suspend breakpoints

[20:12] <Geth> ¦ MoarVM: review: https://github.com/MoarVM/MoarVM/commit/67e5093f0e

[20:37] <FROGGS> jnthn: I found something interesting when hunting the DBDish::mysql instability

[20:37] <FROGGS> this causes a double free quite often: https://github.com/MoarVM/MoarVM/blob/master/src/core/interp.c#L5040

[20:38] <timotimo> oh, huh!

[20:39] <jnthn> Oops...but also, huh...shouldn't that only set during type creation?

[20:40] <timotimo> probably should, yeah

[20:40] <FROGGS> aye

[20:40] <timotimo> .o( put a lock on it )

[20:41] <FROGGS> these two got in:

[20:41] <FROGGS> // debugname = 0x7fffe44d7010 "NativeCall::Types::Pointer[MoarVM::Guts::REPRs::MVMArrayB]"

[20:41] <FROGGS> // debugname = 0x7fff7c080fa0 "Array[DBDish::mysql::Native::MYSQL_BIND]"

[20:42] *** lizmat joined
[20:42] <FROGGS> okay, in Pointer.^parameterize we call .^set_name, which calls that op

[20:42] <FROGGS> that's still during type creation, right?

[20:42] <timotimo> should be, yeah

[20:43] <timotimo> the only way to double-free there is to get two threads into the tiny space between MVM_free(STABLE(obj)->debug_name) and STABLE(obj)->debug_name = debugname

[20:43] <timotimo> which is possible if the encode_C_string causes GC i suppose

[20:43] <FROGGS> that's what I thought too

[20:44] <timotimo> this would be a time for helgrind if it weren't so noisy due to many false-positives :(

[20:52] <FROGGS> wow, yes, it says a lot

[20:54] <FROGGS> hmmm

[20:55] <FROGGS> MVM_string_utf8_encode_C_string cannot cause GC, right? I mean, it returns a char*

[20:55] <timotimo> oh, yes

[21:00] <jnthn> It'd seem to mean that two threads are trying to concurrently set the name

[21:06] <FROGGS> the type named Array[DBDish::mysql::Native::MYSQL_BIND] is on DBDish::mysql::Connection, which get's need'ed not use'ed...

[21:07] <FROGGS> but need was just loading at compile time but no imports?

[21:10] <FROGGS> okay, change it to "use" makes no difference, but that's no surprise

[21:10] <timotimo> did you grab some tracebacks?

[21:11] <FROGGS> a backtrace from gdb?

[21:11] <timotimo> call MVM_dump_backtrace(tc)

[21:11] <timotimo> ^- literally type that into gdb

[21:11] <FROGGS> right

[21:14] <FROGGS> (gdb) call MVM_dump_backtrace(tc)

[21:14] <FROGGS> Invalid cast.

[21:14] <dogbert11> is call the same as p ?

[21:14] <timotimo> just to be sure, tc is available in your currently selected frame?

[21:14] <timotimo> no, p is call + print

[21:15] <FROGGS> (gdb) p tc

[21:15] <FROGGS> $1 = 1.4616321449683623412809166386416848

[21:15] <timotimo> lolwat

[21:15] <timotimo> tc is supposed to be a pointer to a MVMThreadContext :)

[21:15] <timotimo> of course we don't have full control over everybody's code

[21:15] <timotimo> so maybe you're inside mysql's code or something?

[21:16] <FROGGS> (gdb) p MVM_dump_backtrace

[21:16] <FROGGS> $2 = {void (MVMThreadContext *)} 0x7ffff76ac870 <MVM_dump_backtrace>

[21:16] <FROGGS> so, that's in place at least

[21:16] <lizmat> timotimo: re https://github.com/rakudo/rakudo/commit/9f8231c62d89bd71f3bd3ec95c363033c4016a3c#diff-3ae36c14e5051422d15d3c03a841f0bfR1430

[21:16] <lizmat> that being a private method, is that correct?

[21:17] <timotimo> oops, it is not correct!

[21:17] <FROGGS> MVM_interp_run (tc=0x5cf6, tc@entry=0x7fffdc0f3290, initial_invoke=0x0, invoke_data=0x6, invoke_data@entry=0x7fffdc13c4b0) at src/core/interp.c:5040

[21:17] <lizmat> also: https://github.com/rakudo/rakudo/commit/9f8231c62d89bd71f3bd3ec95c363033c4016a3c#diff-3ae36c14e5051422d15d3c03a841f0bfR1436

[21:17] <FROGGS> which tc shall I use?

[21:17] <lizmat> ok,

[21:17] <lizmat> timotimo: will fix it  :-)

[21:17] <timotimo> too late

[21:18] <timotimo> this does probably mean that this is untested by the spec test suite

[21:18] <timotimo> otherwise i wouldn't have gotten a PASS

[21:18] <lizmat> yeah  :-)_

[21:18] <timotimo> lizmat++

[21:19] <timotimo> i wish we could get the supervisor process to not allocate anything on the heap :)

[21:19] <FROGGS> timotimo: look https://gist.githubusercontent.com/FROGGS/7b10f839eae72a5b0821aa389f3434f1/raw/2fecce52c52d9414b3fffbfaa23db221147f2b3d/gistfile1.txt

[21:20] <timotimo> oh, huh. i wonder if NativeHelpers needs to have its guts updated perhaps?

[21:20] <timotimo> it does kind of do terrible things with internal structs if i remember correctly

[21:21] <timotimo> the code start { }; sleep 60 will run the GC 8 times

[21:21] <timotimo> ah, getrusage-total allocates, of course

[21:22] <lizmat> timotimo: perhaps we shouldn't use a sub for that ?

[21:23] <timotimo> no, it's the nqp::getrusage op

[21:23] <lizmat> ah, ok

[21:23] <timotimo> the profiler didn't add allocation logging to that op yet; it'll show up in the next profile i'll take

[21:24] <FROGGS> okay, here we create types at runtime: https://github.com/salortiz/NativeHelpers-Blob/blob/master/lib/NativeHelpers/Blob.pm6#L27

[21:26] <timotimo> hm. we don't actually have anything to log the allocation of nested objects. like getrusage will create a hash with a bunch of numbers in it, but we'll only count the hash

[21:27] <timotimo> ah, not a hash, a BOOTIntArray

[21:27] <timotimo> that's a lot better than i thought

[21:27] <timotimo> 5.8k objects in the 60 seconds i slept

[21:28] <timotimo> in theory we could change getrusage to write to an int array you pass it, so we could re-use that …

[21:28] <lizmat> that sounds like an excellent plan

[21:28] <lizmat> this would sit well with race conditions I guess

[21:29] <timotimo> how do you mean?

[21:29] <lizmat> ah, no, somehow I was thinking it used one buffer internally (probably P5 thinking)

[21:29] <lizmat> the whole issue was that it created a new one each time, right ?

[21:30] <timotimo> oh, no, we'd keep the array in our "user" code

[21:30] <timotimo> a perl6-level getrusage sub would allocate the array and fill it immediately

[21:30] <lizmat> and giving it a list_i puts the responsibility in the hands of the dev

[21:30] <timotimo> but the TPS could re-use the same object over and over

[21:30] <lizmat> right

[21:30] <timotimo> FWIW, there are other objects we allocate a whole lot more of

[21:30] <lizmat> such as ?

[21:31] <timotimo> Num, Scalar, BOOTCode, NumLexRef, BOOTHash, IntLexRef, IntAttrRef are all above 10k

[21:31] <timotimo> Num at 88k

[21:31] <lizmat> hmmm... we shouldn't have any Nums  :-(

[21:31] <timotimo> you think?

[21:32] <lizmat> aaaahhhh  lemme see

[21:32] <timotimo> we do have a bunch of native num attributes, those shouldn't be making Num objects, but that's simply a case of "we box just to immediately unbox" i bet

[21:33] <lizmat> there *is* a infix:<*> involved

[21:34] <timotimo> well, from the allocation numbers i see 41k from Int.pm6's Bridge method, 17.6k from Num.pm6's infix:</>, 17.5k from Real.pm6's infix:</> and *then* Num.pm6's infix:<*>

[21:35] <timotimo> a big portion of Scalar allocations seem to come from iterating over a Range, it seems like

[21:36] <timotimo> i.e. prefix:<^>, iterator from Range.pm6, line 1703 from Rakudo/Iterator.pm6, SET-SELF from 5 lines below that, 11.7k Scalars in Range.pm6's SET-SELF, and 17.6k from Range.pm6's new

[21:36] <timotimo> i'll be AFK for a bit

[21:38] <timotimo> anyway, the range in question is most probably from prod-affinity-workers

[21:39] <timotimo> sounds like an easy win to me

[21:40] * lizmat looks

[21:43] <timotimo> perl6 --profile -e 'start { }; sleep 60'  and then look at the profiler's allocations tab

[21:59] <lizmat> timo: I think I got a good idea what's going on there

[21:59] <lizmat> timotimo: but am too tired now to tinker with this right now

[21:59] <lizmat> (another 6 hours in the car today)

[22:00] <lizmat> so, will look at it tomorrow

[22:09] <timotimo> OK, maybe i'll just do it right now :)

[22:10] <timotimo> you rubberducked good, though :)

[22:20] <timotimo> just by replacing the for ^worker-list.items with a loop ( ) loop i got it down to 6 gc runs in the same time it did 8 before

[22:21] <timotimo> i might need to run 2 minutes to get more precise measurements

[22:21] <MasterDuke> is it faster?

[22:22] <timotimo> it runs pretty much exactly 60 seconds :P

[22:23] <MasterDuke> heh

[22:23] <timotimo> i should have run it with "time" to get proper cpu time measurements

[22:23] <timotimo> The profiled code ran for 60005.54ms. Of this, 29.98ms were spent on garbage collection (that's 0.05%).

[22:23] <timotimo> The profiled code ran for 60005.6ms. Of this, 28.19ms were spent on garbage collection (that's 0.05%).

[22:24] <timotimo> that's before -> after, so somehow it got the tinyest bit slower. which i'll just call "noise" :)

[22:24] <timotimo> now i'm down to 5 collections

[22:26] <timotimo> 124238 (60.21%)

[22:26] <timotimo> 56587  (61.84%)

[22:26] <timotimo> 50127  (59.79%)

[22:27] <timotimo> interesting development (this is call frames in total and percentage eliminated via inlining)

[22:30] <timotimo> aha, i see why getrusage-total isn't being jitted. it's mostly being entered inlined via a frame that also has nqp::cpucores, which isn't jitted

[22:32] <Geth> ¦ MoarVM: b1f64db89b | (Zoffix Znet)++ | src/core/coerce.c

[22:32] <Geth> ¦ MoarVM: Add missing include for Grisu3 dtoa function

[22:32] <Geth> ¦ MoarVM:

[22:32] <Geth> ¦ MoarVM: Fixes https://github.com/MoarVM/MoarVM/issues/825 M#825

[22:32] <Geth> ¦ MoarVM: review: https://github.com/MoarVM/MoarVM/commit/b1f64db89b

[22:32] <synopsebot> M#825 [open]: https://github.com/MoarVM/MoarVM/issues/825 implicit function declaration compiler warning for function ‘dtoa_grisu3’

[22:34] <timotimo> hah, now it's bailing on lseep

[22:34] <timotimo> sleep*

[22:42] <timotimo> lovely!

[22:43] <timotimo> jit-compiled frames: 96.93% (120908)

[22:50] *** greppable6 joined
[22:50] <timotimo> cool, got rid of some NumLexRefs

[22:51] <timotimo> and Num on top of that

[22:53] <timotimo> both prod-affinity-workers and .sum allocate a hash for named arguments even though they don't get passed any; wonder why that happens

[22:55] <timotimo> probably deopt annoyingness

[22:57] <timotimo> huh, prod-affinity-workers doesn't show up in the spesh log :|

[22:57] <MasterDuke> nice. i've never really figured out how to reduce allocations of things

[22:57] *** evalable6 joined
[22:58] <timotimo> oh, no, it is in there, i just misspelt it

[22:59] <timotimo> oh, wrong again

[23:02] <timotimo> funny, it speshes prod-affinity-workers, but only if profiling is turned on. so maybe the profiling overhead makes the cpu usage go up a tiny bit and makes the scheduler decide to create an additional worker?

[23:05] <MasterDuke> what if instead you start some other thread doing random work?

[23:09] <timotimo> then the impact on gc runs won't be as visible

[23:10] <timotimo> running a 120s profile now

[23:10] <timotimo> changed a few / to nqp::div_n

[23:13] <timotimo> oh, huh, 5 gc runs for 120s as well

[23:13] <timotimo> oh wow

[23:13] <timotimo> m: say [+] 53084, 17770, 17760, 13444, 6303, 5959, 5948

[23:13] <camelia> rakudo-moar 97677a792: OUTPUT: «120268␤»

[23:13] *** committable6 joined
[23:14] <timotimo> m: say [+] 24873, 12066, 11896, 11877, 11867, 11861, 11857

[23:14] <camelia> rakudo-moar 97677a792: OUTPUT: «96297␤»

[23:14] <timotimo> m: say (120268 * 2) R/ 96297

[23:14] <camelia> rakudo-moar 97677a792: OUTPUT: «0.4003434␤»

[23:14] <timotimo> so we're now only allocating 40% as many objects - though i didn't account for how big each object is

[23:15] <MasterDuke> that's a big reduction in count!

[23:15] <timotimo> aye

[23:16] <timotimo> i'll run 5 minutes now

[23:18] <timotimo> jnthn was rather not happy about the thought of making the ThreadPoolScheduler's code less readable, though

[23:19] <MasterDuke> i would think if it's been proven bug-free for a while now he'd be more amenable to optimizing it

[23:20] <MasterDuke> can always make a PR for comments

[23:21] <timotimo> OK, so 8 GC runs over 5 minutes, that's not so bad

[23:27] <timotimo> whoops, i made it no longer allocate as many BOOTCode (hardly any any more) but also made prod-affinity-workers unjittable

[23:28] <MasterDuke> "Timotimo's Choice"

[23:29] <timotimo> queuepoll's not jitted :)

[23:30] <MasterDuke> jit all the ops!!

[23:33] <timotimo> yeah why not :P

[23:38] <timotimo> vim shouldn't let me ctrl-p into the install/ folder %)

[23:38] <timotimo> actually, there shouldn't be an install folder under moarvm/ anyway

[23:40] <timotimo> got the frame jitted again, yay

[23:43] *** Kaiepi joined
[23:48] <MasterDuke> and not allocating as many BOOTCodes?

[23:49] <timotimo> 71 over the course of the whole run

[23:53] <MasterDuke> cool beans

[23:53] <timotimo> https://i.imgur.com/3upvzFE.png

[23:53] <timotimo> the tinyest difference %)

[23:56] <timotimo> huh, the bytecode at the end is actually bigger

[23:57] <timotimo> OK, the devirtualized calls are actually more arguments to put on the stack

[23:57] <timotimo> but the call itself is more direct

[23:59] <timotimo> m: say 30290 - 29954

[23:59] <camelia> rakudo-moar 97677a792: OUTPUT: «336␤»

[23:59] <timotimo> that is not much
