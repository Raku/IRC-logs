[00:00] <jnthn> oh, also

[00:00] <jnthn> I guess it's feasible that the set we produce for box/unbox elimination might be worth calling set elimination on too

[00:00] <timotimo> that's true

[00:17] <timotimo> gnite

[00:20] <jnthn> 'night

[01:21] *** Kaypie joined
[01:21] *** Kaiepi left
[02:42] *** lizmat left
[04:42] *** brrt joined
[05:05] <Geth> ¬¶ MoarVM/jit-expr-refactor: 7 commits pushed by (Bart Wiegmans)++

[05:05] <Geth> ¬¶ MoarVM/jit-expr-refactor: e78407ef65 | [JIT] Adhoc template application

[05:05] <Geth> ¬¶ MoarVM/jit-expr-refactor: 7f7b5dc3f9 | Remove op_info pointer

[05:05] <Geth> ¬¶ MoarVM/jit-expr-refactor: bf1eada668 | Document the conditional dependency check

[05:05] <Geth> ¬¶ MoarVM/jit-expr-refactor: bc2efafc3d | Move label assignment to tiler

[05:05] <Geth> ¬¶ MoarVM/jit-expr-refactor: 99f0ebe06e | Add CONST_PTR indirection

[05:05] <Geth> ¬¶ MoarVM/jit-expr-refactor: b9a15ddc69 | [JIT] Remove 'value type' enum

[05:05] <Geth> ¬¶ MoarVM/jit-expr-refactor: d0519a9d18 | [JIT] Parse oplist as a module

[05:05] <Geth> ¬¶ MoarVM/jit-expr-refactor: review: https://github.com/MoarVM/MoarVM/compare/23db4edcbcea...d0519a9d184d

[05:49] *** robertle left
[07:10] *** zakharyas joined
[07:31] *** brrt left
[07:38] *** domidumont joined
[07:43] *** domidumont left
[07:45] *** domidumont joined
[08:04] *** robertle joined
[08:17] *** lizmat joined
[08:54] *** brrt joined
[09:09] <Geth> ¬¶ MoarVM/jit-expr-refactor: 27d7d533c7 | (Bart Wiegmans)++ | src/jit/expr.c

[09:09] <Geth> ¬¶ MoarVM/jit-expr-refactor: [JIT] Assign a size to CONST_LARGE and CONST_PTR

[09:09] <Geth> ¬¶ MoarVM/jit-expr-refactor:

[09:09] <Geth> ¬¶ MoarVM/jit-expr-refactor: Otherwise if we use this in operators that require size

[09:09] <Geth> ¬¶ MoarVM/jit-expr-refactor: information (e.g. EQ) we'll try to insert a CAST, which is nonsense.

[09:09] <Geth> ¬¶ MoarVM/jit-expr-refactor:

[09:09] <Geth> ¬¶ MoarVM/jit-expr-refactor: This surfaced after rebasing on the new expression JIT templates, so, go

[09:09] <Geth> ¬¶ MoarVM/jit-expr-refactor: team.

[09:09] <Geth> ¬¶ MoarVM/jit-expr-refactor: review: https://github.com/MoarVM/MoarVM/commit/27d7d533c7

[09:09] <Geth> ¬¶ MoarVM/jit-expr-refactor: 708a36de72 | (Bart Wiegmans)++ | 2 files

[09:09] <Geth> ¬¶ MoarVM/jit-expr-refactor: [JIT] Mark tree nodes in template precompiler

[09:09] <Geth> ¬¶ MoarVM/jit-expr-refactor:

[09:09] <Geth> ¬¶ MoarVM/jit-expr-refactor: Tree nodes (n) should be distinguished from large constant

[09:09] <Geth> ¬¶ MoarVM/jit-expr-refactor: references (c), input operands (i), within-template links (l) and

[09:09] <Geth> ¬¶ MoarVM/jit-expr-refactor: constant symbols or numbers (.). So that we can handle the nodes

[09:09] <Geth> ¬¶ MoarVM/jit-expr-refactor: specially in the template application logic.

[09:09] <Geth> ¬¶ MoarVM/jit-expr-refactor: review: https://github.com/MoarVM/MoarVM/commit/708a36de72

[09:27] *** zakharyas left
[09:27] <jnthn> o/

[09:28] <nwc10> \o

[09:32] <lizmat> o/

[09:32] <brrt> \o

[09:33] <Geth> ¬¶ MoarVM: vendethiel++ created pull request #903: fix minor typos

[09:33] <Geth> ¬¶ MoarVM: review: https://github.com/MoarVM/MoarVM/pull/903

[09:34] <Geth> ¬¶ MoarVM/du-chains-and-opts: 2356999dbb | ven++ (committed using GitHub Web editor) | src/spesh/optimize.c

[09:34] <Geth> ¬¶ MoarVM/du-chains-and-opts: fix minor typos

[09:34] <Geth> ¬¶ MoarVM/du-chains-and-opts: review: https://github.com/MoarVM/MoarVM/commit/2356999dbb

[09:34] <Geth> ¬¶ MoarVM/du-chains-and-opts: 78f8402780 | (Jonathan Worthington)++ (committed using GitHub Web editor) | src/spesh/optimize.c

[09:34] <Geth> ¬¶ MoarVM/du-chains-and-opts: Merge pull request #903 from vendethiel/patch-1

[09:34] <Geth> ¬¶ MoarVM/du-chains-and-opts:

[09:34] <Geth> ¬¶ MoarVM/du-chains-and-opts: fix minor typos

[09:34] <Geth> ¬¶ MoarVM/du-chains-and-opts: review: https://github.com/MoarVM/MoarVM/commit/78f8402780

[09:41] *** domidumont left
[09:48] <Geth> ¬¶ MoarVM/du-chains-and-opts: 5f8fbec65a | (Jonathan Worthington)++ | src/spesh/usages.h

[09:48] <Geth> ¬¶ MoarVM/du-chains-and-opts: Turn off DU chain checks by default

[09:48] <Geth> ¬¶ MoarVM/du-chains-and-opts:

[09:48] <Geth> ¬¶ MoarVM/du-chains-and-opts: Now that things seem to be working nicely, we'll disable it, so once

[09:48] <Geth> ¬¶ MoarVM/du-chains-and-opts: this branch is merged users won't pay the price of the sanity checking

[09:48] <Geth> ¬¶ MoarVM/du-chains-and-opts: slowing the optimizer down.

[09:48] <Geth> ¬¶ MoarVM/du-chains-and-opts: review: https://github.com/MoarVM/MoarVM/commit/5f8fbec65a

[09:49] <jnthn> I think that branch can merge now

[09:49] <lizmat> whee!  :-)

[09:49] <brrt> \o/

[09:50] <jnthn> The DU chains themselves have been passing their sanity checks for a while.

[09:50] <jnthn> There's many more things to do, but I don't think I need to do them in a branch. :)

[09:50] <lizmat> .oO( immediate dogfooding )

[09:51] <jnthn> Stage parse      :  48.649

[09:51] <jnthn> I'm sure that's another second lower than it was

[09:52] * nwc10 hopes that dogfood isn't the lunch menu

[10:01] <Geth> ¬¶ MoarVM/master: 54 commits pushed by (Jonathan Worthington)++, (Timo Paulssen)++, (Elizabeth Mattijsen)++, ven++

[10:01] <Geth> ¬¶ MoarVM/master: review: https://github.com/MoarVM/MoarVM/compare/1b075ec1a7c1...3d6aece9b48d

[10:02] <jnthn> Well, that took us some way past the 10,000 commit mark :)

[10:15] <samcv> woo do we have a 10,000 commit party?

[10:35] *** stmuk_ joined
[10:36] *** stmuk left
[10:42] <jnthn> :)

[10:42] <lizmat> jnthn: are you going to bump ?

[10:42] <jnthn> Next little mystery: why doesn't an invoke_v ever inline or optimize...

[10:42] <lizmat> jnthn: or is it too soon ?

[10:42] <jnthn> Well, I've commits probably coming soon in both MoarVM and NQP that do some further improvements :)

[10:43] <lizmat> "probably coming soon" as in today ?

[10:43] <lizmat> :-)

[10:43] <jnthn> Yes :)

[10:43] <lizmat> cool!

[10:43] * lizmat can wait

[10:43] <lizmat> but only barely  :-)

[10:55] <jnthn> grmbl

[10:56] <jnthn> So the various non-object-return invoke ops weren't marked :logged, which in turn meant they couldn't find any logged facts about the type of the invocant

[10:57] <jnthn> We'll emit some more invoke_v after an improvement I've done in the QAST compiler

[10:57] <jnthn> Adding :logged helps very nicely.

[10:57] <jnthn> In that the calls then get inlined etc.

[10:57] <jnthn> Alas, it breaks the NQP build

[11:08] <jnthn> And the frame it claims breaks it doesn't even use any of the ops I stuck the annotations on

[11:31] <jnthn> Grr, still not seeing what's up

[11:31] <jnthn> lunch &

[11:37] <samcv> jnthn: not sure if this is safe, but doing this cut time from 5 seconds to 4.5 secs on https://gist.github.com/samcv/9a65a586ea21b20def8efe4aaaab55de#file-time-nqp

[11:37] <samcv> this is what i did https://gist.github.com/samcv/1955ad025b1822876dc642b3e1b3e00b

[11:38] <samcv> assuming worklist->include_gen2 never changes during the worklist being used

[11:40] <samcv> normally the outer if is: *item_to_add && (worklist->include_gen2 || !((*item_to_add)->flags & MVM_CF_SECOND_GEN))

[11:40] <samcv> and i split it into one that assumes true so it has one less if closure, and the other that assumes true and just checks one less variable

[11:47] <Geth> ¬¶ MoarVM/jit-expr-refactor: ca667f53b6 | (Bart Wiegmans)++ | 5 files

[11:47] <Geth> ¬¶ MoarVM/jit-expr-refactor: [JIT] Remove cast information from expr ops table

[11:47] <Geth> ¬¶ MoarVM/jit-expr-refactor:

[11:47] <Geth> ¬¶ MoarVM/jit-expr-refactor: We only need this in one place, so might as well define it there.

[11:47] <Geth> ¬¶ MoarVM/jit-expr-refactor: review: https://github.com/MoarVM/MoarVM/commit/ca667f53b6

[12:23] <jnthn> samcv: It doesn't change during the lifetime of the worklist

[12:23] <samcv> ok

[12:23] <jnthn> So such an opt is safe

[12:24] <jnthn> Similar could be done for VMArray too

[12:25] <jnthn> Those macros also should go into worklist.h, but I figure you just put them there for testing. Maybe could be written as static inline too

[12:25] <samcv> yeah

[12:25] <jnthn> But yeah, that's a nice optimization opportunity for aggregates

[12:25] <jnthn> samcv++

[12:26] <nwc10> I forget if I'm asking a questiong that I've asked before - do we have anything like http://speed.pypy.org/ that touts how things are improving?

[12:27] <lizmat> there's the bench repo ?

[12:27] <nwc10> although OMG that does have too little detail on the front page, and information overload immediately behind

[12:27] <nwc10> I was more meaning pretty graphs that massage things to look impressive :-)

[12:30] <samcv> it looks like i don't get too much more speedup past that point.. probably the rest taken up by hash iteration

[12:30] <samcv> gc is at 60% at the point i'm at now. which is still totally terrible but yeah

[12:31] <samcv> process_worklist is at 20% so it's not like MVMHash_gc_mark is quite as crazy as it was when i first started optimizing

[12:31] <samcv> (while MVM_hash_gc_mark is at 35% currently)

[12:37] <jnthn> oh, heck, I found it

[12:37] <jnthn> wow

[12:37] <jnthn> There's an osrpoint right before a prepargs

[12:38] <jnthn> We then stack up an sp_resolvecode and guard ahead of the prepargs

[12:38] <jnthn> But the osrpoint location ends up still after those new instructions

[12:39] <jnthn> So when we OSR, we end up without stuff we need

[12:39] <jnthn> Nothing to do with invoke_v at all, it is just the first case that triggered this situation

[12:40] <nwc10> the "situation" being that the OSR is stored as "before this instruction", which spesh treats as "before this exact opcode" but really should be "before the opcodes that come to represent this logical unit of work" ?

[12:41] <jnthn> Yes

[12:41] <nwc10> good. I did understand it well enough

[12:41] <nwc10> does that mean that the fix is relatively simple? Or are there cases where "before this exact opcode" is correct, and others where "before this logical unit of work" are correct, so there needs to be more metadata stored?

[12:42] <jnthn> I think it's as simple as checking for an OSR deopt annotation upon instruction insertion and moving it

[12:42] <nwc10> if so good, that was what I meant by the first case (but didn't know the detail of the "how")

[12:42] <nwc10> the "relatively simple" :-)

[12:43] <jnthn> Otherwise it'd imply we were inserting instructions with visible side-effects

[12:43] <jnthn> Well, so long as you don't count deopt as a visible side-effect, which it is if you profile, but... :)

[12:43] *** domidumont joined
[12:57] <jnthn> That helps. There's now something ungood during CORE.setting build, but that might be my NQP change

[12:59] <jnthn> hm, alas no

[13:01] <nwc10> do you have sufficient second machine to test the "simple" OSR fix alone on MoarVM master (through NQP to spectest?)

[13:01] <lizmat> jnthn: any specifics

[13:01] <lizmat> ?

[13:01] <nwc10> because if that is a stand-along bug fix, it's useful to have that done

[13:01] <nwc10> (and would also confirm that the fix isn't exposing some other bug that CORE.setting was relying on)

[13:02] <jnthn> Yeah, will try that alone also

[13:02] <jnthn> But the error is "Void return not allowed to context requiring a return value"

[13:03] <jnthn> And only happens under inlining

[13:03] <lizmat> that doesn't ring any bell for me  :-(

[13:03] <jnthn> so seems very related to the allowing opt of invoke_v

[13:03] <jnthn> Well, more opt of...

[13:04] <jnthn> I wonder if uninlining isn't handling the situation well...

[13:06] <jnthn> Hm, not obviously

[13:10] <jnthn> lol

[13:10] <jnthn> The inliner didn't delete the void return instruction

[13:11] <jnthn> So it tried to return from the enclosing thing

[13:18] <timotimo> that's funny :D

[13:18] <jnthn> Easily fixed too

[13:18] <timotimo> for sure

[13:24] <Geth> ¬¶ MoarVM: 1371879313 | (Jonathan Worthington)++ | src/spesh/inline.c

[13:24] <Geth> ¬¶ MoarVM: Delete void return instruction when inlining

[13:24] <Geth> ¬¶ MoarVM: review: https://github.com/MoarVM/MoarVM/commit/1371879313

[13:24] <Geth> ¬¶ MoarVM: 7e625b031a | (Jonathan Worthington)++ | src/spesh/manipulate.c

[13:24] <Geth> ¬¶ MoarVM: Move OSR point backwards on instruction insertion

[13:24] <Geth> ¬¶ MoarVM:

[13:24] <Geth> ¬¶ MoarVM: So that when we stack up resolve/guard instructions ahead of an

[13:24] <Geth> ¬¶ MoarVM: instruction that is an OSR entry point, then we will enter on OSR

[13:24] <Geth> ¬¶ MoarVM: prior to the inserted instructions.

[13:24] <Geth> ¬¶ MoarVM: review: https://github.com/MoarVM/MoarVM/commit/7e625b031a

[13:25] <Geth> ¬¶ MoarVM: 1b1edfe5ff | (Jonathan Worthington)++ | 2 files

[13:25] <Geth> ¬¶ MoarVM: Mark invoke_v as :logged

[13:25] <Geth> ¬¶ MoarVM:

[13:25] <Geth> ¬¶ MoarVM: So that we are able to resolve statistics about it and optimize the

[13:25] <Geth> ¬¶ MoarVM: invocation.

[13:25] <Geth> ¬¶ MoarVM: review: https://github.com/MoarVM/MoarVM/commit/1b1edfe5ff

[13:44] *** brrt left
[13:44] <jnthn> *sigh* Still so many tricky problems

[13:48] <jnthn> I've been looking at `my $foo = "blah"; for ^10_000_000 { my int $i = $foo.chars }` as my example

[13:48] <jnthn> Now it does successfully turn the decont_i into a set of the original unboxed value

[13:49] <jnthn> However, there's two things that keep the boxed value alive

[13:49] <jnthn> One is that it's guarded. We can prove the guard isn't needed, but don't yet do away with it.

[13:50] <jnthn> The second is that the value was read the other side of the guard, thus meaning the other side of a deopt point

[13:51] <jnthn> Which is now gone, but still, we don't know that it isn't used across other deopt points

[13:51] <jnthn> So we'll need a better way to model deopt usages

[13:59] <timotimo> so, i'm wondering: since the OSR we got from the int @a = ^10_000 was so subpar, under what circumstances does the same optimized code get used in the future?

[14:00] <timotimo> i.e. if a program optimizes that STORE method via osr first, then uses it a bunch of times, will those times use the result of the osr?

[14:00] <jnthn> I guess it depends on if the arg types match

[14:01] <timotimo> is it intended that we're not using the arguments in that specific case for facts on the param ops?

[14:01] <jnthn> Dunno, could just be something not yet done

[14:02] <timotimo> i'm not terribly eager to look into osr code :D

[14:05] <timotimo> it really just made a certain specialization, so that'd be why the args aren't used

[14:06] <timotimo> the spesh log doesn't give a type tuple for the stats that get OSR hits; in fact, for that frame there are no type tuples at all

[14:14] <timotimo> i must be misunderstanding something

[14:44] *** brrt joined
[14:50] <samcv> uthash is not too terrible, but it's also really weird. how we store the pointer to the hash table in every single hash item

[14:50] <samcv> not sure why the developers though that was a great idea

[14:50] <samcv> and complicates deallocating the hash since you have to make sure you don't delete whichever item you use as the "head" of the hash. though that's arbitrary. and then deleting that special element becomes more expensive

[14:52] <samcv> i tried to refactor it to remove this, but after many hours i figured out it would probably be faster to add support for another hash library so i stopped in sadness

[14:53] <jnthn> I'm not sure we should use any library by this point, just replace it with our own that's desinged according to the properties we want

[14:54] <samcv> yeah. i mean i may adapt something to our uses

[14:54] <jnthn> I still think we want a single blob of memory

[14:54] <samcv> well i *will* do that. but if i can help it i won't write my own

[14:54] <samcv> yep

[14:54] <jnthn> And we only ever allocate a new one upon growth

[14:54] <samcv> yep

[14:55] <jnthn> If we then only ever read the address of that block once before doing any operations, and allocate it using the FSA + safepoint mechanism, then we fix the explosions under concurrent mis-use, I'd expect.

[14:55] <jnthn> (That doesn't mean it'll be well behaved under concurrent mis-use, just not explosive. That's enough.)

[14:55] <samcv> yeah

[14:56] <samcv> but hands down the worst design decision of uthash is having one master hash handle, and relying in everything thta all handles contain references to the table

[14:57] <samcv> and to fix it i'd have to change uthash entirely (every single function/macro almost). plus do the work i'd have to do anyway changing moarvm code

[14:58] <brrt> i think the 'table' bit of a hash table is relatively easy, it is the 'hash' bit that i've found hard :-)

[14:59] <samcv> yeah

[14:59] <samcv> and also how it assigns values. like it uses the offset of the struct to return elements. and also having it being full of macros

[15:00] <samcv> ok fine. that's just as insane a design decision. but how they made it necesitates macros...

[15:01] <jnthn> Well, I think the idea behind it was you that you can use any C structure has your hash

[15:01] <jnthn> Uh, hash element

[15:02] <jnthn> Which we don't really need

[15:04] <samcv> yeah. that would be my first worst part about it. only reason it isn't is because that has a reason it could be a benefit for some people

[15:05] <samcv> heh

[15:07] *** robertle left
[15:09] *** brrt left
[15:47] <[Coke]> MoarVM panic: Internal error: zeroed target thread ID in work pass

[15:48] <[Coke]> The spawned command 'perl6' exited unsuccessfully (exit code: 17)

[15:48] <[Coke]> (this on a branch of docs. Will update everything and see if I can repro it.)

[16:17] *** domidumont left
[17:10] <jnthn> Finally think I've figured out a workable, more accurate, deopt tracking algorithm

[17:16] <jnthn> Also for gurads, I think we need to at least model such that they introduce a new SSA version

[17:16] <jnthn> So then we can distinguish the facts before the guard and the facts after the guard

[17:16] <timotimo> that'd be nice

[17:17] <jnthn> The the deopt, I found a few algorithms that totally won't work

[17:18] <jnthn> But I think I've hit on one that maybe will

[17:19] <[Coke]> (the rakudo update made it vanish, whee)

[17:19] <jnthn> We keep track of writers with outstanding reads. When we hit a deopt point, we record that deopt point against all of the writers with outstanding reads

[17:21] <jnthn> In the event of a loop, we encounter a read before a write. In the immediate we could disregard it, and then that'd mean the write followed by a deopt point would record that deopt point on the write.

[17:25] <jnthn> Trouble is that the write then lives on forever and potentially gets other not relevant deopt points makekd on it. Hm.

[17:25] <jnthn> *marked

[17:28] <jnthn> Ah, maybe dropping them after we saw all the other reads will cut it

[17:29] <jnthn> Hm, not, it won't, it'll be premature

[17:31] <jnthn> Oh, I think we can do it by saying if we saw all the predecessors of the basic block that had the missing read, then we're good.

[17:33] * jnthn goes for some rest :)

[17:49] *** domidumont joined
[17:50] *** lizmat left
[18:43] *** undersightable6 joined
[18:51] <samcv> removing the previous pointer reduces mem usage by 8.7MB. (max usage 242MB originally)

[18:52] <samcv> from the hash script i've been testing

[18:56] <samcv> at least if /usr/bin/time is to be believed

[18:58] <samcv> that seems to line up closish with 1 million * 64 bits + a bit extra

[19:00] <jnthn> Nice :)

[19:07] *** domidumont left
[19:15] *** Deknos joined
[19:20] *** dogbert17 joined
[19:33] *** robertle joined
[19:46] <dogbert17> hmm, I still have code which is considerably slower now than with 2018.06

[19:47] <timotimo> what's it look like?

[19:47] <dogbert17> one observation is that the function taking the most time, according to perf, is now MVM_spesh_arg_guard_run instead of MVM_interp_run which was the costliest in 2018.06

[19:48] <timotimo> can you get a spesh log and grep out all "waiting for" lines?

[19:48] <dogbert17> timotimo: want a gist

[19:48] <dogbert17> ok

[19:48] *** Deknos left
[19:48] <timotimo> just a shot in the dark

[19:50] <dogbert17> only ~10 lines like 'Was waiting 61216us for logs on the log queue.'

[19:51] <timotimo> OK, that's good

[19:52] <dogbert17> this function, in fourth place, is new, compared to 2018.06 resolve_using_guards (5.82%)

[19:52] <dogbert17> in the perf report that is

[19:56] <dogbert17> .seen AlexDaniel

[19:56] <yoleaux> I saw AlexDaniel 17:52Z in #perl6: <AlexDaniel> I was about to click üëç too :)

[19:56] *** stmuk joined
[19:57] <dogbert17> don't we have a bot which might be able to figure out when/if a perf regression occured?

[19:59] *** stmuk_ left
[20:05] *** brrt joined
[20:05] <dogbert17> timotimo: want to have a look at the code? It might feel slightly familiar :)

[20:05] <timotimo> sure

[20:05] <dogbert17> https://gist.github.com/dogbert17/f663fabba14ffe6fda2fc33182b78c46

[20:06] <dogbert17> on my system 2018.06 takes ~7.5 sec while HEAD(Camelia takes ~10 sec

[20:07] <timotimo> you're not comparing camelia vs your machine, are you?

[20:08] <timotimo> benchable ought to be able to figure out where it changed

[20:08] <timotimo> benchable6: https://gist.githubusercontent.com/dogbert17/f663fabba14ffe6fda2fc33182b78c46/raw/965c4bde104e905d559bf150d2751f3edc58dd9c/gistfile1.txt

[20:08] <benchable6> timotimo, I cannot recognize this command. See wiki for some examples: https://github.com/perl6/whateverable/wiki/Benchable

[20:08] <timotimo> benchable6: releases https://gist.githubusercontent.com/dogbert17/f663fabba14ffe6fda2fc33182b78c46/raw/965c4bde104e905d559bf150d2751f3edc58dd9c/gistfile1.txt

[20:08] <benchable6> timotimo, Successfully fetched the code from the provided URL

[20:08] <benchable6> timotimo, starting to benchmark the 31 given commits

[20:08] *** lizmat joined
[20:09] <dogbert17> timotimo: no, I'n not comparing against Camelia rather using the same version at home

[20:10] <timotimo> ok

[20:10] <timotimo> "releases" wasn't the best idea, i guess

[20:10] <timotimo> that'll take what 5 minutes?

[20:10] <dogbert17> it shouldn't take too long ...

[20:11] <timotimo> if it's 10 seconds, that'd be 310 seconds

[20:11] <dogbert17> hopefully it's woth the wait :)

[20:11] <dogbert17> *worth

[20:12] <dogbert17> but if it only tests releases I guess we won't learn much

[20:12] <timotimo> also true

[20:12] <timotimo> it'll "zoom in on performance differences", too

[20:12] <dogbert17> aha

[20:12] <benchable6> timotimo, ¬´hit the total time limit of 240 seconds¬ª

[20:12] <timotimo> dangit

[20:13] <dogbert17> grr

[20:13] <timotimo> benchable6: 2018.06,HEAD https://gist.githubusercontent.com/dogbert17/f663fabba14ffe6fda2fc33182b78c46/raw/965c4bde104e905d559bf150d2751f3edc58dd9c/gistfile1.txt

[20:13] <benchable6> timotimo, Successfully fetched the code from the provided URL

[20:13] <benchable6> timotimo, starting to benchmark the 2 given commits

[20:13] <dogbert17> clever

[20:14] <dogbert17> quite a bit of stuff has become faster recently but not everything

[20:14] <timotimo> bbiab for dinner prep

[20:14] <benchable6> timotimo, ¬¶2018.06: ¬´4.9331¬ª ¬¶HEAD: ¬´9.1578¬ª

[20:14] <benchable6> timotimo, benchmarked the given commits and found a performance difference > 10%, now trying to bisect

[20:14] <dogbert17> ooh, it seems to be working, amazing

[20:16] *** brrt left
[20:17] <benchable6> timotimo, ¬´hit the total time limit of 240 seconds¬ª

[20:18] <nwc10> sigh, where's the `sudo benchmarkable` option? :-/

[20:18] <MasterDuke> it runs the code 5 times per commit, so anything longer than ~2-3s will likely timeout

[20:19] <MasterDuke> though i should probably increase the timeout for benchable6

[20:22] <dogbert17> MasterDuke: that would be awesome, although I could change the script so it runs faster

[20:25] <dogbert17> benchable6: 2018.06,HEAD https://gist.githubusercontent.com/dogbert17/f663fabba14ffe6fda2fc33182b78c46/raw/965c4bde104e905d559bf150d2751f3edc58dd9c/gistfile1.txt

[20:25] <benchable6> dogbert17, Successfully fetched the code from the provided URL

[20:25] <benchable6> dogbert17, starting to benchmark the 2 given commits

[20:25] <nwc10> to be clear, I wasn't aware how sophisticated but easy benchable6 was. (Or at least, is supposed to be, if you don't give it too slow a code example)

[20:26] <benchable6> dogbert17, ¬¶2018.06: ¬´4.9101¬ª ¬¶HEAD: ¬´9.1712¬ª

[20:26] <benchable6> dogbert17, benchmarked the given commits and found a performance difference > 10%, now trying to bisect

[20:26] <dogbert17> heh, those are the same times as before?

[20:27] <MasterDuke> the numbers in the loops look the same

[20:27] <dogbert17> I edited the script, perhaps the url changed

[20:28] <dogbert17> yes it did, sigh

[20:28] <dogbert17> let's play the waiting game then

[20:29] <benchable6> dogbert17, ¬´hit the total time limit of 240 seconds¬ª

[20:29] <MasterDuke> i just upped it to 300, i'll restart the bot

[20:30] <dogbert17> cool

[20:30] *** benchable6 left
[20:32] <MasterDuke> rm, it restarted, but i don't see it rejoined...

[20:32] <MasterDuke> *hm

[20:32] *** benchable6 joined
[20:32] <MasterDuke> there we go

[20:32] <dogbert17> benchable6: 2018.06,HEAD https://gist.githubusercontent.com/dogbert17/f663fabba14ffe6fda2fc33182b78c46/raw/bd705aa0219141acc40811d452ef14563260a19b/gistfile1.txt

[20:32] <benchable6> dogbert17, Successfully fetched the code from the provided URL

[20:32] <benchable6> dogbert17, starting to benchmark the 2 given commits

[20:33] <benchable6> dogbert17, ¬¶2018.06: ¬´2.1404¬ª ¬¶HEAD: ¬´3.7665¬ª

[20:33] <benchable6> dogbert17, benchmarked the given commits and found a performance difference > 10%, now trying to bisect

[20:36] <benchable6> dogbert17, https://gist.github.com/4cdc381083c9db8f4ef3c380f23001b8

[20:38] <MasterDuke> https://github.com/rakudo/rakudo/commit/d3c5381b747169f951db578bebbfd0c5d8165116

[20:38] <dogbert17> ok, so it's this one: https://github.com/rakudo/rakudo/commit/d3c5381b747169f951db578bebbfd0c5d8165116

[20:39] <MasterDuke> really should make those links...

[20:39] <dogbert17> might be ok then since it warns that there may be initial slowdown followed by cool opts at a later date

[20:40] <dogbert17> benchable6++

[20:59] *** japhb_ joined
[21:01] *** japhb left
[21:16] *** stmuk_ joined
[21:18] *** stmuk left
[21:22] <timotimo> the profiler is unhappy with this code :(

[21:22] <timotimo> i think i see very deep recursion?

[22:06] <MasterDuke> any thoughts/comments on https://github.com/MoarVM/MoarVM/pull/899 ?

[22:25] <dogbert17> timotimo: yes, that's why I thought you might remember seeing it before. It has been discussed earlier since it messed things up for the profiler.

[22:26] <timotimo> mhm

[22:26] <dogbert17> perhaps not so easy to remember, you've probably looked at hundreds of perl6 snippets since :)

[22:27] <dogbert17> I believe you were a bit sceptical towards the junction that's used in the script

[22:55] *** stmuk joined
[22:57] *** stmuk_ left
[23:22] *** lizmat left
[23:25] *** lizmat joined
[23:25] <Geth> ¬¶ MoarVM/new-deopt-point-algo: 7e1ab4b2bd | (Jonathan Worthington)++ | 5 files

[23:25] <Geth> ¬¶ MoarVM/new-deopt-point-algo: Sketch out more precise deopt algorithm

[23:25] <Geth> ¬¶ MoarVM/new-deopt-point-algo:

[23:25] <Geth> ¬¶ MoarVM/new-deopt-point-algo: As described in the commit body. It seems to do about the right thing,

[23:25] <Geth> ¬¶ MoarVM/new-deopt-point-algo: except in the case of loops, where it makes a gross overestimate of

[23:25] <Geth> ¬¶ MoarVM/new-deopt-point-algo: what needs to be preserved due to that part not yet being implemented.

[23:25] <Geth> ¬¶ MoarVM/new-deopt-point-algo: Includes dumping of what deopt indices are keeping instructions alive.

[23:25] <Geth> ¬¶ MoarVM/new-deopt-point-algo: review: https://github.com/MoarVM/MoarVM/commit/7e1ab4b2bd

[23:27] <jnthn> MasterDuke: Just glanced it, seems OK in principle

[23:27] <jnthn> Though I'm a bit fried from trying to figure out a more precise deopt algo :)

[23:51] <jnthn> sleep o/

[23:55] <timotimo> gnite jnthn

