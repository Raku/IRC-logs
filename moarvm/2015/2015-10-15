[00:10] *** hohoho joined
[00:43] *** hohoho joined
[00:46] *** tokuhirom joined
[01:47] *** ilbot3 joined
[02:48] *** tokuhirom joined
[04:16] *** vendethiel joined
[04:49] *** tokuhirom joined
[05:51] <dalek> Heuristic branch merge: pushed 70 commits to MoarVM/even-moar-jit by bdw

[06:32] *** hohoho joined
[06:41] *** tokuhirom joined
[06:47] *** Ven joined
[06:54] *** Ven_ joined
[07:11] *** Ven joined
[07:14] *** FROGGS joined
[07:21] *** Ven_ joined
[07:25] *** zakharyas joined
[07:35] *** TimToady joined
[07:49] *** leont joined
[08:06] *** pyrimidi_ joined
[08:40] *** tokuhirom joined
[08:50] *** Ven joined
[09:47] *** tokuhirom joined
[09:48] *** brrt joined
[09:53] <brrt> \o #moarvm

[09:53] <brrt> i apologise for breaking the build yesterday

[09:53] <brrt> connectivity in the bus was rather spotty, and when i noticed the error i couldn't correct it anymore

[09:58] <brrt> FROGGS++ for fixing, naturally :-)

[09:58] <FROGGS> hi brrt

[09:58] <FROGGS> :o)

[09:58] <brrt> vendethiel: yeah, comparing gcc and v8 is not100% fair in that way

[09:58] <jnthn> .oO( The wifi on the bus goes up and down, up and down... )

[09:58] <brrt> but it was an.. interesting result, shall we say?

[10:04] <brrt> at any rate, nobody is really asking from dynamic languages to be as fast as C or fortran

[10:16] <nwc10> oh, I think you're wrong there. Plenty of folks want the moon, on a stick, now, for free.

[10:16] <nwc10> without worrying about whether they are strong enough to hold it up.

[10:16] <nwc10> no-one sane and reasonable. :-)

[10:17] <nwc10> See http://www.chiark.greenend.org.uk/~sgtatham/putty/wishlist/moon-on-stick.html

[10:17] <nwc10> and the image linked there with "here" :-)

[10:17] <FROGGS> :D

[10:18] <brrt> lol

[10:18] * brrt lunch &

[10:20] <arnsholt> "May be useful for compatibility testing" =D =D =D

[10:47] <brrt> ok, yes, unrealistic expectations abound

[10:53] <nwc10> eg "

[10:53] <nwc10> RPerl compiles your low-magic Perl 5 code to run hundreds of times faster than interpreted Perl, with full backward compatibility!

[10:53] <nwc10> "

[10:54] <brrt> have i seen that?

[10:54] <nwc10> I found it here: https://www.kickstarter.com/projects/wbraswell/perl-5-optimizing-compiler-rperl-v12

[10:54] <nwc10> I read it on the Internet, it must be true.

[10:54] <brrt> oh lord, restricted dynamic language subsets

[10:55] <brrt> also, hundreds of times faster?

[10:55] <brrt> is this ehm.. for real?

[10:56] <nwc10> he's really working on it. And I believe that he really believes that given enough time, he can acchieve that.

[10:56] <brrt> hmmmm

[10:56] <brrt> how restricted are we talking about?

[10:57] <brrt> hmm, no i don't think that can be done, at all

[10:58] <brrt> i mean, handoptimized assembly can't reach 100s of times faster than just perl

[10:58] <nwc10> as best I'm aware of, no-one else working on anything comparable thinks like he does

[10:59] <jnthn> brrt: Perhaps in some cases it maybe can, but in general I agree

[10:59] <brrt> i'm not sure whether that is a good thing for him...

[10:59] <nwc10> I know what I really think, but this is a logged channel.

[11:00] <brrt> i'm thinking of the tests of the potential benefits for the tiler i did last year. i got a factor 6 speedup and that was some pretty good code that would have to do some pretty good optimization before the compiler would ever do that

[11:00] <brrt> (from the old JIT, that is)

[11:01] <brrt> now the old JIT will give you another factor 4 speedup on integer / numeric code, so that brings you at most at 24

[11:01] <brrt> and that is tight code where the benefit from JIT compilation is optimal

[11:01] <brrt> most code looks nothing like that

[11:02] <lizmat> fwiw, the way I understand rperl, is that it is a real compiler, not a JIT

[11:03] <lizmat> as in, compile / optimize everything *before* you start running

[11:03] <jnthn> I think we call it "Ahead Of Time", not "real" ;)

[11:03] <lizmat> hehe...  yeah, sorry, /me is old school  :-)

[11:03] <jnthn> .oO( The JIT who wanted to be a real compiler :D )

[11:04] <lizmat> from a time where you had interpreters or compilers, and the two would never mix  :-)

[11:04] <jnthn> But yeah, I think it's been fairly well demonstrated that if you want to optimize dynamic languages, dynamic analyses tend to beat static analyses.

[11:05] <jnthn> The paper on basic block versioning somebody linked here not so long ago did a nice job of showing that.

[11:05] <nwc10> the way I understand Rperl (without digging into the backend source code) is that it's effectively a Perl (subset) to C++ transpiler

[11:05] <nwc10> that then takes advantage of all the power of a C++ compiler

[11:05] <brrt> uhuh

[11:05] <brrt> like compilation speed ^^

[11:05] <nwc10> so, as long as you know all your types in advance

[11:05] <lizmat> what *i* find interesting about rperl, is its semi-intimate knowledge of the Perl 5 grammar  :-)

[11:06] <brrt> jnthn: didn't read that, could you link it again?

[11:06] <jnthn> uh, don't know I have it handy

[11:06] <lizmat> which might become useful for v5 again in the future

[11:06] * jnthn looks

[11:06] <jnthn> brrt: Pretty sure it's http://pointersgonewild.com/2015/09/24/basic-block-versioning-my-best-result-yet/

[11:07] <jnthn> brrt: Of note the bit around "To put things in perspective, I decided to compare this result with whatâ€™s achievable using a static type analysis. I devised a scheme to give me an upper bound on the number of type tests a static analysis could possibly eliminate. "

[11:07] <brrt> oh, cool

[11:08] <jnthn> Which I thought was an interesting way to look at it.

[11:12] *** Ven joined
[11:18] <brrt> many interest, yes

[11:19] <brrt> in our case, it seems that this could already work at the spesh level?

[11:19] <brrt> i'm asking so i know i don't have to do it ^^

[11:19] <jnthn> :P

[11:19] <jnthn> Yeah, spesh level I'd say

[11:20] <brrt> cool

[11:48] *** tokuhirom joined
[11:56] *** Ven joined
[11:59] <brrt> what's funny is that this: http://rperl.org/the_low_magic_perl_commandments.html is about as far as 'perl' can get from perl

[12:02] <psch> brrt: "you can write java in any language" comes to mind

[12:03] <brrt> supposing the benchmarks are real, it's pretty impressive though

[12:03] <brrt> ah, that was harsh, i do believe the benchmarks are real

[12:19] <nwc10> I have no reason to doubt the benchmarks

[12:20] <nwc10> I'm just not convinced that this is a better trade off than using Inline::CPP

[13:05] *** hohoho joined
[13:08] <brrt> jnthn: on closer examinatin, that seems almost to simple to work

[13:11] <brrt> (reading further)

[13:12] <brrt> authors themselves admit it's pretty similar to trace compilation

[13:13] *** hohoho joined
[13:13] *** hohoho_ joined
[13:22] <jnthn> brrt: Which "that" specifically? BBV?

[13:22] <brrt> yes

[13:22] <brrt> and they rely very much on compiling-on-demand

[13:23] <brrt> which of course we don't do

[13:23] <brrt> i wonder whether tracing would be simpler for us to implement, or something like this

[13:24] <jnthn> Well, given we already have a CFG, the idea of a BB-level analysis seemed at least worth a look :)

[13:25] <[Coke]> I would be interested if anyone can duplicate the run times they were seeing. I was unable to get rperl working on my OSX box.

[13:25] <[Coke]> (but I think that was mostly on me.)

[13:26] <brrt> or OSX :-P

[13:50] *** tokuhirom joined
[14:12] *** Ven joined
[14:51] *** tokuhirom joined
[15:16] *** FROGGS joined
[16:18] *** Ven joined
[16:53] *** tokuhirom joined
[17:16] *** Peter_R joined
[17:53] *** tokuhirom joined
[18:23] <FROGGS> wow, I get a double free in t/spec/S17-lowlevel/lock.t

[18:47] <hoelzro> o/ #moarvm

[18:51] <[Coke]> hio

[18:51] <nwc10> \o hoelzro

[18:52] <hoelzro> o/ [Coke], nwc10

[19:44] *** retupmoca joined
[19:55] *** tokuhiro_ joined
[20:05] *** dalek joined
[21:56] *** tokuhiro_ joined
[22:23] *** leont joined
[22:50] *** tokuhiro_ joined
[23:30] *** diakopter joined
[23:31] *** dalek joined
[23:35] *** Util joined
[23:39] *** hohoho joined
