[00:03] *** reportable6 left
[00:04] *** reportable6 joined
[00:21] *** squashable6 joined
[00:35] <vrurg> [Coke]: to clone you need the original. But it's too late to steal it!

[00:38] *** squashable6 left
[00:43] *** squashable6 joined
[02:09] *** squashable6 left
[02:10] *** squashable6 joined
[03:07] *** squashable6 left
[03:10] *** squashable6 joined
[04:10] *** reportable6 left
[04:10] *** shareable6 left
[04:10] *** sourceable6 left
[04:10] *** benchable6 left
[04:10] *** squashable6 left
[04:10] *** quotable6 left
[04:10] *** nativecallable6 left
[04:10] *** releasable6 left
[04:10] *** bisectable6 left
[04:10] *** statisfiable6 left
[04:10] *** evalable6 left
[04:10] *** coverable6 left
[04:10] *** unicodable6 left
[04:10] *** tellable6 left
[04:10] *** committable6 left
[04:10] *** greppable6 left
[04:10] *** bloatable6 left
[04:10] *** notable6 left
[04:10] *** linkable6 left
[04:10] *** coverable6 joined
[04:11] *** statisfiable6 joined
[04:12] *** sourceable6 joined
[04:12] *** linkable6 joined
[04:12] *** reportable6 joined
[04:12] *** squashable6 joined
[04:12] *** notable6 joined
[04:13] *** committable6 joined
[04:13] *** nativecallable6 joined
[04:19] *** squashable6 left
[04:21] *** squashable6 joined
[05:11] *** shareable6 joined
[05:11] *** tellable6 joined
[05:12] *** unicodable6 joined
[05:12] *** bisectable6 joined
[05:13] *** benchable6 joined
[06:02] *** reportable6 left
[06:05] *** reportable6 joined
[06:11] *** vrurg_ joined
[06:12] *** releasable6 joined
[06:13] *** vrurg left
[07:10] *** quotable6 joined
[07:12] *** bloatable6 joined
[07:19] <Nicholas> good *, #moarvm 

[07:22] <nine> Good sunrise!

[07:53] *** squashable6 left
[08:13] *** evalable6 joined
[08:54] *** squashable6 joined
[08:55] <Geth> ¬¶ MoarVM/fix_phi_out_of_bounds_read: bf106de221 | (Stefan Seifert)++ | 2 files

[08:55] <Geth> ¬¶ MoarVM/fix_phi_out_of_bounds_read: Fix out of bounds read of PHI facts in spesh

[08:55] <Geth> ¬¶ MoarVM/fix_phi_out_of_bounds_read: 

[08:55] <Geth> ¬¶ MoarVM/fix_phi_out_of_bounds_read: During spesh optimization, we remove reads of registers with dead writers from

[08:55] <Geth> ¬¶ MoarVM/fix_phi_out_of_bounds_read: PHI nodes. It could happen that the PHI node ended up with no registers to read

[08:55] <Geth> ¬¶ MoarVM/fix_phi_out_of_bounds_read: at all. However the following analysis code assumed that we'd always have at

[08:55] <Geth> ¬¶ MoarVM/fix_phi_out_of_bounds_read: least 1 register to read from, resulting in an array read out of bounds error

[08:55] <Geth> ¬¶ MoarVM/fix_phi_out_of_bounds_read: and a variety of failure modes.

[08:55] <Geth> ¬¶ MoarVM/fix_phi_out_of_bounds_read: <‚Ä¶commit message has 5 more lines‚Ä¶>

[08:55] <Geth> ¬¶ MoarVM/fix_phi_out_of_bounds_read: review: https://github.com/MoarVM/MoarVM/commit/bf106de221

[08:55] <Geth> ¬¶ MoarVM: niner++ created pull request #1610: Fix out of bounds read of PHI facts in spesh

[08:55] <Geth> ¬¶ MoarVM: review: https://github.com/MoarVM/MoarVM/pull/1610

[08:55] <nine> dogbert17: fix for complex.t ^^^

[08:56] <Geth> ¬¶ MoarVM/fix_phi_out_of_bounds_read: 8a684b3304 | (Stefan Seifert)++ | 2 files

[08:56] <Geth> ¬¶ MoarVM/fix_phi_out_of_bounds_read: Fix out of bounds read of PHI facts in spesh

[08:56] <Geth> ¬¶ MoarVM/fix_phi_out_of_bounds_read: 

[08:56] <Geth> ¬¶ MoarVM/fix_phi_out_of_bounds_read: During spesh optimization, we remove reads of registers with dead writers from

[08:56] <Geth> ¬¶ MoarVM/fix_phi_out_of_bounds_read: PHI nodes. It could happen that the PHI node ended up with no registers to read

[08:56] <Geth> ¬¶ MoarVM/fix_phi_out_of_bounds_read: at all. However the following analysis code assumed that we'd always have at

[08:56] <Geth> ¬¶ MoarVM/fix_phi_out_of_bounds_read: least 1 register to read from, resulting in an array read out of bounds error

[08:56] <Geth> ¬¶ MoarVM/fix_phi_out_of_bounds_read: and a variety of failure modes.

[08:56] <Geth> ¬¶ MoarVM/fix_phi_out_of_bounds_read: <‚Ä¶commit message has 7 more lines‚Ä¶>

[08:56] <Geth> ¬¶ MoarVM/fix_phi_out_of_bounds_read: review: https://github.com/MoarVM/MoarVM/commit/8a684b3304

[08:56] <nine> (just added a reference to the GH issue to the commit message=

[08:59] <MasterDuke> nice

[09:10] *** greppable6 joined
[09:17] <dogbert17> nine++, was it an easy fix?

[09:22] <nine> A few hours in total

[09:22] <nine> Suddenly made a lot of sense when I dumped the spesh graph and saw that PHI that was not actually accessing the register that we got the bogus facts for

[09:38] <dogbert17> I wonder how often this actually happen

[09:39] <dogbert17> *happens

[09:47] <lizmat> moarning!

[09:48] <lizmat> so, with regards to the release... are we in agreement to postpone the release to 4 Dec ?

[09:48] * nine agrees

[09:54] <jnthnwrthngtn> moarning o/

[09:55] <Nicholas> \o

[09:56] <lizmat> and if we are in agreement on the postponement, does that also mean that the attrinited work by jnthn should still go in after that release

[09:56] <lizmat> or that we move that forward ?

[09:58] <jnthnwrthngtn> I locally ran blin on that branch over the weekend, didn't look at the results yet

[10:25] *** evalable6 left
[10:25] *** linkable6 left
[10:27] *** evalable6 joined
[11:27] *** evalable6 left
[11:27] *** evalable6 joined
[11:37] <dogbert17> dogbert@dogbert-VirtualBox:~/repos/oo-monitors$ perl6 -Ilib t/basic.t 

[11:37] <dogbert17> ===SORRY!=== Error while compiling /home/dogbert/repos/oo-monitors/t/basic.t

[11:37] <dogbert17> Missing or wrong version of dependency 'gen/moar/stage2/NQPHLL.nqp' (from '/home/dogbert/repos/oo-monitors/lib/OO/Monitors.pm6 (OO::Monitors)')

[11:37] <dogbert17> at /home/dogbert/repos/oo-monitors/t/basic.t:1

[11:37] <dogbert17> what would be needed in order to figure out why this problem occurs after a bump?

[12:02] *** reportable6 left
[12:05] *** reportable6 joined
[12:05] <MasterDuke> jnthnwrthngtn: an interesting change in behavior from new-disp, referenced in my most recent comment on https://github.com/rakudo/rakudo/pull/4650

[12:07] <MasterDuke> committable6: 2021.09,2021.10 my Int $a; try { $a.VAR.log; CATCH { default { say $_.typename } } }

[12:07] <committable6> MasterDuke, ¬¶2021.09: ¬´Int‚ê§¬ª ¬¶2021.10: ¬´Scalar‚ê§¬ª

[12:25] <lizmat> MasterDuke: I guess that enforces my point  about improving the error message about containers :-)

[12:28] *** linkable6 joined
[12:37] <jnthnwrthngtn> MasterDuke: That looks more like a fix to me than anything? :)

[12:38] <MasterDuke> heh, yeah. just wondering if there are any other places we should look for such things

[12:39] <MasterDuke> though, tbh, i'm not sure why the invocant is `Int`. shouldn't it be `Scalar` also?

[13:30] <lizmat> and yet another Rakudo Weekly News hits the Net: https://rakudoweekly.blog/2021/11/23/2021-47-david-h-adler-rip/

[14:14] <lizmat> timo: answer to your question about a golf of the reducing CPU usage on race

[14:15] <lizmat> say (^5000000).roll(20).race(:1batch).map: { [+] ^$_ .map: { $_ * $_ } }

[14:15] <lizmat> the answer is not important, the CPU usage is

[14:18] <timo>  having only four cores makes this possibly not easy to reproduce?

[14:19] * lizmat only has 8 though

[14:21] <lizmat> saw similar behaviour on a 4 core machine

[14:22] <lizmat> https://gist.github.com/lizmat/2e5bb69739d0fc44867e9024fdc0dd47   # snapper

[14:27] <MasterDuke> yeah, i see pretty much the same thing

[14:39] <lizmat> hmmm... maybe this is not a good example... 

[14:39] <lizmat> playing with some debug messages in core

[14:43] <lizmat> hmmm... is nqp::time threadsafe ?

[14:44] <jnthnwrthngtn> Struggle to imagine it not being; it calculates and returns a simple numeric value?

[14:45] <lizmat> I'm just seeing strange values in debug messages, like "13: completed in 7936936 msecs"

[14:45] <lizmat> that would be more than 2 hours  :-)

[14:46] <lizmat> I basically added:

[14:46] <lizmat> my $from = nqp::time;

[14:46] <lizmat> say "$*THREAD.id(): starting task";

[14:46] <evalable6> lizmat, rakudo-moar 54649dd2a: OUTPUT: ¬´1: starting task‚ê§¬ª

[14:46] <lizmat> to !run-one

[14:46] <lizmat> and:

[14:46] <lizmat> say "$*THREAD.id(): completed in { ((nqp::time() - $from) / 1000).Int } msecs";

[14:46] <lizmat> at the end

[14:51] <jnthnwrthngtn> The the units of nqp::time micros or nanos?

[14:51] <lizmat> yeah, the golf is flawed

[14:51] <lizmat> nanos

[14:52] <jnthnwrthngtn> Ah, maybe you intended msecs to be micro rather than mili, and I assumed milli...

[15:00] <lizmat> say (^5000000).roll(20).race(:1batch).map: { [+] ^$_ .race.map: { $_ * $_ } }     # better golf

[15:00] <lizmat> and this better golf shows no decline in CPU usage, so I guess it *is* my log loading algorithm that is to blame

[15:02] <jnthnwrthngtn> Is your loading CPU or I/O bound, ooc?

[15:02] <lizmat> well, I'd say CPU bound, as the IO is just a .slurp

[15:31] <lizmat> timo: please disregard my golf, until I have a better one

[16:21] *** evalable6 left
[16:21] *** linkable6 left
[16:24] *** evalable6 joined
[16:26] <dogbert17> nine: I have now been running complex.t in a loop for a couple of hours and it hasn't crashed so your PHI fix works perfectly

[16:30] * dogbert17 wonders if nine's PR might have fixed the hyper bug as well

[16:38] <nine> \o/

[16:38] *** [Coke] left
[16:39] <MasterDuke> nine: btw, did you see https://dev.azure.com/MoarVM/MoarVM/_build/results?buildId=909&view=logs&jobId=d0f6403b-d137-5ae0-a533-f4bebafcc251&j=d0f6403b-d137-5ae0-a533-f4bebafcc251&t=608d3b9b-a763-5670-6013-39559d8f7fdf ?

[16:40] <nine> oh no

[16:40] <MasterDuke> and while there's some recent talk about releases and whether to delay merging branches, anyone have thoughts/comments/suggestions on https://github.com/MoarVM/MoarVM/pull/1608 ?

[16:44] <nine> MasterDuke: I actually have an idea about that failure

[16:45] <MasterDuke> oh?

[16:46] <nine> res is uninitialized here: https://github.com/MoarVM/MoarVM/blob/master/src/core/nativecall_libffi.c#L217 but added to frame roots here: https://github.com/MoarVM/MoarVM/blob/master/src/core/nativecall_libffi.c#L326

[16:52] <MasterDuke> probably unrelated, but `values` leaks here https://github.com/MoarVM/MoarVM/blob/master/src/core/nativecall_libffi.c#L220

[16:52] <Geth> ¬¶ MoarVM: 0006714d07 | (Stefan Seifert)++ | src/core/nativecall_libffi.c

[16:52] <Geth> ¬¶ MoarVM: Fix use of uninitialized memory in native callbacks with libffi

[16:52] <Geth> ¬¶ MoarVM: 

[16:52] <Geth> ¬¶ MoarVM: We're GC rooting res.o but didn't initialize the local variable. This could

[16:52] <Geth> ¬¶ MoarVM: cause memory corruption or segfaults when the GC was trying to process this

[16:52] <Geth> ¬¶ MoarVM: object.

[16:52] <Geth> ¬¶ MoarVM: review: https://github.com/MoarVM/MoarVM/commit/0006714d07

[16:55] <MasterDuke> it isn't used at all in callback_handler

[16:56] *** [Coke] joined
[16:56] <nine> MasterDuke: indeed! Please just remove it :)

[16:57] <MasterDuke> it's alloca'ed in MVM_nativecall_invoke and MVM_nativecall_dispatch, but the individual elements are MVM_malloc'ed, so there could be a leak if an exception is thrown partway through the `for (i = 0; i < num_args; i++) {` loops

[17:41] <MasterDuke> oh wait, don't all the element in `values` leak even if there's no exception?

[17:44] <nine> MasterDuke: looks like, yes

[18:02] *** reportable6 left
[18:19] <Geth> ¬¶ MoarVM: 66688b941e | (Daniel Green)++ | src/core/nativecall_libffi.c

[18:19] <Geth> ¬¶ MoarVM: Remove unused variable

[18:19] <Geth> ¬¶ MoarVM: review: https://github.com/MoarVM/MoarVM/commit/66688b941e

[18:20] <MasterDuke> that was simple enough to just do, the other stuff i'll do in a branch/pr

[18:21] <nine> Looks to me like those little mallocs for arg handling could become allocas easily

[18:21] *** linkable6 joined
[18:22] <MasterDuke> ah, maybe i'll add them to https://github.com/MoarVM/MoarVM/pull/1608

[18:28] *** TempIRCLogger__ left
[18:31] *** lizmat left
[18:35] *** lizmat joined
[18:36] *** TempIRCLogger joined
[18:39] *** TempIRCLogger left
[18:39] *** TempIRCLogger joined
[18:40] *** lizmat_ joined
[18:43] *** lizmat left
[18:44] *** lizmat_ left
[18:44] *** lizmat joined
[19:04] *** reportable6 joined
[19:32] <nine> Btw. I've managed to catch one of those segfaults in the p6-GLib build process in rr

[19:43] <lizmat> nine++

[19:44] <MasterDuke> nice

[19:49] <nine> Seems like none of the hypothesis so far is fitting to this new data

[19:54] <MasterDuke> m: use nqp; my num $a; $a = nqp::time_n for ^1_000_000; say now - INIT now

[19:54] <camelia> rakudo-moar 3b6135b0d: OUTPUT: ¬´0.04672441‚ê§¬ª

[19:54] <MasterDuke> m: use nqp; my num $a; $a = now.Num for ^1_000_000; say now - INIT now     # huh, i thought this was closer to nqp::time_n than it is...

[19:54] <camelia> rakudo-moar 3b6135b0d: OUTPUT: ¬´2.437410707‚ê§¬ª

[19:56] <timo> well, what does it spesh to?

[19:56] <nine> What I've got so far: we're doing a return_o. This calls MVM_frame_try_return which finds an exit handler on the current frame. It then calls MVM_frame_dispatch_from_c to run this exit handler. MVM_frame_dispatch_from_c set's the caller's return address: cur_frame->return_address = *(tc->interp_cur_op)

[19:57] <nine> But cur_op was already advanced, so it actually points right at the end of the bytecode

[19:57] <nine> When we return from that exit handler, we then start processing whatever follows the bytecode in memory.

[19:58] <nine> Now this so far is pretty clear. What isn't is why this only happens now and then not more deterministically.

[20:04] *** linkable6 left
[20:04] *** evalable6 left
[20:06] <MasterDuke> ugh. $.tai needing to be a Rational in Instant is really annoying

[20:07] <MasterDuke> even the 'after' spesh of 'Num has gcd_I

[20:12] <MasterDuke> ah, but some manual inlining seems to help

[20:12] <MasterDuke> got it down to 0.325s

[20:35] *** nine left
[20:36] *** nine joined
[21:06] *** evalable6 joined
[21:09] <nine> Oooh....it has something to do with spesh. I added an assertion to the runloop to reliably catch when we exit the proper bytecode. With MVM_SPESH_BLOCKING=1 it fails every time while with MVM_SPESH_DISABLE=1 I haven't seen it fail yet.

[21:29] <nine> But it's a quite unusual spesh issue. It happens even with MVM_SPESH_LIMIT=1

[21:29] <nine> So it's not speshing of any particular frame that causes the issue, but spesh being active at all. But then, I don't understand how MVM_SPESH_BLOCKING=1 can make such a difference

[21:31] <MasterDuke> NO_DELAY change anything?

[22:44] * japhb is looking forward to MasterDuke's PR speeding up `now`

[23:04] *** linkable6 joined
[23:13] <japhb> D'oh!  Now I see it.  Sigh.  ETOOMANYCHANNELS

