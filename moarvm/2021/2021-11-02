[00:03] *** reportable6 left
[00:14] *** dogbert11 joined
[00:14] *** dogbert17 left
[01:05] *** reportable6 joined
[01:42] *** evalable6 joined
[02:15] *** frost joined
[03:21] *** Voldenet_ joined
[03:22] *** Voldenet left
[03:22] *** Voldenet_ is now known as Voldenet

[04:13] *** kjp left
[04:15] *** kjp joined
[04:41] *** linkable6 joined
[05:41] *** evalable6 left
[05:41] *** linkable6 left
[05:44] *** evalable6 joined
[06:02] *** reportable6 left
[06:04] *** reportable6 joined
[07:18] *** unicodable6 left
[07:18] *** sourceable6 left
[07:18] *** releasable6 left
[07:18] *** bloatable6 left
[07:18] *** benchable6 left
[07:18] *** statisfiable6 left
[07:18] *** bisectable6 left
[07:18] *** notable6 left
[07:18] *** evalable6 left
[07:18] *** quotable6 left
[07:18] *** reportable6 left
[07:18] *** shareable6 left
[07:18] *** squashable6 left
[07:18] *** greppable6 left
[07:18] *** committable6 left
[07:18] *** nativecallable6 left
[07:18] *** tellable6 left
[07:18] *** coverable6 left
[07:18] *** squashable6 joined
[07:18] *** tellable6 joined
[07:19] *** unicodable6 joined
[07:19] *** quotable6 joined
[07:19] *** sourceable6 joined
[07:19] *** bloatable6 joined
[07:19] *** greppable6 joined
[07:21] *** shareable6 joined
[07:42] *** linkable6 joined
[07:43] <MasterDuke> i'm not sure why the patch in here is causing the error during rakudo's install https://gist.github.com/MasterDuke17/82c5adac973524490488bad4538f4af7

[07:56] <nine> MasterDuke: patch looks ok to me

[07:57] <nine> Maybe it's unblocking code that contains an op with a buggy JIT implementation? We've had that before

[07:57] <MasterDuke> good. and interestingly, i just re-ran the rakudo build 30s ago (only tried it once last night) and it completed fine

[07:58] <MasterDuke> oh wow! but half the test file in `make m-test` had fails

[07:59] <MasterDuke> though amusingly, i get an occasional passing TODO in t/01-sanity/99-test-basic.t

[08:01] <MasterDuke> (which made more sense when i looked and that test file tests Test, so the passing todo isn't real)

[08:04] <MasterDuke> i am on this branch https://github.com/MoarVM/MoarVM/pull/1584 though everything was ok before this most recent change. but i guess i'll check that patch against master to see if anything changes

[08:06] <nine> I'd test which part of the patch causes the issues. It does 5 things: move those sanity checks from interp to inside the function, change the one existing JIT to a function call and add 3 more JITed ops. 5 parts you can easily test individually.

[08:07] <MasterDuke> that too

[08:19] *** benchable6 joined
[08:19] *** releasable6 joined
[08:20] *** committable6 joined
[08:20] *** notable6 joined
[08:21] *** statisfiable6 joined
[08:21] *** reportable6 joined
[08:22] <MasterDuke> seems to be ctxouter. if i don't jit it, spectests are fine. put it back in, instafails

[08:23] <MasterDuke> (i.e., the other three ops are fine jitted)

[08:33] <nine> I'd bet on ctxouter unblocking something broken

[08:37] <MasterDuke> this is using the spesh frame walker, there were a bunch of changes around it recently, right?

[08:38] <nine> yes

[08:57] <MasterDuke> does this look ok?

[08:57] <MasterDuke> (rr) p *name

[08:57] <MasterDuke> $3 = {common = {header = {sc_forward_u = {forwarder = 0x0, sc = {sc_idx = 0, idx = 0}, st = 0x0}, owner = 1, flags1 = 0 '\000', flags2 = 2 '\002', size = 48}, st = 0x560ce45dbd50}, body = {storage = {blob_32 = 0x560ce4ce1cf0, 

[08:57] <MasterDuke>       blob_ascii = 0x560ce4ce1cf0 "::?CLASS\220^\021\232\n\177", blob_8 = 0x560ce4ce1cf0 "::?CLASS\220^\021\232\n\177", strands = 0x560ce4ce1cf0, any = 0x560ce4ce1cf0}, storage_type = 2, num_strands = 0, num_graphs = 8, 

[08:57] <MasterDuke>     cached_hash_code = 11759813466749862169}}

[09:03] <MasterDuke> looks to me like there's some junk on the end of the string

[09:04] <MasterDuke> but it utf8 encode seemingly without problem to "::?CLASS"

[09:06] <nine> That's ok. Those blobs are just not 0 terinated. You only have to look at the first num_graphs characters

[09:20] *** evalable6 joined
[10:00] <jnthnwrthngtn> moarning o/

[10:04] <lizmat> jnthnwrthngtn  o/

[10:07] <jnthnwrthngtn> Hope y'all have been having fun while I was away. At least, I see commits :)

[10:12] <lizmat> yeah, some commits and some issues as well

[10:13] <lizmat> https://github.com/rakudo/rakudo/issues/4601 cost me some time to figure out

[10:13] <lizmat> :-(

[10:18] <nine> Well, Sunday was definitely fun :) https://niner.name/pictures/PANO_20211031_124518.jpg  Yesterday was a bit unproductive as body was rather unambiguously demanding rest

[10:18] <lizmat> wow

[10:19] *** coverable6 joined
[10:19] <lizmat> definitely above the tree line  :-)

[10:19] *** bisectable6 joined
[10:22] <jnthnwrthngtn> That's pretty :)

[10:25] <jnthnwrthngtn> Just taken care of the last comment on https://github.com/rakudo/rakudo/pull/4591 

[10:25] <nine> We first went up the Hagler 1669m and then continued to the Hoher Nock at 1963m. Our route: https://bit.ly/2ZI8yUW

[10:26] <lizmat> jnthnwrthngtn: I guess this will need correction then: https://docs.raku.org/language/variables#Pre-defined_lexical_variables

[10:26] <lizmat> There are three special variables that are available in every block:

[10:29] <jnthnwrthngtn> Yes, that's a bit confusing. Only $_ is fresh per block (and then it's *bound* to what is in the outer scope, so you'd need to make a binding, not assignment)

[10:29] <lizmat> yeah, working on fixing the doc

[10:29] <jnthnwrthngtn> lizmat++

[10:30] <lizmat> m: sub a() { dd MY::.keys }; a   # what about $¬¢ ?

[10:30] <camelia> rakudo-moar 580b3ba79: OUTPUT: ¬´("\$¬¢", "\$_", "\$/", "\$!").Seq‚ê§¬ª

[10:31] <lizmat> implementation detail ?

[10:33] <nine> It appears in spec tests

[10:33] <lizmat> ah, interesting

[10:34] <lizmat> but only twice

[10:35] <jnthnwrthngtn> Only declared inside of rules/regexes

[10:41] <MasterDuke> so the exception i'm getting is thrown by MVM_frame_lexical_primspec, because MVM_get_lexical_by_name returns MVM_INDEX_HASH_NOT_FOUND. which is correct, the name it's looking for isn't there. but i don't know why

[10:42] <dogbert11> there are two (?) dispatchy bugs lurking about as well. They only show up under the toughest conditions, i.e. MVM_SPESH_NODELAY=1, MVM_SPESH_BLOCKING=1 and a very small nursery.

[10:42] <Geth> ¬¶ MoarVM/new-special-return: 17 commits pushed by (Jonathan Worthington)++

[10:42] <Geth> ¬¶ MoarVM/new-special-return: review: https://github.com/MoarVM/MoarVM/compare/bb6427d7bdfa...ad760b58598e

[10:43] <lizmat> even though $¬¢ is tested for in roast, it feels like an implementation detail

[10:43] <dogbert11> Messages are 'Can only use manipulate a capture known in this dispatch' and 'Already set resume init args for this dispatcher' respectively

[10:45] <dogbert11> The second one has a large chance of being GC related, https://gist.github.com/dogbert17/9c648c7e45d31a09abc686c6e046b573

[10:45] <jnthnwrthngtn> lizmat: I assure you if it was an implementation detail I woulda picked a different name ;-)

[10:45] <jnthnwrthngtn> I'm quite sure it appears in S05

[10:46] <jnthnwrthngtn> Yup, 12 mentions in https://raw.githubusercontent.com/perl6/specs/master/S05-regex.pod

[10:46] <jnthnwrthngtn> How useful it is, is another question :)

[10:47] <jnthnwrthngtn> Especially after the Cursor/Match unification

[10:48] <nine> dogbert11: FWIW I've yet again failed to reproduce. This time the range.t issue

[10:49] <lizmat> jnthnwrthngtn: ok, I will *not* document it  :-)

[10:49] <dogbert11> nine: how is that possible :) what nursery size are you using?

[10:51] <dogbert11> also, are you on master or your nativecall branch?

[10:58] <Geth> ¬¶ MoarVM/faster-frame-alloc: fcddd1210d | (Jonathan Worthington)++ | src/core/frame.c

[10:58] <Geth> ¬¶ MoarVM/faster-frame-alloc: Split specialized and unspecialized frame alloc

[10:58] <Geth> ¬¶ MoarVM/faster-frame-alloc: 

[10:58] <Geth> ¬¶ MoarVM/faster-frame-alloc: This allows for us to eliminate more checks than the compiler can, and

[10:58] <Geth> ¬¶ MoarVM/faster-frame-alloc: also seems - at least on some compilers - to make it possible to inline

[10:58] <Geth> ¬¶ MoarVM/faster-frame-alloc: into MVM_frame_dispatch. One further opportunity is that we can fold two

[10:58] <Geth> ¬¶ MoarVM/faster-frame-alloc: calls to memset into one in the common on-callstack allocation case of

[10:58] <Geth> ¬¶ MoarVM/faster-frame-alloc: specialized frames.

[10:58] <Geth> ¬¶ MoarVM/faster-frame-alloc: review: https://github.com/MoarVM/MoarVM/commit/fcddd1210d

[10:58] <Geth> ¬¶ MoarVM/faster-frame-alloc: 9903a543dc | (Jonathan Worthington)++ | src/core/frame.c

[10:59] <Geth> ¬¶ MoarVM/faster-frame-alloc: Elide frame initialization check with spesh cand

[10:59] <Geth> ¬¶ MoarVM/faster-frame-alloc: 

[10:59] <Geth> ¬¶ MoarVM/faster-frame-alloc: If we are at the point that we are making a call where we have already

[10:59] <Geth> ¬¶ MoarVM/faster-frame-alloc: identified a specialization of a frame, then clearly that frame has been

[10:59] <Geth> ¬¶ MoarVM/faster-frame-alloc: invoked before. Thus move the frame initialization check into the path

[10:59] <Geth> ¬¶ MoarVM/faster-frame-alloc: of the non-specialized invocation.

[10:59] <Geth> ¬¶ MoarVM/faster-frame-alloc: review: https://github.com/MoarVM/MoarVM/commit/9903a543dc

[11:01] <nine> dogbert11: on master with 4K nursery while MVM_SPESH_NODELAY=1 MVM_SPESH_BLOCKING=1 ./rakudo-m -Ilib t/spec/S02-types/range.t ; do :; done

[11:01] <nine> dogbert11: also tried MVM_GC_DEBUG 3

[11:11] <dogbert11> I failed to mention that I actually run that test with a 20k nursery and MVM_GC_DEBUG=1

[11:11] * dogbert11 hides

[11:19] *** evalable6 left
[11:19] *** linkable6 left
[11:20] *** nativecallable6 joined
[11:20] <Geth> ¬¶ MoarVM: jnthn++ created pull request #1589: Some small optimizations for frame allocation

[11:20] <Geth> ¬¶ MoarVM: review: https://github.com/MoarVM/MoarVM/pull/1589

[11:22] *** evalable6 joined
[11:22] <lizmat> jnthnwrthngtn : https://github.com/Raku/doc/commit/c867f164ba

[11:22] <jnthnwrthngtn> grmbl, seems I'll have to go searching for why https://github.com/MoarVM/MoarVM/pull/1581 causes `make test` failures on CI

[11:22] <jnthnwrthngtn> (Which I've yet to see locally)

[11:23] <MasterDuke> two of them are using clang, do you every try that locally?

[11:25] <jnthnwrthngtn> I thought this machine was builidng with clang but now I see it's GCC :)

[11:26] <jnthnwrthngtn> lizmat: I wonder whether "in each routine" would be better than "in each sub / method" (in that it's more complete; for example, it's true of submethod but also regex/token/rule routines too)

[11:26] <lizmat> ok... I was wondering about that, but yeah

[11:27] <jnthnwrthngtn> lizmat: An alternative solution to "declare your own $/" would also be "declare a sub instead"

[11:27] <lizmat> map: sub () { 

[11:27] <lizmat> you mean?

[11:27] <jnthnwrthngtn> Yes

[11:27] <jnthnwrthngtn> Well, it'd have to be map: sub ($_) {

[11:28] <lizmat> ah, yes

[11:28] <jnthnwrthngtn> Dunno to what degree this is better, but it's maybe a nice alternative

[11:29] <jnthnwrthngtn> Otherwise it looks good to me

[11:29] <lizmat> I feel the "my $/;" is more self-documenting

[11:29] <lizmat> it's easy to lose track of why a `sub` was used there, and then a subsequent refactorer might think:

[11:29] <lizmat> why a sub?  that's not needed!   

[11:30] <lizmat> and bang, your program suddenly becomes unreliable

[11:30] <jnthnwrthngtn> True

[11:30] <lizmat> so I feel like not documenting that option :-)

[11:34] <jnthnwrthngtn> MasterDuke: Sadly, building with clang did not magically surface the error

[11:35] <jnthnwrthngtn> Hmm

[11:36] <jnthnwrthngtn> Type check failed for return value; expected CompUnit:D but got Method (method sink (Mu: *%_...)

[11:36] <jnthnwrthngtn> at /home/vsts/work/1/rakudo/t/02-rakudo/04-diag.t:3

[11:37] <jnthnwrthngtn> The test varies, but the error seems to reliably be that one

[11:43] <jnthnwrthngtn> oh, interesting...actually one of the 3 is a SEGV in t/04-nativecall/08-callbacks.t

[11:46] <jnthnwrthngtn> Which I can't reproduce either :(

[11:47] <jnthnwrthngtn> nine: Did you ever manage to reproduce the issues seen in CI locally for https://github.com/MoarVM/MoarVM/pull/1581 ?

[11:48] <jnthnwrthngtn> The PR actually does two things: replace the special return mechanism, and then do some follow-up optimizations. I guess I'll try a version of the PR that only has the first half of it.

[11:52] <Geth> ¬¶ MoarVM/new-special-return-only: 07be913d17 | (Jonathan Worthington)++ | src/core/callstack.h

[11:52] <Geth> ¬¶ MoarVM/new-special-return-only: Add temporary define for migrating Rakudo extops

[11:52] <Geth> ¬¶ MoarVM/new-special-return-only: review: https://github.com/MoarVM/MoarVM/commit/07be913d17

[11:52] <Geth> ¬¶ MoarVM/new-special-return-only: 03ff8e7a28 | (Jonathan Worthington)++ | src/gc/finalize.c

[11:52] <Geth> ¬¶ MoarVM/new-special-return-only: Avoid a NULL dereference when running finalizers

[11:52] <Geth> ¬¶ MoarVM/new-special-return-only: review: https://github.com/MoarVM/MoarVM/commit/03ff8e7a28

[11:53] <Geth> ¬¶ MoarVM: jnthn++ created pull request #1590: Migrate special return to callstack

[11:53] <Geth> ¬¶ MoarVM: review: https://github.com/MoarVM/MoarVM/pull/1590

[12:00] <lizmat> jnthnwrthngtn: fwiw, your latest rakudo merge seems to negatively affect test-t (1 or 2% or so)

[12:02] *** reportable6 left
[12:10] <jnthnwrthngtn> lizmat: Curious, I'd have really have expected much of an effect there.

[12:10] <jnthnwrthngtn> For the things it helps, the win is a lot greater than 1 or 2%.

[12:12] <lizmat> there's that  :-)

[12:12] <lizmat> just giving data points, not arguing  :-)

[12:13] <jnthnwrthngtn> For example, with Agrammon we saw a 9% slowdown after new-disp. With this, it became a few percent improvement over pre-new-disp

[12:13] <lizmat> works for me :-)

[12:14] <jnthnwrthngtn> Still, I wonder what the mechanism is that causes a change in test-t.

[12:14] <jnthnwrthngtn> Does the effect appear in test-t-20?

[12:15] <jnthnwrthngtn> (That is, has this change made warm-up a bit worse?)

[12:16] <dogbert11> nine: before you give up, can you try this golf on normal settings, i.e. no flags set and no changes to the GC flags? https://gist.github.com/dogbert17/af79a02ec68dab2066ab58cb4d9c6711

[12:16] <jnthnwrthngtn> Sigh, turns out that https://github.com/MoarVM/MoarVM/pull/1590 is even more faily

[12:17] <jnthnwrthngtn> lunch &

[12:22] *** linkable6 joined
[12:30] <lizmat> hmmm... maybe it's just a matter of slightly increased startup time ?

[13:02] <nine> jnthnwrthngtn: no, my reproduction success rate has been abysmal in the past week

[13:03] *** reportable6 joined
[13:21] <jnthnwrthngtn> lizmat: Maybe, that's why I asked about test-t-20 (which would be less sensitive to that)

[13:28] <timo> jnthnwrthngtn: i think the last faster-frame-alloc commit prevents frames that have already run from hitting the instrumentation barrier after profiling has been turned on, like when you profile a program that evals, for example, a bunch of frames in the compiler would already be specialized

[13:28] <lizmat> I see an improvement there for sure: 14.3 seconds -> 13.8 seconds 

[13:28] <lizmat> for test-t-20

[13:30] <jnthnwrthngtn> timo: It's not that the frames have been specialized, but rather that they are called via specialization linking

[13:35] <timo> ah, so really only from another specialized frame already

[13:35] <timo> so there'd always be an outer that's invoked normally and can hit the barrier

[13:35] <timo> well, not "outer"

[13:36] <nine> caller?

[13:37] <lizmat> jnthnwrthngtn: fwiw, I don't see much of a difference with test-t --race

[13:37] <lizmat> perhaps that just doesn't run long enough

[13:37] <jnthnwrthngtn> lizmat: Perhaps so

[13:42] *** MasterDuke left
[13:47] <lizmat> afk for a bit&

[14:09] *** MasterDuke joined
[14:40] *** vrurg left
[14:42] *** vrurg joined
[15:00] *** frost left
[16:06] <dogbert11> nine: are you on a train getting home from $work?

[16:12] *** evalable6 left
[16:12] *** linkable6 left
[16:12] *** evalable6 joined
[16:12] <timo> he took a midnight train --> Any $ where ...

[16:13] <lizmat> jnthnwrthngtn: comparing https://logs.liz.nl/raku-dev/2021-10-29.html#17:35 with https://logs.liz.nl/raku-dev/2021-11-02.html#14:43 shows test-ts are slower  :-(

[16:13] <dogbert11> timo: can I trick you to run https://gist.github.com/dogbert17/af79a02ec68dab2066ab58cb4d9c6711 in order to see if you can repro the error we discussed yesterday?

[16:17] <dogbert11> or anyone else for that matter :)

[16:18] <MasterDuke> `Already set resume init args for this dispatcher`

[16:19] <dogbert11> MasterDuke++, this means I'm not insane and the problem is real

[16:22] <dogbert11> perhaps jnthnwrthngtn can figure out what's wrong just by looking at the gist

[16:39] <timo> i also get the error message locally

[16:51] <jnthnwrthngtn> lizmat: Hm, didn't you post a number earlier showing a faster test-t-20?

[16:51] <lizmat> that was on my local machine

[16:53] <nine> dogbert11: I've just come home from work

[16:54] <jnthnwrthngtn> lizmat: OK, let's see tomorrow's numbers to make sure it's not just that the machine they were posted from was otherwise busy

[16:55] <lizmat> yup

[16:55] <jnthnwrthngtn> dogbert11: I can reproduce it. It goes away with MVM_SPESH_DISABLE=1.

[16:55] <jnthnwrthngtn> dogbert11: But doesn't crash but instead changes behavior (loads of warnings) with MVM_SPESH_BLOCKING=1

[16:57] <nine> dogbert11: FWIW I can finally reproduce it as well with your golf

[16:58] <nine> jnthnwrthngtn: could it be because finalizers are now run at an inopportune time?

[17:00] <jnthnwrthngtn> nine: That ain't merged yet :)

[17:00] <jnthnwrthngtn> Actually I was on the branch that changed that and went back to MoarVM master to check it

[17:00] <nine> It also occurs with finalizers disabled

[17:00] <jnthnwrthngtn> Turns out the warnings are there too

[17:00] <jnthnwrthngtn> It's just that when it crashes it does so before it has time to spit out a load of them, it seems

[17:02] <jnthnwrthngtn> oh, it's not stable; it behaves differently run to run

[17:04] <dogbert11> suddenly everyone can repro, cool :)

[17:04] <jnthnwrthngtn> Gah, disabling hash randomization doesn't get it completely stable

[17:08] <jnthnwrthngtn> It spesh bissects to Spesh of '' (cuid: 3, file: x.raku:14)

[17:10] <timo> any deopts happen right before explosion?

[17:10] <jnthnwrthngtn> uh, this makes no sense, how on earth do we end up in nextwith?

[17:10] <jnthnwrthngtn>    at gen/moar/BOOTSTRAP/v6c.nqp:6545  (/home/jnthn/dev/rakudo/blib/Perl6/BOOTSTRAP/v6c.moarvm:)

[17:10] <jnthnwrthngtn>  from SETTING::src/core.c/control.pm6:142  (/home/jnthn/dev/rakudo/blib/CORE.c.setting.moarvm:nextwith)

[17:10] <jnthnwrthngtn>  from x.raku:14  (<ephemeral file>:)

[17:10] <jnthnwrthngtn>  from x.raku:14  (<ephemeral file>:)

[17:10] <jnthnwrthngtn> There is no nextwith in this whole file

[17:11] <nine> neither in Test.pm6

[17:11] <jnthnwrthngtn> Nor...right

[17:11] <nine> err... .rakumod

[17:11] <lizmat> .oO( if it walks like a duck, and quacks like a duck, and it still isn't a duck, the name is wrong )

[17:11] *** nine left
[17:13] *** linkable6 joined
[17:14] *** nine joined
[17:15] *** nine left
[17:15] *** nine joined
[17:16] <jnthnwrthngtn> Anybody repro this?

[17:16] <jnthnwrthngtn> $ ./rakudo-m -e 'for ^10000 { <0+0i> ~~ 1..10 }'

[17:16] <jnthnwrthngtn> Segmentation fault (core dumped)

[17:17] <lizmat> yup

[17:17] <lizmat> segfaults for me

[17:17] <MasterDuke> yep, in setup_translated_resumption

[17:17] <jnthnwrthngtn> OK, that's a better golf them

[17:17] <jnthnwrthngtn> *then

[17:18] <lizmat> need about 2200 iterations to get a reliable segfault

[17:18] <MasterDuke> (gdb) p *ri

[17:18] <MasterDuke> $2 = {dp = 0x0, deopt_idx = 0, res_idx = 0, state_register = 0, init_registers = 0xa5}

[17:19] <jnthnwrthngtn> Still there with JIT disabled, which makes the gdb stacktrace a little easier

[17:19] <dogbert11> jnthnwrthngtn: I get a SEGV

[17:20] <dogbert11> disappears with MVM_SPESH_INLINE_DISABLE=1

[17:22] <lizmat> confirmed

[17:22] <jnthnwrthngtn> Yup, that also correlates with where we are in find_internal

[17:23] * dogbert11 is starting to believe that this bug is in BIG trouble :)

[17:24] <jnthnwrthngtn> oh duh

[17:24] <jnthnwrthngtn> This looks silly

[17:27] <jnthnwrthngtn> Have a probable fix, testing

[17:33] <Geth> ¬¶ MoarVM: 9380981185 | (Jonathan Worthington)++ | src/disp/resume.c

[17:33] <Geth> ¬¶ MoarVM: Fix thinko in inline dispatch resumption search

[17:33] <Geth> ¬¶ MoarVM: 

[17:33] <Geth> ¬¶ MoarVM: We are doing a forward search through the resumptions (because they were

[17:33] <Geth> ¬¶ MoarVM: emitted in reverse - most nested dispatcher first - already at dispatch

[17:33] <Geth> ¬¶ MoarVM: program compilation time). Thus the index should be incremented to reach

[17:33] <Geth> ¬¶ MoarVM: the end, not decremented. Could lead to out-of-bounds reads or just an

[17:33] <Geth> ¬¶ MoarVM: attempt to resume the wrong dispatcher depending on how it came up.

[17:33] <Geth> ¬¶ MoarVM: review: https://github.com/MoarVM/MoarVM/commit/9380981185

[17:38] <jnthnwrthngtn> I really don't understand why CI is such a mess on https://github.com/MoarVM/MoarVM/pull/1590 but I can't produce any of these issues locally

[17:40] <lizmat> jnthnwrthngtn: feels like it's time for a bump

[17:42] <MasterDuke> aw. ^^^ fix doesn't also fix my lexical not found on my jit branch

[17:42] <jnthnwrthngtn> Does anybody confirm it fixes the problem on their machine?

[17:42] <MasterDuke> does for me (the complex example)

[17:42] <jnthnwrthngtn> OK, good

[17:45] <dogbert11> jnthnwrthngtn++

[17:46] <jnthnwrthngtn> lizmat: Given it seems the fix is confirmed to help, can do

[17:46] <lizmat> ok, will do

[17:46] <jnthnwrthngtn> If anybody would be able to try and reproduce the Rakudo make test failures with 1590 it'd be appreciated. I can't at all.

[17:47] <dogbert11> fix works splendidly

[17:47] <jnthnwrthngtn> (Build the MoarVM branch new-special-return-only; you'll need to rebuild Rakudo also due to extops changes)

[17:48] <dogbert11> will do

[17:56] <MasterDuke> t/02-rakudo/08-slangs.t ......................................... Dubious, test returned 255 (wstat 65280, 0xff00)\

[17:57] <MasterDuke> but i had to have a spectest running at the same time

[17:57] <jnthnwrthngtn> MasterDuke: OK, at least somebody can repro something

[17:57] <jnthnwrthngtn> With spesh blocking + a small nursery I managed to get one spectest to fail

[17:58] <jnthnwrthngtn> aha, and the small nursery is significant

[18:02] *** reportable6 left
[18:31] <jnthnwrthngtn> OK, this failure is seemingly to do with finalization

[18:31] <jnthnwrthngtn> I'm not really sure what makes the timing of it unfortunate enough to cause bother

[18:39] <dogbert11> perhaps a good dinner might clarify things?

[19:11] <jnthnwrthngtn> Yeah, cooking a dal at the moment

[19:18] <MasterDuke> now this is interesting. on my branch where i'm jitting ctx(caller|outer)(skipthunks) and i get tons of those `Frame has no lexical with name ...` because of ctxcallerskipthunks, a test and spectest both pass if i disable the expr jit. even though none of those four ops have templates!

[19:19] <MasterDuke> so yeah, it seems like the jitting of ctxcallerskipthunks is uncovering some other problem

[19:28] <dogbert11> interesting, the new-special-return-only branch has tests failing with the other dispatchy error, the one nine unfortunately didn't manage to repro

[19:29] <dogbert11> using an 8k nursery, t/spec/S06-advanced/wrap.rakudo.moar and t/spec/S12-methods/defer-next.t fail with 'Can only use manipulate a capture known in this dispatch'

[19:32] <MasterDuke> ah, the jitting of ctx(caller|outer)(skipthunks) causes problems even if i just cherry-pick that commit onto master. new error now though, `No such method 'exception' for invocant of type 'X::AdHoc'`

[19:40] <lizmat> hmmmm... for the first time in a long time, I got a hang in t/spec/S17-promise/nonblocking-await.t 

[19:40] <lizmat> (this is on master everything)

[20:03] *** reportable6 joined
[20:12] *** sena_kun left
[20:16] *** sena_kun joined
[21:06] <Geth> ¬¶ MoarVM/new-special-return: 5dd58a5db3 | (Jonathan Worthington)++ | src/gc/finalize.c

[21:06] <Geth> ¬¶ MoarVM/new-special-return: Don't lose exception handler results

[21:06] <Geth> ¬¶ MoarVM/new-special-return: 

[21:06] <Geth> ¬¶ MoarVM/new-special-return: Invoking finalizers could end up replacing tc->last_handler_result,

[21:06] <Geth> ¬¶ MoarVM/new-special-return: leading to missing or incorrect values being observed for that after

[21:06] <Geth> ¬¶ MoarVM/new-special-return: finalizers had run. Don't run finalizers when there is such a risk.

[21:06] <Geth> ¬¶ MoarVM/new-special-return: review: https://github.com/MoarVM/MoarVM/commit/5dd58a5db3

[21:10] <jnthnwrthngtn> Here's hoping that might fix the CI failures I couldn't reproduce.

[21:16] <dogbert11> how does that branch differ from new-special-return-only

[21:23] <nine> new-special-return-only is only a part of the branch with the purpose of narrowing down the changes that cause the failure

[21:24] <nine> jnthnwrthngtn: looking at that commit I wonder if that problem could have appeared before as well? Or manifested in a different way (like not calling the finalizer when there's a last_handler_result)

[21:26] <dogbert11> nine: new-special-return-only contains errors but perhaps jnthnwrthngtn fixed those in the other branch

[21:27] <jnthnwrthngtn> nine: I did wonder the same; I don't know why the previous approach wouldn't have been vulnerable to the same issue

[21:28] <jnthnwrthngtn> Sigh. The PR still fails. With things I can't reproduce.

[21:28] <jnthnwrthngtn> Not sure what to do about that. 

[21:30] <Geth> ¬¶ MoarVM/new-special-return-only: 733dbd3293 | (Jonathan Worthington)++ | src/gc/finalize.c

[21:30] <Geth> ¬¶ MoarVM/new-special-return-only: Don't lose exception handler results

[21:30] <Geth> ¬¶ MoarVM/new-special-return-only: 

[21:30] <Geth> ¬¶ MoarVM/new-special-return-only: Invoking finalizers could end up replacing tc->last_handler_result,

[21:30] <Geth> ¬¶ MoarVM/new-special-return-only: leading to missing or incorrect values being observed for that after

[21:30] <Geth> ¬¶ MoarVM/new-special-return-only: finalizers had run. Don't run finalizers when there is such a risk.

[21:30] <Geth> ¬¶ MoarVM/new-special-return-only: review: https://github.com/MoarVM/MoarVM/commit/733dbd3293

[21:30] <jnthnwrthngtn> Let's see how things look in this one also

[21:32] <Geth> ¬¶ MoarVM/new-special-return: c77a89fae6 | (Jonathan Worthington)++ | src/core/callstack.c

[21:32] <Geth> ¬¶ MoarVM/new-special-return: Don't run finalizers at all

[21:32] <Geth> ¬¶ MoarVM/new-special-return: 

[21:32] <Geth> ¬¶ MoarVM/new-special-return: To see if that is to blame for the `make test` failures

[21:32] <Geth> ¬¶ MoarVM/new-special-return: review: https://github.com/MoarVM/MoarVM/commit/c77a89fae6

[21:33] <jnthnwrthngtn> On the upside, https://github.com/MoarVM/MoarVM/pull/1589 has passed CI

[21:39] <dogbert11> the 'Can only use manipulate a capture known in this dispatch' problem is still present after applyiong your latest commit

[21:40] <jnthnwrthngtn> dogbert11: That exists on master too, yes?

[21:41] <dogbert11> yes, but it has been very difficult to repro there the last few days

[21:42] <dogbert11> running './rakudo-gdb-m -Ilib t/spec/S06-advanced/wrap.rakudo.moar' with MVM_GC_DEBUG=3 leads to a SEGV

[21:42] <jnthnwrthngtn> Which branch?

[21:43] <dogbert11> new-special-return-only

[21:43] <jnthnwrthngtn> And with default settings, or a small nursery?

[21:43] <jnthnwrthngtn> Well, other than the one you mentioned

[21:43] <dogbert11> the only changes are MVM_GC_DEBUG=3 and building with --no-optimize

[21:44] <dogbert11> #0  log_parameter (tc=0x55555555ae30, cid=192924, arg_idx=0, param=0x55555563f5e8) at src/spesh/log.c:95

[21:44] <dogbert11> #1  0x00007ffff79449cd in MVM_spesh_log_entry (tc=0x55555555ae30, cid=192924, sf=0x5555579948b0, args=...) at src/spesh/log.c:130

[21:44] <dogbert11> #2  0x00007ffff7829b5a in MVM_frame_dispatch (tc=0x55555555ae30, code=0x5555579edfb8, args=..., spesh_cand=-1) at src/core/frame.c:552

[21:44] <dogbert11> #3  0x00007ffff790df5f in run_resume (tc=0x55555555ae30, record=0x7ffff722da38, disp=0x555557a89890, capture=0x55555563f5e8, thunked=0x7fffffffc048) at src/disp/program.c:618

[21:44] <dogbert11> #4  0x00007ffff7917158 in MVM_disp_program_record_end (tc=0x55555555ae30, record=0x7ffff722da38, thunked=0x7fffffffc048) at src/disp/program.c:2793

[21:46] <dogbert11> running with that flag set to 3 is slooow

[21:47] <jnthnwrthngtn> uh, yes, I'm still waiting to see if I can repro it

[21:47] *** linkable6 left
[21:47] *** evalable6 left
[21:47] <dogbert11> would be nice, it's an elusive beast

[21:49] <dogbert11> to be fair, I had the nursery set to 20k but I'm under the impression that this value is ignored when MVM_GC_DEBUG=3

[21:49] <dogbert11> could I be wrong about that perhaps

[21:51] <jnthnwrthngtn> No, I think it collects every allocation

[21:52] <dogbert11> yeah, but perhaps it tries to move nurseries around every allocation ?

[21:53] <dogbert11> four megs is more than 20k but perhaps I'm spouting nonsense here

[21:53] <jnthnwrthngtn> OK, https://github.com/MoarVM/MoarVM/pull/1581 fails even without running any finalizers

[21:55] <dogbert11> and using a 20k nursery make the program run faster than a four meg nursery when MVM_GC_DEBUG=3, at least on my system

[21:55] <jnthnwrthngtn>     # expected a match with: /'(1 2 3 4 5 6 7 8 9 10)' .* 'test is good'/

[21:55] <jnthnwrthngtn>     #                   got: "(1 2 3 4 5 6 7 8 9 10)\nUnhandled lexical type 'num32' in lexprimspec\n\n"

[21:55] <dogbert11> oh, you got something

[21:55] <jnthnwrthngtn> On CI

[21:55] <dogbert11> ah

[21:57] <jnthnwrthngtn> OK, here goes with adding one commit at a time to the PR to see where it breaks...

[21:58] <dogbert11> well the SEGV show up cinsistently on my system

[21:58] <jnthnwrthngtn> That one is still running yet

[21:58] <jnthnwrthngtn> ah

[21:59] <jnthnwrthngtn> yes, just got the segv

[21:59] <dogbert11> yessssssss :)

[21:59] <jnthnwrthngtn> (gdb) p *param

[21:59] <jnthnwrthngtn> $2 = {header = {sc_forward_u = {forwarder = 0xefefefefefefefef, sc = {sc_idx = 4025479151, 

[21:59] <jnthnwrthngtn>         idx = 4025479151}, st = 0xefefefefefefefef}, owner = 4025479151, flags1 = 239 '\357', 

[21:59] <jnthnwrthngtn>     flags2 = 239 '\357', size = 61423}, st = 0xefefefefefefefef}

[22:00] <jnthnwrthngtn> Well, that's not looking good.

[22:00] <dogbert11> 0xefefefefefefefef looks suspicious

[22:01] <dogbert11> src/gc/orchestrate.c-#if MVM_GC_DEBUG >= 3

[22:01] <dogbert11> src/gc/orchestrate.c:                memset(other->nursery_fromspace, 0xef, other->nursery_fromspace_size);

[22:01] <dogbert11> src/gc/orchestrate.c-#endif

[22:02] <MasterDuke> also in src/core/threadcontext.c

[22:02] <dogbert11> but this probably explains why running with a 20k nursery is faster

[22:21] <jnthnwrthngtn> Indeed, it is rather faster that way

[22:21] <jnthnwrthngtn> dogbert11: You could reproduce this on master also, yes?

[22:25] <jnthnwrthngtn> Uhh. https://github.com/MoarVM/MoarVM/pull/1590 has presently only one commit that adds what should be only dead code. And...fails CI too

[22:26] <jnthnwrthngtn> oh, it's a git checkout failure...that ain't my code :)

[22:26] <jnthnwrthngtn> Let's re-run that one.

[22:26] <lizmat> .oO( it's alive, but not as we know it! )

[22:26] <MasterDuke> just a git error

[22:27] <Geth> ¬¶ MoarVM: 4d145b4651 | (Jonathan Worthington)++ | src/disp/program.c

[22:27] <Geth> ¬¶ MoarVM: Add missing GC mark of resume capture

[22:27] <Geth> ¬¶ MoarVM: review: https://github.com/MoarVM/MoarVM/commit/4d145b4651

[22:27] <jnthnwrthngtn> dogbert11: ^^ fixes it

[22:28] <jnthnwrthngtn> (Committed in master 'cus it's wrong there too...)

[22:30] <dogbert11> yay

[22:34] <jnthnwrthngtn> No matter how many times I click re-run on that git failed test, it doesn't re-run...

[22:35] <MasterDuke> i think it doesn't actually restart it until the others are finished

[22:35] *** sena_kun left
[22:36] <jnthnwrthngtn> ah, ok

[22:38] *** sena_kun joined
[22:39] <dogbert11> jnthnwrthngtn: the problem on master was reported as https://github.com/MoarVM/MoarVM/issues/1580

[22:39] <dogbert11> perhaps it can be closed

[22:40] <jnthnwrthngtn> Hm, that's a different (although very possibly related) failure mode to the one I just fixed

[22:42] <dogbert11> yeah, the stacktrace doesn't look quite the same but it did look the same if I ran the SEGV example with an 8k nursery and MVM_GC_DEBUG=1

[22:43] <dogbert11> I guess it could be two different bugs, let me investigate, I'll apply your latest master commit to new-special-return-only and give it a go

[22:43] <jnthnwrthngtn> MasterDuke: Yes, seems I can queue a re-run for the git failure now it did the others; thanks for the tip

[22:44] <MasterDuke> np. took me quite a while to realize that so figured i'd share the wealth

[22:50] *** linkable6 joined
[22:50] *** evalable6 joined
[22:54] *** sena_kun left
[22:56] <dogbert11> the 'Can only use manipulate a capture known in this dispatch' was either fixed by the latest commit or it has gone into hiding, can't repro any longer

[22:57] <Geth> ¬¶ MoarVM/new-special-return-only: 5dfbc68774 | (Jonathan Worthington)++ | src/core/loadbytecode.c

[22:57] <Geth> ¬¶ MoarVM/new-special-return-only: Migrate loadbytecode usage of special return

[22:57] <Geth> ¬¶ MoarVM/new-special-return-only: review: https://github.com/MoarVM/MoarVM/commit/5dfbc68774

[22:59] <jnthnwrthngtn> hmm...pushing that doesn't seem to have triggered the azure checks...

[23:00] <Geth> ¬¶ MoarVM/new-special-return-only: 9b499f94fe | (Jonathan Worthington)++ | src/core/loadbytecode.c

[23:00] <Geth> ¬¶ MoarVM/new-special-return-only: Migrate loadbytecode usage of special return

[23:00] <Geth> ¬¶ MoarVM/new-special-return-only: review: https://github.com/MoarVM/MoarVM/commit/9b499f94fe

[23:00] <MasterDuke> it looks like github might be having problems

[23:00] <jnthnwrthngtn> Oh

[23:01] <jnthnwrthngtn> I wondered if it was somehow trying to optimize it out because it saw that hash sometime before now...

[23:01] <jnthnwrthngtn> (What I just pushed is just ammending it with a new date so it's another sha-1)

[23:01] <jnthnwrthngtn> Which has immediately triggered the CI

[23:06] <jnthnwrthngtn> I'm tired, will continue trying to CI-bissect this tomorrow, or if my office machine will repro it maybe.

[23:36] *** sena_kun joined
[23:53] *** sena_kun left
[23:57] *** squashable6 left
[23:58] *** squashable6 joined
