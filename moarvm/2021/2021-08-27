[00:02] *** reportable6 left
[00:04] *** reportable6 joined
[01:04] *** linkable6 left
[01:04] *** evalable6 left
[01:04] *** evalable6 joined
[02:05] *** linkable6 joined
[03:15] <timo> in inline cache we only free inline cache entries by freeing them from the fsa, but we dont look into the entries to get at the dp themselves to free them

[03:15] <timo> so that is where the fix for the leak probably lies

[03:28] <timo> oh thats not what cleanup entry does i guess

[03:44] <Geth> ¦ MoarVM/new-disp: 88081523a4 | (Timo Paulssen)++ | src/disp/inline_cache.c

[03:44] <Geth> ¦ MoarVM/new-disp: destroy dispatch programs from inline entries when destroying static frames

[03:44] <Geth> ¦ MoarVM/new-disp: review: https://github.com/MoarVM/MoarVM/commit/88081523a4

[03:46] <timo> this reduces the amount of leaked allocations from dispatch programs

[04:40] *** linkable6 left
[04:40] *** evalable6 left
[04:41] *** linkable6 joined
[05:08] *** frost joined
[06:02] *** reportable6 left
[06:42] *** evalable6 joined
[07:04] *** reportable6 joined
[07:47] <Nicholas> good *, #moarvm 

[07:49] <Nicholas> jnthnwrthngtn: you suggested that new-disp could be more performant if the dispatch program "dispatcher" could use computed goto (where available). I had a quick look, but it's not obvious to me *which* switch statement in which file is the main dispatch loop that this would be for. Could you drop a hint?

[07:49] <Nicholas> (also, not promising that I'll get to this, but I figured that if I can't spot it, someone else possibly can't either)

[07:52] <MasterDuke> Nicholas: i think it's here https://github.com/MoarVM/MoarVM/blob/new-disp/src/disp/program.c#L2633 but don't take my word for it

[07:53] <MasterDuke> timo++

[07:53] <MasterDuke> before: definitely lost: 355,776 bytes in 6,286 blocks and indirectly lost: 1,267,760 bytes in 19,160 blocks

[07:54] <MasterDuke> after: definitely lost: 26,832 bytes in 412 blocks and indirectly lost: 84,112 bytes in 1,256 blocks

[07:54] <MasterDuke> invalid frees are still there

[08:24] <Geth> ¦ MoarVM: bb14f42376 | (Stefan Seifert)++ | src/gc/finalize.c

[08:24] <Geth> ¦ MoarVM: Fix finalizers of gen2 objects rarely getting run

[08:24] <Geth> ¦ MoarVM: 

[08:24] <Geth> ¦ MoarVM: After garbage collection we go through the list of objects with finalizers (the

[08:24] <Geth> ¦ MoarVM: finalize list) and move objects that were collected to the list of finalizers

[08:24] <Geth> ¦ MoarVM: to run. We rewrite the finalize list as we walk through it, in order to only

[08:24] <Geth> ¦ MoarVM: keep live objects. However during a nursery-only collection we only considered

[08:24] <Geth> ¦ MoarVM: nursery objects. Objects that were already in gen2 were not moved to the

[08:24] <Geth> ¦ MoarVM: collapsed finalize list. Thus we never ran those object's finalizers.

[08:24] <Geth> ¦ MoarVM: 

[08:24] <Geth> ¦ MoarVM: Fix by indiscriminately moving objects we don't have a closer look at to the

[08:24] <Geth> ¦ MoarVM: collapsed list.

[08:24] <Geth> ¦ MoarVM: review: https://github.com/MoarVM/MoarVM/commit/bb14f42376

[08:42] *** linkable6 left
[08:42] *** evalable6 left
[08:42] *** evalable6 joined
[08:57] *** Altai-man left
[09:22] <Geth> ¦ MoarVM/fixkey-zero-static: 104d32c16c | (Nicholas Clark)++ | 5 files

[09:22] <Geth> ¦ MoarVM/fixkey-zero-static: Change `permroot_descriptions` and related functions to `const char *`.

[09:22] <Geth> ¦ MoarVM/fixkey-zero-static: review: https://github.com/MoarVM/MoarVM/commit/104d32c16c

[09:22] <Geth> ¦ MoarVM/fixkey-zero-static: 3283717532 | (Nicholas Clark)++ | 3 files

[09:22] <Geth> ¦ MoarVM/fixkey-zero-static: `entry_size == 0` enables static storage in MVMFixKeyHashTable.

[09:22] <Geth> ¦ MoarVM/fixkey-zero-static: 

[09:22] <Geth> ¦ MoarVM/fixkey-zero-static: The hashtable internally stores just pointers, which indirect to the

[09:22] <Geth> ¦ MoarVM/fixkey-zero-static: stored structure, so that the stored structure remains at a fixed address

[09:22] <Geth> ¦ MoarVM/fixkey-zero-static: in memory.

[09:22] <Geth> ¦ MoarVM/fixkey-zero-static: 

[09:22] <Geth> ¦ MoarVM/fixkey-zero-static: The default was always to allocate storage with the Fixed Size Allocator.

[09:22] <Geth> ¦ MoarVM/fixkey-zero-static: <…commit message has 5 more lines…>

[09:22] <Geth> ¦ MoarVM/fixkey-zero-static: review: https://github.com/MoarVM/MoarVM/commit/3283717532

[09:22] <Geth> ¦ MoarVM/fixkey-zero-static: 277e3928c9 | (Nicholas Clark)++ | 4 files

[09:22] <Geth> ¦ MoarVM/fixkey-zero-static: Convert the system calls hash to MVMFixKeyHashTable.

[09:22] <Geth> ¦ MoarVM/fixkey-zero-static: 

[09:22] <Geth> ¦ MoarVM/fixkey-zero-static: MVMFixKeyHashTable can now handle indirecting to static storage. Hence add

[09:22] <Geth> ¦ MoarVM/fixkey-zero-static: `MVMString *name` to the start of `struct MVMDispSysCall` and store this in

[09:22] <Geth> ¦ MoarVM/fixkey-zero-static: an MVMFixKeyHashTable, instead of having a `struct MVMDispSysCallHashEntr`

[09:22] <Geth> ¦ MoarVM/fixkey-zero-static: which indirected to it, and was stored in an MVMStrHashTable.

[09:22] <Geth> ¦ MoarVM/fixkey-zero-static: 

[09:22] <Geth> ¦ MoarVM/fixkey-zero-static: As the key is now *in* the struct with a fixed storage address, use

[09:22] <Geth> ¦ MoarVM/fixkey-zero-static: `MVM_gc_root_add_permanent_desc` to mark it for GC. Hence we no longer need

[09:22] <Geth> ¦ MoarVM/fixkey-zero-static: to iterate over the hash on each GC run adding the keys to the worklist.

[09:22] <Geth> ¦ MoarVM/fixkey-zero-static: review: https://github.com/MoarVM/MoarVM/commit/277e3928c9

[09:31] *** Altai-man joined
[09:35] <Geth> ¦ MoarVM: nwc10++ created pull request #1531: Convert the systems calls hash to MVMFixKeyHashTable

[09:35] <Geth> ¦ MoarVM: review: https://github.com/MoarVM/MoarVM/pull/1531

[10:07] *** Kaiepi left
[10:17] <jnthnwrthngtn> Nicholas: The one MasterDuke pointed to is indeed the correct one

[10:20] <timo> i see some good commits

[10:27] <nine> If anyone else is wondering what computed gotos actually are and why they are faster than switch statements despite all the magic compilers can do, this is an excellent read: https://eli.thegreenplace.net/2012/07/12/computed-goto-for-efficient-dispatch-tables

[10:32] *** Kaiepi joined
[10:33] <timo> ah yes, having a separate entry in the branch prediction table for every instruction in the interpreter can be a big win

[10:34] <timo> and jitting can be an even bigger win here :p

[10:44] <jnthnwrthngtn> It can, though I'm not sure if it's worth JITting dispatch programs "alone"; need to see if there is significant time spent in them post-spesh

[10:44] *** linkable6 joined
[10:47] <jnthnwrthngtn> Sigh, the anaesthetic from this morning's dentist trip seems to have numbed my brain too...

[10:47] <jnthnwrthngtn> Or maybe that's just because I didn't have any coffee today

[10:53] <nine> The coffee situation sounds fixable, so I guess it's worth a try :)

[10:57] <jnthnwrthngtn> Nicholas: I think the hash PR for syscalls is alright, though I left a note about a potential downside

[10:57] <jnthnwrthngtn> (That wasn't one that you mentioned)

[10:57] <jnthnwrthngtn> Though I'm still inclined to think the upsides are better

[11:01] <jnthnwrthngtn> The other mildly "amusing" thing that happened this morning is that I figured I'd sit on the terrace for a bit to try and relax before going to the dentist (I get really anxious about such things). Except there was some construction work happening across the street, so all I heard the whole time was drilling.

[11:02] <nine> Well, compared to those drills...

[11:03] <timo> at least theer wasnt a jackhammer going

[11:03] <jnthnwrthngtn> Uff, yes.

[11:04] <jnthnwrthngtn> There's like 3 buildings near mine all undergoing reconstruction at the moment (2 new roofs, 1 is gaining an extra floor and thus a new roof also).

[11:05] <timo> sometimes teeth get new roofs as well

[11:07] <jnthnwrthngtn> Sigh, just about everything that needs doing with new-disp right now requires thinking hard. :)

[11:08] <jnthnwrthngtn> 1) spesh of dispatch programs that set resumption init state, 2) Figure out istype's future (it'll stay, but what's behind it wants to change), 3) the remaining spectest fails, but aside from the code-gen problem that causes the hang, none of it looks like easy debugging.

[11:09] <nine> So, attack that code-gen bug?

[11:09] <jnthnwrthngtn> I was sorta hoping somebody else might pick that relative LHF-ish :)

[11:09] <timo> difficult to figure out how to free the dispatch programs properly? might need to put them in the fsa as well so the safepoint mechanism can be used for their bodies and ops

[11:09] <nine> It is a bit bothersome anyway. I removed that test file locally, to get rid of the hangs in spectest

[11:09] <nine> jnthnwrthngtn: oh, in that case, what do you know about the bug?

[11:10] <jnthnwrthngtn> nine: The QAST compiler seems to fail to spit out a coercion op

[11:10] <jnthnwrthngtn> nine: So we read an uninitialized register

[11:11] <jnthnwrthngtn> If you dump the bytecode of the thing that hangs, the location of the missing instruction is obvious enough.

[11:12] <jnthnwrthngtn> lunch, bbiab

[11:18] *** frost left
[11:36] <nine> jnthnwrthngtn: ha! If I had known that the missing instruction is _that_ obvious

[11:41] <Geth> ¦ MoarVM/new-disp: 104d32c16c | (Nicholas Clark)++ | 5 files

[11:41] <Geth> ¦ MoarVM/new-disp: Change `permroot_descriptions` and related functions to `const char *`.

[11:41] <Geth> ¦ MoarVM/new-disp: review: https://github.com/MoarVM/MoarVM/commit/104d32c16c

[11:41] <Geth> ¦ MoarVM/new-disp: 3283717532 | (Nicholas Clark)++ | 3 files

[11:41] <Geth> ¦ MoarVM/new-disp: `entry_size == 0` enables static storage in MVMFixKeyHashTable.

[11:41] <Geth> ¦ MoarVM/new-disp: 

[11:41] <Geth> ¦ MoarVM/new-disp: The hashtable internally stores just pointers, which indirect to the

[11:41] <Geth> ¦ MoarVM/new-disp: stored structure, so that the stored structure remains at a fixed address

[11:41] <Geth> ¦ MoarVM/new-disp: in memory.

[11:41] <Geth> ¦ MoarVM/new-disp: 

[11:41] <Geth> ¦ MoarVM/new-disp: The default was always to allocate storage with the Fixed Size Allocator.

[11:41] <Geth> ¦ MoarVM/new-disp: <…commit message has 5 more lines…>

[11:41] <Geth> ¦ MoarVM/new-disp: review: https://github.com/MoarVM/MoarVM/commit/3283717532

[11:41] <Geth> ¦ MoarVM/new-disp: 277e3928c9 | (Nicholas Clark)++ | 4 files

[11:41] <Geth> ¦ MoarVM/new-disp: Convert the system calls hash to MVMFixKeyHashTable.

[11:42] <Geth> ¦ MoarVM/new-disp: 

[11:42] <Geth> ¦ MoarVM/new-disp: MVMFixKeyHashTable can now handle indirecting to static storage. Hence add

[11:42] <Geth> ¦ MoarVM/new-disp: `MVMString *name` to the start of `struct MVMDispSysCall` and store this in

[11:42] <Geth> ¦ MoarVM/new-disp: an MVMFixKeyHashTable, instead of having a `struct MVMDispSysCallHashEntr`

[11:42] <Geth> ¦ MoarVM/new-disp: which indirected to it, and was stored in an MVMStrHashTable.

[11:42] <Geth> ¦ MoarVM/new-disp: 

[11:42] <Geth> ¦ MoarVM/new-disp: As the key is now *in* the struct with a fixed storage address, use

[11:42] <Geth> ¦ MoarVM/new-disp: `MVM_gc_root_add_permanent_desc` to mark it for GC. Hence we no longer need

[11:42] <Geth> ¦ MoarVM/new-disp: to iterate over the hash on each GC run adding the keys to the worklist.

[11:42] <Geth> ¦ MoarVM/new-disp: review: https://github.com/MoarVM/MoarVM/commit/277e3928c9

[11:53] <nine> Wow....can it really be that all this time we missed an MVMROOT of the result register during nativecall callbacks?

[12:02] *** reportable6 left
[12:04] *** reportable6 joined
[12:06] * lizmat would not be surprised ?

[12:09] <nine> Of course, when I try to provoke it so I can test my fix with confidence, nothing explodes

[12:13] <lizmat> sneaky

[12:15] <nine> Ok, I can in the full blown apllication. Just not in any reduced test case

[12:32] * jnthnwrthngtn back

[12:33] <jnthnwrthngtn> With coffee, now I can feel my mouth again :)

[12:34] <jnthnwrthngtn> nine: I've no idea how obvious it would be to fix said missing instruction... :)

[12:36] *** sena_kun joined
[12:46] <nine> I think is_full_collection still needs some more tuning. As it is, any long running process with varying objects in the working set that live long enough to make it into gen2 will grow without bounds even if the working set does have a limited size.

[12:47] <nine> That's why ironically, sprinkling nqp::force_gc into your loops can make memory consumption worse rather than better.

[12:48] <jnthnwrthngtn> What do you mean by "varying objects"?

[12:48] <nine> What happens is: we have a steady flow of objects that make it into gen2 and could die a while later. On every GC run we look at promoted size. If it exceeds 20M we look at the process' RSS. If that has grown by more than 20 % since the last full collection, we trigger another one.

[12:49] <nine> Next time around our baseline RSS to compare with is larger by ~ 20 %. So we wait longer till we trigger a full collection. Next time 20 % more again, and so on.

[12:50] <nine> What I mean is if (part of) the objects your application allocates usually live long enough to make it into gen2, but could be collected at some point.

[12:51] <nine> We're fine if all objects are very short lived and those which make it into gen2 live for the whole life time of the process anyway. Objects with medium life times will lead to unbounded process growth.

[12:52] <nine> And there's an even worse case: RSS only counts physical memory. It doesn't include swap. So precisely in those situations where you've already gobbled up all available RAM and are swapping, we don't see any RSS growth at all and will never trigger a full collection anymore.

[12:52] <lizmat> that would explain the memory growth I'm seeing on the irc logs server

[12:53] <lizmat> lots of big objects that possibly didn't make it to the nursery to begin with

[12:53] <lizmat> that are potentially short lived

[12:59] <nine> Yesterday's trivial example I used to find the finalizer issue demonstrated it quite well. It quickly grew to multiple gigabytes until I added a nursery-runs counter to the GC and had it do a full collection after at most 100 nursery collections.

[12:59] <jnthnwrthngtn> nine: Yeah, sounds like it could do with a different approach to avoid that kind of effect. It *is* a good strategy for programs that build up an increasingly large data structure as they work (compilation is such an example)

[13:00] <lizmat> reminds me of a "use less 'memory'" pragma  :-)

[13:00] <lizmat> afk for a few hours&

[13:00] <jnthnwrthngtn> Wonder if we can cheaply factor it how much the GC collected

[13:01] <jnthnwrthngtn> (in a full collection

[13:01] <jnthnwrthngtn> )

[13:01] <jnthnwrthngtn> And use the discrepancy between promoted and collected to decide on whether to increase the ceiling

[13:03] <nine> Yes, I figured we could use something along those lines. But after the 13 hour session yesterday and today's continued bug hunting, I'm not smart enough anymore to put those thoughts into an algorithm

[13:05] <nine> But then it has an incredibly productive week so far :) With the finalizer fix, the callback GC issue fix and https://github.com/niner/Inline-Perl5/commit/8aaec9c441319dd334cc9901acbf43251d9f0d6d our application has become stable and memory usage is fine

[13:06] <nine> That Inline::Perl5 issue was unexpected reentrancy caused by finalizers and has plagued us for 2 years as it defied every attempt to reproduce. Until yesterday, when I worked on the memory leak.

[13:07] <Geth> ¦ MoarVM: 7a385a3506 | (Stefan Seifert)++ | 2 files

[13:07] <Geth> ¦ MoarVM: Fix possible access to fromspace in NativeCall callbacks

[13:07] <Geth> ¦ MoarVM: 

[13:07] <Geth> ¦ MoarVM: An untimely garbage collection between setting the result object in the

[13:07] <Geth> ¦ MoarVM: callback and unmarshalling of that result object could lead to an outdated

[13:07] <Geth> ¦ MoarVM: pointer in res.o and further to segfaults and other nastiness.

[13:07] <Geth> ¦ MoarVM: Fix by adding res.o to the roots. Callbacks always return objects, so no

[13:07] <Geth> ¦ MoarVM: no special handling of primitive types necessary.

[13:07] <Geth> ¦ MoarVM: review: https://github.com/MoarVM/MoarVM/commit/7a385a3506

[13:23] <jnthnwrthngtn> nine++ # wish I could say the same for my week...

[13:24] <dogbert11> jnthnwrthngtn: given that there's still work to be done on new-disp is it reasonable that this tiny program, https://gist.github.com/dogbert17/9cb16b7a768daae1382fffab72b274de takes three times as long to run on new-disp than on master=

[13:25] <jnthnwrthngtn> Yes

[13:26] <jnthnwrthngtn> Inlining is very very very very very important. :)

[13:26] <dogbert11> thanks :)

[13:41] <dogbert11> what could this be - lang-call cannot invoke object of type 'VMNull' belonging to no language

[13:44] <jnthnwrthngtn> Something tried to invoke a null object. A golf of that could be interesting.

[13:45] <dogbert11> so far it only appears with a small (64k) nursery when running /spec/S32-exceptions/misc.t

[13:46] <jnthnwrthngtn> Goodness, the profiler producing the SQL is slow

[13:46] *** rakuUser joined
[15:48] *** nebuchadnezzar left
[16:22] <Geth> ¦ MoarVM/new-disp: 074a3c43e8 | (Jonathan Worthington)++ | 8 files

[16:22] <Geth> ¦ MoarVM/new-disp: Prepare for translating dispatch resume inits

[16:22] <Geth> ¦ MoarVM/new-disp: 

[16:22] <Geth> ¦ MoarVM/new-disp: These are cases where a dispatch wishes to keep state around in case

[16:22] <Geth> ¦ MoarVM/new-disp: there is a resumption later - that is, every Rakudo method and multi

[16:22] <Geth> ¦ MoarVM/new-disp: dispatch. The aim is to minimize runtime cost in the case that a

[16:22] <Geth> ¦ MoarVM/new-disp: resumption never happens, which is the overwhelmingly common case.

[16:22] <Geth> ¦ MoarVM/new-disp: 

[16:22] <Geth> ¦ MoarVM/new-disp: <…commit message has 15 more lines…>

[16:22] <Geth> ¦ MoarVM/new-disp: review: https://github.com/MoarVM/MoarVM/commit/074a3c43e8

[16:33] <dogbert11> here's another golf, for me it fails with a compilation error

[16:34] <dogbert11> MVM_SPESH_NODELAY=1 MVM_SPESH_BLOCKING=1 ./rakudo-m -e 'my $foo is default(Nil) = 42; for 41,42,43 -> $a, $b, $c { my $foo is default(Nil) = 42; }'

[16:34] <dogbert11> hopefully someone can repro

[16:43] <jnthnwrthngtn> yes, can repro it

[16:44] <jnthnwrthngtn> Is this relatively new, or around for a while?

[16:46] <jnthnwrthngtn> The specialized frame breaking it is src/core.c/Variable.pm6:53

[16:46] <jnthnwrthngtn> Which fits the error

[16:50] <dogbert11> jnthnwrthngtn: atm I don't know how new it is, I can try to find out though.

[16:51] <dogbert11> I have seen several errors like this for a few days but this is the first golf

[16:53] <dogbert11> it *might* have something to do with nine's lang-hllize dispatch work but that is only a guess

[16:54] *** linkable6 left
[16:54] *** evalable6 left
[16:55] *** evalable6 joined
[16:56] *** linkable6 joined
[17:00] <jnthnwrthngtn> "Malformed UTF-8 near bytes 20 4c 8b at line 1 col 53"

[17:00] <jnthnwrthngtn> ?!

[17:00] <jnthnwrthngtn> But yeah, that's the thing nine++ was hunting

[17:02] <jnthnwrthngtn> heh

[17:02] <jnthnwrthngtn> P6opaque: no such attribute '$!descriptor' on type L\213V\030I\017\277B...

[17:39] <Geth> ¦ MoarVM/new-disp: 7566af62b2 | (Jonathan Worthington)++ | src/spesh/optimize.c

[17:39] <Geth> ¦ MoarVM/new-disp: Fix mis-optimization of reads from Scalars

[17:39] <Geth> ¦ MoarVM/new-disp: 

[17:39] <Geth> ¦ MoarVM/new-disp: Just having the type facts is not enough, we also need to know that it

[17:39] <Geth> ¦ MoarVM/new-disp: is concrete.

[17:39] <Geth> ¦ MoarVM/new-disp: 

[17:39] <Geth> ¦ MoarVM/new-disp: Surprisingly this hasn't come up before now, but did so when hllize

[17:39] <Geth> ¦ MoarVM/new-disp: became a dispatcher. It inserts a guard on the type only (but not on

[17:39] <Geth> ¦ MoarVM/new-disp: <…commit message has 6 more lines…>

[17:39] <Geth> ¦ MoarVM/new-disp: review: https://github.com/MoarVM/MoarVM/commit/7566af62b2

[17:41] <jnthnwrthngtn> nine: I think ^^ is a fix for the hllize thing you were hunting. In the end not actually a deopt bug; we did a deopt because we'd read "junk", and then the same junk thing was used to in getattribute afterwards.

[17:52] <jnthnwrthngtn> home time &

[17:53] *** sena_kun left
[18:03] *** reportable6 left
[18:03] *** reportable6 joined
[18:12] *** Altai-man left
[18:41] *** Altai-man joined
[19:57] *** MasterDuke47 joined
[19:58] <MasterDuke47> is --full-cleanup still supposed to work when we exit because of a compilation error?

[19:59] <MasterDuke47> `valgrind --leak-check=full raku --full-cleanup -e 'my $a = \c[LATIN]'` has a ton of leaks

[20:06] <jnthnwrthngtn> Today's discovery: if you profile CORE.setting build on new-disp you do get SQL output fine...you can read it and run the statements against a SQLite database to import it...BUT

[20:06] <jnthnwrthngtn> One of the lines is 160MB line!

[20:06] <jnthnwrthngtn> *long!

[20:06] <jnthnwrthngtn> And Java uses UTF-16 internally so that's a 320MB String that you pretty much can't avoid forming. 

[20:07] * jnthnwrthngtn wonders if we might be able to spread that rather long line over a few :)

[20:08] <jnthnwrthngtn> MasterDuke47: If it calls nqp::exit it's worth checking if that actually triggers a full cleanup or not

[20:10] <MasterDuke47> k, i'll see if i can track that down

[20:13] <MasterDuke47> .tell Nicholas here's a valgrind log of compiling CORE.e with --full-cleanup and it has a bunch of hash-related leaks https://gist.github.com/MasterDuke17/3afad8db74f523569fa667b1ec9fbb4d

[20:13] <tellable6> MasterDuke47, I'll pass your message to Nicholas

[20:17] <dogbert11> if anyone is interested in a SEGV just run: MVM_SPESH_NODELAY=1 MVM_SPESH_BLOCKING=1 ./rakudo-m -Ilib t/spec/S32-io/io-spec-win.t

[20:17] <MasterDuke47> jnthnwrthngtn: btw, the creation of very large profiles was mildly optimized, but kind of just to the point that it was actually able to do so in most cases instead of dying or producing something that couldn't be used

[20:19] <MasterDuke47> so i'm sure there are more optimizations to be found

[20:20] <MasterDuke47> dogbert11: on new-disp?

[20:20] <jnthnwrthngtn> Yeah, it takes longer to output the profile than it did to record it :)

[20:20] <dogbert11> MasterDuke47: yes

[20:21] <jnthnwrthngtn> dogbert11: Did you confirm the fix I pushed earlier, btw?

[20:21] <dogbert11> jnthnwrthngtn: yes indeed, it worked perfectly

[20:21] <dogbert11> Thread 2 "spesh optimizer" received signal SIGSEGV, Segmentation fault.

[20:22] <dogbert11> [Switching to Thread 0x7ffff6ef8700 (LWP 799641)]

[20:22] <dogbert11> annotate_inline_start_end (inlinee_last_bb=0x7ffff3f9e1c0, inlinee_last_bb=0x7ffff3f9e1c0, inline_boundary_handler=<optimized out>, idx=4, inlinee=0x7ffff3623fa0, inliner=0x4, tc=0x5555555f1670)

[20:22] <dogbert11>     at src/spesh/inline.c:1301

[20:22] <dogbert11> 1301     end_ann->next             = end_ann_target->annotations;

[20:23] <dogbert11> end_ann_target is NULL

[20:23] <MasterDuke47> `valgrind --leak-check=full ./install/bin/raku --full-cleanup -e 'use nqp; nqp::exit'` does indeed report a ton of leaks. should nqp::exit respect --full-cleanup?

[20:24] <MasterDuke47> i think i would expect it to

[20:27] <MasterDuke47> ah, nqp::exit is pretty simple. it's just `MVMint64 exit_code = GET_REG(cur_op, 0).i64; MVM_io_flush_standard_handles(tc); exit(exit_code);`

[20:28] <MasterDuke47> should it instead call MVM_vm_exit or MVM_vm_destroy_instance?

[20:30] <jnthnwrthngtn> MasterDuke47: Yes, but only if --full-cleanup is set, I think

[20:31] <jnthnwrthngtn> Probably MVM_vm_exit

[20:31] <jnthnwrthngtn> Maybe that already checks the full cleanup flag

[20:31] <MasterDuke47> it doesn't

[20:31] <jnthnwrthngtn> (Normally we don't want to try and clean up, the OS can do it so much faster.)

[20:31] <jnthnwrthngtn> (So it is only useful if doing leak checking)

[20:32] <MasterDuke47> MVM_vm_exit and MVM_vm_destroy_instance are only ever called from main.c which is where the --full-cleanup flag is checked for

[20:36] <MasterDuke47> should nqp::exit instead just exit the interpreter, and then let normal exiting happen (i.e., MVM_vm_exit or MVM_vm_destroy_instance)?

[20:40] <dogbert11> as far as the SEGV is concerned, it's older than https://github.com/MoarVM/MoarVM/commit/4ddb32cb70b751c193c93c9cfe98a7eca0f3d836

[20:43] <jnthnwrthngtn> MasterDuke47: I think it should do as it does today when it's not full cleanup, and if there is full cleanup call MVM_vm_destroy_instance(instance); before exit

[20:45] <MasterDuke47> aiui nothing outside of main.c knows whether --full-cleanup was used, so we'd have to put that information somewhere that interp.c can know about it

[20:48] <jnthnwrthngtn> Yes

[20:48] <jnthnwrthngtn> Flag on MVMInstance probably

[20:48] <MasterDuke47> k

[21:02] *** linkable6 left
[21:02] *** evalable6 left
[21:03] *** evalable6 joined
[21:04] *** linkable6 joined
[21:49] <Geth> ¦ MoarVM: MasterDuke17++ created pull request #1532: Respect --full-cleanup in nqp::exit

[21:49] <Geth> ¦ MoarVM: review: https://github.com/MoarVM/MoarVM/pull/1532

[21:58] *** MasterDuke47 left
[23:26] *** greppable6 left
[23:26] *** reportable6 left
[23:26] *** linkable6 left
[23:26] *** unicodable6 left
[23:26] *** committable6 left
[23:26] *** statisfiable6 left
[23:26] *** coverable6 left
[23:26] *** bisectable6 left
[23:26] *** squashable6 left
[23:26] *** tellable6 left
[23:26] *** sourceable6 left
[23:26] *** releasable6 left
[23:26] *** benchable6 left
[23:26] *** shareable6 left
[23:26] *** quotable6 left
[23:26] *** notable6 left
[23:26] *** bloatable6 left
[23:26] *** evalable6 left
[23:26] *** nativecallable6 left
[23:26] *** quotable6 joined
[23:26] *** statisfiable6 joined
[23:26] *** tellable6 joined
[23:27] *** bloatable6 joined
[23:28] *** coverable6 joined
[23:28] *** unicodable6 joined
[23:28] *** squashable6 joined
[23:28] *** releasable6 joined
[23:29] *** committable6 joined
[23:29] *** notable6 joined
