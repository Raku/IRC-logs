[00:02] *** reportable6 left
[00:05] *** reportable6 joined
[00:06] *** colemanx left
[00:07] *** colemanx joined
[00:48] *** patrickb left
[00:48] *** patrickz joined
[01:44] *** patrickz left
[03:04] *** bisectable6 left
[03:04] *** releasable6 left
[03:04] *** greppable6 left
[03:04] *** benchable6 left
[03:04] *** unicodable6 left
[03:04] *** sourceable6 left
[03:04] *** evalable6 left
[03:04] *** committable6 left
[03:04] *** linkable6 left
[03:04] *** coverable6 left
[03:04] *** reportable6 left
[03:04] *** quotable6 left
[03:04] *** nativecallable6 left
[03:04] *** notable6 left
[03:04] *** squashable6 left
[03:04] *** tellable6 left
[03:04] *** bloatable6 left
[03:04] *** statisfiable6 left
[03:04] *** shareable6 left
[03:05] *** quotable6 joined
[03:05] *** bloatable6 joined
[03:05] *** unicodable6 joined
[03:06] *** shareable6 joined
[03:06] *** squashable6 joined
[03:07] *** reportable6 joined
[03:07] *** committable6 joined
[03:07] *** statisfiable6 joined
[04:06] *** sourceable6 joined
[04:06] *** coverable6 joined
[04:07] *** tellable6 joined
[04:07] *** linkable6 joined
[04:07] *** evalable6 joined
[04:07] *** bisectable6 joined
[04:07] *** greppable6 joined
[04:07] *** notable6 joined
[04:57] *** frost joined
[05:05] *** releasable6 joined
[05:06] *** benchable6 joined
[06:02] *** reportable6 left
[06:03] *** squashable6 left
[06:05] *** reportable6 joined
[06:06] *** nativecallable6 joined
[08:01] *** squashable6 joined
[09:27] *** frost left
[12:02] *** reportable6 left
[12:12] <Geth> Â¦ MoarVM/new-disp-nativecall-libffi: 8 commits pushed by (Stefan Seifert)++, (Nicholas Clark)++

[12:12] <Geth> Â¦ MoarVM/new-disp-nativecall-libffi: ed71f3ca8f | Support JIT compilation of native calls with rw args

[12:12] <Geth> Â¦ MoarVM/new-disp-nativecall-libffi: aad646e3f1 | Support JIT compilation of native calls with rw args for libffi

[12:12] <Geth> Â¦ MoarVM/new-disp-nativecall-libffi: 28c3a560b5 | Fix NULL pointer results getting boxed after native calls

[12:12] <Geth> Â¦ MoarVM/new-disp-nativecall-libffi: d28b8284d6 | Fix NULL pointer results getting boxed after native calls for libffi

[12:12] <Geth> Â¦ MoarVM/new-disp-nativecall-libffi: 37028fd7dd | Log types of native routine's return values.

[12:12] <Geth> Â¦ MoarVM/new-disp-nativecall-libffi: 66b06afde5 | Support JIT compilation of native calls with VMArray arguments

[12:12] <Geth> Â¦ MoarVM/new-disp-nativecall-libffi: effc13f16d | Remove obsolete native call JIT implementation based on invoke protocol

[12:12] <Geth> Â¦ MoarVM/new-disp-nativecall-libffi: d38239d46c | Remove old invoke protocol

[12:12] <Geth> Â¦ MoarVM/new-disp-nativecall-libffi: review: https://github.com/MoarVM/MoarVM/compare/b210c7d7100a...d38239d46c3e

[12:39] <nine> Turns out on new-disp-nativecall the Inline::Perl5 segfaults/assertion errors disappear

[12:43] <lizmat> so maybe a push forward is more efficient than trying to fix the issue with the current setup?

[12:48] <nine> That's certainly tempting. But without knowing what the exact problem is, it's hard to decide whether the problem is really fixed by new-disp-nativecall or if it just goes into hiding again.

[13:00] <nine> The issue also goes away if I prohibit JIT compilation of sp_resumption. Of course that doesn't mean that sp_resumption is at fault as this could just stop the JIT from reaching the actually broken part. But then, in the affected frame, sp_resumption is only followed by sp_guardconc and sp_runbytecode_o.

[13:02] *** patrickb joined
[13:03] <nine> Prohibiting JIT of sp_runbytecode_o does _not_ fix the problem. And sp_guardconc is ooooold (JITed since 2014) and will appear in most JIT compiled frames. Would be surprising if a problem only appears now.

[13:04] <nine> So sp_resumption it is? Well yes, but how? All it actually does at runtime is write VMNull into a register. It is kinda hard to get this wrong.

[13:05] *** reportable6 joined
[13:06] <nine> And not just that, since we already have an op for writing a VMNull into a register (nqp::null), the actual implementation has been there since 2014 as well.

[13:09] <lizmat> hmmm  

[13:09] * lizmat assumes battery operated humming duck mode

[13:15] <nine> Now sp_resumption is a strange beast. If it only wrote that VMNull there wouldn't be a point of having this op in the first place. Its purpose seems more internal to spesh. It takes a variable number of operands with the apparent purpose of keeping spesh from eliminating them.

[13:22] <timo> yeah, it reserves a bit of space on the stack frame for use in resumption data

[13:22] <timo> like access to the original dispatch's arguments

[13:27] <nine> Btw. the "is built" feature is a 6.e thing and thus not yet available for use code, isn't it?

[13:28] <nine> The docs don't mention this

[13:33] <timo> i don't know

[13:34] <nine> Well it got introduced in 2020, so can't be in 6.c or 6.d

[13:35] <lizmat> is built works everywhere, afaik

[13:36] <lizmat> it's not versioned afaik

[13:36] <lizmat> afk&

[13:36] <jnthnwrthngtn> nine: See src/core/oplist, which has an explanation of sp_resumption just above the op definition

[13:37] <jnthnwrthngtn> The purpose is partly "keep spesh from eliminating them", but also to make sure we can recover those registers in the event of a resumption

[13:41] <nine> lizmat: but the only way to tell the system that I need a compiler feature is to state a minimum language version. There are 6.d compiler versions without "is built" support, so I'd have to state 6.e, which doesn't exist yet.

[13:45] <nine> Another interesting fact: the error goes away when I remove the local patch that speeds up the objectkeeper by using an IterationBuffer instead of array: https://gist.github.com/niner/23eedda15d16c703546e17a20fc7c19c

[13:48] <nine> Now the ObjectKeeper's .free method is involved as it's called by the broken frame. What's really strange though is that that broken frame (found by spesh bisecting) is not what I get for tc->cur_frame. And the assertion failure happens ins getlexstatic_o which is not in use in that frame

[13:51] <jnthnwrthngtn> Does the JITted machine code correspond to the frame?

[13:53] <nine> That's the thing.... though the errors go away (absolutely reliably) when I disable the JIT, they do not actually occur in JITed frames.

[13:55] <jnthnwrthngtn> Is there a deopt from a JITted frame just before the issue?

[13:55] <timo> does introducing IterationBuffer as a "dependency" to the serialization context change anything?

[13:56] <timo> does rr's chaos mode do anything interesting?

[13:57] <nine> jnthnwrthngtn: there are different failure modes. The "Internal error: Unwound entire stack and missed handler" one does make some sense though. It happens when a nested runloop executes a return_o. This goes via MVM_frame_try_return/MVM_callstack_unwind_frame/unwind_after_handler/MVM_frame_unwind_to to MVM_callstack_unwind_frame which returns 0 due to the MVM_CALLSTACK_RECORD_NESTED_RUNLOOP entry on the 

[13:57] <nine> callstack, leading to the error message

[13:58] <nine> I don't see any relevant deopts

[14:03] <nine> The weird thing about this is that it's trying to unwind to command_eval. Definitely not the right target for the return

[14:08] <nine> Aha, there's an exception and it's "Attempt to read past end of string heap when locating string"

[14:08] <nine> So just another symptom of some general screw up

[14:09] <nine> timo: the program is non-threaded (and I'm running with MVM_SPESH_BLOCKING=1), so chaos mode probably won't show anything interesting.

[14:09] <timo> ah, dang

[14:11] <nine> Smaller nursery makes it appear sooner. Still in a native callback though

[14:12] <timo> hm, i wonder if we need to introduce optional redzones in more places for use in --valgrind

[14:12] <timo> maybe something's exploding for some reason like that and isn't getting caught because reasons

[14:15] <nine> And with a 4K nursery I can reproduce it even on new-disp-nativecall, so no, can't just storm ahead on this :(

[14:15] <nine> But still no joy reproducing it without JIT

[14:17] <jnthnwrthngtn> nine: Hm, that'd imply that there's an unhandled exception in a callback?

[14:18] <jnthnwrthngtn> (The presence of the nested runloop boundary I mean)

[14:18] <jnthnwrthngtn> I think we used to detect those and try to nicely report them, but I wonder if it regressed (a possible victim of my work on rearranging returns)

[14:19] <jnthnwrthngtn> (Nicely report as in "oops", as in we don't consider it a condition we can recover from)

[14:20] <jnthnwrthngtn> The wrong string heap number and the getlexstatic_o together make me wonder if there is no getlexstatic_o really, it's just we're in a bad location in the bytecode stream (a mis-deopt would explain it but you didn't spot one of those)

[14:20] <jnthnwrthngtn> And so interpreting random things (and so interpreting things as string indexes that aren't, etc.)

[14:21] <jnthnwrthngtn> That or the bytecode stream is out of sync with the cu, static info, etc.

[14:21] <jnthnwrthngtn> afk for a bit, going to zizkov for walk/beer/curry :)

[14:25] <patrickb> jnt

[14:26] <patrickb> jnthn: The cert of commaide.com does not apply to www.commaide.com. But the links at the top of cro.services link to www.commaide.com

[14:27] <nine> jnthnwrthngtn: the wrong place in the bytecode part kinda fits with sp_resumption and what I meant with it being a strange beast. It's clearly not the runtime effect of JITed sp_resumption. But maybe we somehow handle it wrong when calculating the bytecode position when we return to the interpreter.

[14:27] <nine> Of course that would make much more sense if some actual deopt happened

[14:56] <timo> something going wrong with the callsite thats referenced in the resumption op?

[14:56] <timo> so it sort of changes its length on accident?

[14:57] <nine> resumption doesn't reference a callsite

[14:58] <timo> oh ok so the number of arguments it takes is in an inline cache or something

[14:59] <nine> Nah, it's just sp_resumption reg, int, int, ... with reg getting VMNulled, the first int being some index and the second int the number of varargs

[15:00] <nine> Somehow it's a mixture of JITed sp_resumption, finalizers and nested runloops

[15:02] <nine> I get the "Unwound entire stack and missed handler" message even though all callbacks have a CATCH block

[15:02] <nine> New one: MoarVM panic: No frame at top of callstack

[15:02] <timo> so, CONTROL then?

[15:06] <nine> No, they also got CONTROL blocks

[15:06] <timo> OK

[15:07] <timo> well it sounds kind of like memory corruption froom where im standing, which is maybe a bit too far away to be of much use

[15:16] <dogbert17> there seems to be quite a few bugs present in MoarVM atm, unless it's the same problem showing itself under different circumstances

[18:02] *** reportable6 left
[20:10] <nine> dogbert17: that's not terribly surprising considering the amount of changes that went in lately

[20:25] <dogbert17> true, now it's a question of finding them :)

[20:28] <nine> LOL, this is hilarious

[20:29] <nine> So...my bug somehow involves sp_resumption, GC and nested runloops, right? Except that it actually doesn't. sp_resumption is innocent and the GC just caused more callbacks to appear.

[20:29] <japhb> "hilarious" in the "OMG seriously?" sense?

[20:30] <nine> What happens is that the frame that the callback is running is completely JIT compiled, including the return_o. Now return_o replaces the current frame with its caller which in this case is the frame that calls the native code that eventually runs the callback.

[20:31] <nine> Exiting from the nested runloop is signified by the MVM_CALLSTACK_RECORD_NESTED_RUNLOOP record on the call stack. When MVM_callstack_unwind_frame encounters that it immediately returns 0 to signal that we need to stop the runloop.

[20:32] <nine> MVM_frame_try_return just forwards that result: return MVM_callstack_unwind_frame(tc, 0);

[20:33] <nine> The return_o op then checks this result: if (MVM_frame_try_return(tc) == 0) goto return_label;

[20:33] <nine> Now what does JIT code do? if (MVM_UNLIKELY(!tc->cur_frame)) { /* somehow unwound our top frame */ goto return_label; }

[20:34] <nine> s/JIT code/sp_jit_enter/

[20:34] <nine> It doesn't ever see that result and instead checks tc->cur_frame which at that time already points at the caller

[20:35] <nine> So we happily continue a runloop and venture forth into unexplored territorry of random memory

[20:36] <timo> wheeeee!

[20:37] <japhb> .oO( "We're going on a trip, / in our favorite rocket ship, / zooming through the sky ..." )

[20:42] <Geth> Â¦ MoarVM/fix_jited_return_from_native_runloops: 8a91bf8eb0 | (Stefan Seifert)++ | src/core/interp.c

[20:42] <Geth> Â¦ MoarVM/fix_jited_return_from_native_runloops: Fix JITed return from nested runloops

[20:42] <Geth> Â¦ MoarVM/fix_jited_return_from_native_runloops: 

[20:42] <Geth> Â¦ MoarVM/fix_jited_return_from_native_runloops: When a callback frame is completely JIT compiled, including a return_o, we did

[20:42] <Geth> Â¦ MoarVM/fix_jited_return_from_native_runloops: not notice that it's time to exit the runloop. MVM_callstack_unwind_frame will

[20:42] <Geth> Â¦ MoarVM/fix_jited_return_from_native_runloops: already have set tc->cur_frame to the frame that called the native routine that

[20:42] <Geth> Â¦ MoarVM/fix_jited_return_from_native_runloops: in turn ran the callback and returned 0 to signal that the runloop should end.

[20:42] <Geth> Â¦ MoarVM/fix_jited_return_from_native_runloops: This 0 got forwarded by MVM_frame_try_return but JIT compiled code does not

[20:42] <Geth> Â¦ MoarVM/fix_jited_return_from_native_runloops: <â¦commit message has 8 more linesâ¦>

[20:42] <Geth> Â¦ MoarVM/fix_jited_return_from_native_runloops: review: https://github.com/MoarVM/MoarVM/commit/8a91bf8eb0

[20:42] <Geth> Â¦ MoarVM: niner++ created pull request #1601: Fix JITed return from nested runloops

[20:42] <Geth> Â¦ MoarVM: review: https://github.com/MoarVM/MoarVM/pull/1601

[21:04] *** reportable6 joined
[22:03] <timo> got a clue why the mac build may have failed the test for `use Test; use Test; print "pass"`?

[22:04] <timo> https://dev.azure.com/MoarVM/MoarVM/_build/results?buildId=884&view=logs&j=bfe96415-bac8-5ed9-337c-3aa47377474c&t=2ff4f438-ed71-5e9a-65b5-c3ebba086224&l=4577

