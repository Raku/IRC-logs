[00:02] *** reportable6 left
[01:04] *** reportable6 joined
[01:12] *** psydroid joined
[02:12] *** AlexDaniel joined
[04:17] *** sourceable6 left
[04:17] *** releasable6 left
[04:17] *** reportable6 left
[04:17] *** statisfiable6 left
[04:17] *** bisectable6 left
[04:17] *** linkable6 left
[04:17] *** squashable6 left
[04:17] *** nativecallable6 left
[04:17] *** benchable6 left
[04:17] *** committable6 left
[04:17] *** greppable6 left
[04:17] *** bloatable6 left
[04:17] *** coverable6 left
[04:17] *** unicodable6 left
[04:17] *** shareable6 left
[04:17] *** quotable6 left
[04:17] *** tellable6 left
[04:17] *** evalable6 left
[04:17] *** notable6 left
[04:17] *** greppable6 joined
[04:18] *** shareable6 joined
[04:18] *** linkable6 joined
[04:18] *** committable6 joined
[04:18] *** benchable6 joined
[04:18] *** quotable6 joined
[04:18] *** sourceable6 joined
[04:19] *** statisfiable6 joined
[04:19] *** notable6 joined
[04:20] *** bloatable6 joined
[05:18] *** bisectable6 joined
[05:19] *** tellable6 joined
[05:19] *** releasable6 joined
[05:20] *** squashable6 joined
[06:18] *** unicodable6 joined
[06:19] *** evalable6 joined
[07:06] *** reportable6 joined
[07:20] *** coverable6 joined
[08:41] <nine> What I've found out so far: that oops happens because the inline cache entry was freed already when it got replaced by MVM_disp_inline_cache_transition in a different thread.

[08:46] <nine> So multiple threads are working at the same callsite. One thread is kinda in the middle of an elaborate dispatch when the GC gets triggered. The other thread was a bit faster and finished recording a dispatch program.

[08:47] <nine> When finishing recording, we update the inline cache entry scheduling the previous version to be freed at a safe point

[08:49] <nine> That old cache entry however is still pointed to by a callstack record in the other thread.

[09:05] <nine> I fear this will need jnthnwrthngtn to fix, as the proper course of action is anything but clear. There is a fix/workaround for this specific case, since we can detect the situation reliably (ic_entry_ptr does not point at ic_entry), but freeing a cache entry that's still pointed at by a callstack sounds like a recipie for desaster

[09:05] <nine> Memory management in multi threaded programs is hard

[09:07] <nine> Btw. to reproduce, set nursery size to something small like 6K and then while rr record /home/nine/rakudo/install/bin/moar --execname=/home/nine/rakudo/rakudo-m --libpath=/home/nine/rakudo --libpath=/home/nine/rakudo/blib --libpath=/home/nine/rakudo/install/share/nqp/lib /home/nine/rakudo/rakudo.moarvm -e 'await (^5).map({start { print qqx{echo $_} } })' ; do true ; done

[10:18] *** nativecallable6 joined
[10:37] *** frost-lab joined
[10:45] <nine> I created an issue for this, so the information does not get lost: https://github.com/MoarVM/MoarVM/issues/1529

[10:45] *** frost-lab left
[11:24] *** sena_kun joined
[12:02] *** reportable6 left
[12:16] <dogbert17> hmm, I don't think we have any more bugs atm

[12:19] * dogbert17 starts looking ...

[12:20] <sena_kun> dogbert17, there are still 180 tickets opened. :P

[12:22] <dogbert17> sena_kun: true, but atm I'm concentrating on new-disp

[12:23] <dogbert17> we need to give nine++ something to do :)

[12:25] <[Coke]> Still have the windows nativecall failures when run simultaneously

[12:25] <[Coke]> I haven't figured out yet how to get one of those failures to show up in the debugger so we can figure it out

[14:53] *** squashable6 left
[14:54] *** squashable6 joined
[15:04] *** reportable6 joined
[17:42] *** sena_kun left
[17:55] *** ilogger2 left
[17:56] *** ilogger2 joined
[18:02] *** reportable6 left
[20:07] *** harrow left
[20:07] *** harrow joined
[20:39] *** MasterDuke58 left
[21:45] *** MasterDuke joined
